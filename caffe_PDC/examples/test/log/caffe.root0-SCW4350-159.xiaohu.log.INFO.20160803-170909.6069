Log file created at: 2016/08/03 17:09:09
Running on machine: root0-SCW4350-159
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0803 17:09:09.371934  6069 caffe.cpp:185] Using GPUs 3
I0803 17:09:10.436379  6069 caffe.cpp:190] GPU 3: GeForce GTX TITAN X
I0803 17:09:10.877324  6069 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.0001
display: 20
max_iter: 400000
lr_policy: "poly"
power: 0.5
momentum: 0.9
weight_decay: 0.0002
snapshot: 10000
snapshot_prefix: "model/facial_point"
solver_mode: GPU
device_id: 3
net: "train_val.prototxt"
clip_gradients: 10
I0803 17:09:10.877547  6069 solver.cpp:91] Creating training net from net file: train_val.prototxt
I0803 17:09:10.878497  6069 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer facial_point
I0803 17:09:10.878803  6069 net.cpp:49] Initializing net from parameters: 
name: "facial_point_net"
state {
  phase: TRAIN
}
layer {
  name: "facial_point"
  type: "NewFacialPointData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  new_facial_point_data_param {
    source: "train_aug_list.txt"
    batch_size: 32
    shuffle: true
    new_height: 224
    new_width: 224
    is_color: true
    point_num: 68
    ext_scale: 1.2
    xy_mean: 0.03
    xy_std: 0.03
    wh_mean: 1
    wh_std: 0.03
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv5"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "68point"
  type: "InnerProduct"
  bottom: "fc7"
  top: "68point"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 136
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "68point"
  bottom: "label"
  top: "loss"
}
layer {
  name: "loc_reg_affine"
  type: "InnerProduct"
  bottom: "68point"
  top: "theta"
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "constant"
      value: 0
    }
    bias_filler {
      type: "file"
      file: "bias_init.txt"
    }
  }
}
layer {
  name: "theta_loss"
  type: "LocLoss"
  bottom: "theta"
  top: "theta_loss"
  loss_weight: 1
  loc_loss_param {
    threshold: 1
  }
}
layer {
  name: "st_pts"
  type: "PointTransformer"
  bottom: "label"
  bottom: "theta"
  top: "local/label"
  propagate_down: false
  propagate_down: false
  pt_param {
    in_width: 224
    in_height: 224
    out_width: 224
    out_height: 224
    transform_type: AFFINE
  }
}
layer {
  name: "st_layer"
  type: "SpatialTransformer"
  bottom: "data"
  bottom: "theta"
  top: "local/data"
  st_param {
    output_H: 224
    output_W: 224
    to_compute_dU: false
  }
}
layer {
  name: "local_conv1"
  type: "Convolution"
  bottom: "local/data"
  top: "local/conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu1"
  type: "ReLU"
  bottom: "local/conv1"
  top: "local/conv1"
}
layer {
  name: "local_norm1"
  type: "LRN"
  bottom: "local/conv1"
  top: "local/norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "local_pool1"
  type: "Pooling"
  bottom: "local/norm1"
  top: "local/pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "local_conv2"
  type: "Convolution"
  bottom: "local/pool1"
  top: "local/conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu2"
  type: "ReLU"
  bottom: "local/conv2"
  top: "local/conv2"
}
layer {
  name: "local_pool2"
  type: "Pooling"
  bottom: "local/conv2"
  top: "local/pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "local_conv3"
  type: "Convolution"
  bottom: "local/pool2"
  top: "local/conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu3"
  type: "ReLU"
  bottom: "local/conv3"
  top: "local/conv3"
}
layer {
  name: "local_conv4"
  type: "Convolution"
  bottom: "local/conv3"
  top: "local/conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu4"
  type: "ReLU"
  bottom: "local/conv4"
  top: "local/conv4"
}
layer {
  name: "local_conv5"
  type: "Convolution"
  bottom: "local/conv4"
  top: "local/conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu5"
  type: "ReLU"
  bottom: "local/conv5"
  top: "local/conv5"
}
layer {
  name: "local_pool3"
  type: "Pooling"
  bottom: "local/conv5"
  top: "local/pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "local_fc6"
  type: "InnerProduct"
  bottom: "local/pool3"
  top: "local/fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "local_relu6"
  type: "ReLU"
  bottom: "local/fc6"
  top: "local/fc6"
}
layer {
  name: "local_drop6"
  type: "Dropout"
  bottom: "local/fc6"
  top: "local/fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "local_fc7"
  type: "InnerProduct"
  bottom: "local/fc6"
  top: "local/fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "local_relu7"
  type: "ReLU"
  bottom: "local/fc7"
  top: "local/fc7"
}
layer {
  name: "local_drop7"
  type: "Dropout"
  bottom: "local/fc7"
  top: "local/fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "local_68point"
  type: "InnerProduct"
  bottom: "local/fc7"
  top: "local/68point"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 136
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "local_loss"
  type: "EuclideanLoss"
  bottom: "local/68point"
  bottom: "local/label"
  top: "local/loss"
}
I0803 17:09:10.880496  6069 layer_factory.hpp:77] Creating layer facial_point
I0803 17:09:10.880548  6069 net.cpp:91] Creating Layer facial_point
I0803 17:09:10.880559  6069 net.cpp:399] facial_point -> data
I0803 17:09:10.880589  6069 net.cpp:399] facial_point -> label
I0803 17:09:10.880607  6069 new_facial_point_data_layer.cpp:187] Opening file train_aug_list.txt
I0803 17:09:10.884806  6069 new_facial_point_data_layer.cpp:197] Shuffling data
I0803 17:09:10.885962  6069 new_facial_point_data_layer.cpp:203] A total of 12592 images.
I0803 17:09:10.938750  6069 new_facial_point_data_layer.cpp:263] output data size: 32,3,224,224
I0803 17:09:10.979109  6069 net.cpp:141] Setting up facial_point
I0803 17:09:10.979185  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:10.979198  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:10.979205  6069 net.cpp:156] Memory required for data: 19284992
I0803 17:09:10.979221  6069 layer_factory.hpp:77] Creating layer data_facial_point_0_split
I0803 17:09:10.979254  6069 net.cpp:91] Creating Layer data_facial_point_0_split
I0803 17:09:10.979264  6069 net.cpp:425] data_facial_point_0_split <- data
I0803 17:09:10.979282  6069 net.cpp:399] data_facial_point_0_split -> data_facial_point_0_split_0
I0803 17:09:10.979300  6069 net.cpp:399] data_facial_point_0_split -> data_facial_point_0_split_1
I0803 17:09:10.979414  6069 net.cpp:141] Setting up data_facial_point_0_split
I0803 17:09:10.979459  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:10.979467  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:10.979475  6069 net.cpp:156] Memory required for data: 57820160
I0803 17:09:10.979480  6069 layer_factory.hpp:77] Creating layer label_facial_point_1_split
I0803 17:09:10.979491  6069 net.cpp:91] Creating Layer label_facial_point_1_split
I0803 17:09:10.979498  6069 net.cpp:425] label_facial_point_1_split <- label
I0803 17:09:10.979507  6069 net.cpp:399] label_facial_point_1_split -> label_facial_point_1_split_0
I0803 17:09:10.979517  6069 net.cpp:399] label_facial_point_1_split -> label_facial_point_1_split_1
I0803 17:09:10.979560  6069 net.cpp:141] Setting up label_facial_point_1_split
I0803 17:09:10.979573  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:10.979578  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:10.979585  6069 net.cpp:156] Memory required for data: 57854976
I0803 17:09:10.979591  6069 layer_factory.hpp:77] Creating layer conv1
I0803 17:09:10.979616  6069 net.cpp:91] Creating Layer conv1
I0803 17:09:10.979624  6069 net.cpp:425] conv1 <- data_facial_point_0_split_0
I0803 17:09:10.979635  6069 net.cpp:399] conv1 -> conv1
I0803 17:09:10.979985  6069 net.cpp:141] Setting up conv1
I0803 17:09:10.980012  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:10.980020  6069 net.cpp:156] Memory required for data: 203848704
I0803 17:09:10.980039  6069 layer_factory.hpp:77] Creating layer relu1
I0803 17:09:10.980051  6069 net.cpp:91] Creating Layer relu1
I0803 17:09:10.980058  6069 net.cpp:425] relu1 <- conv1
I0803 17:09:10.980067  6069 net.cpp:386] relu1 -> conv1 (in-place)
I0803 17:09:10.980082  6069 net.cpp:141] Setting up relu1
I0803 17:09:10.980090  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:10.980096  6069 net.cpp:156] Memory required for data: 349842432
I0803 17:09:10.980103  6069 layer_factory.hpp:77] Creating layer norm1
I0803 17:09:10.980113  6069 net.cpp:91] Creating Layer norm1
I0803 17:09:10.980119  6069 net.cpp:425] norm1 <- conv1
I0803 17:09:10.980128  6069 net.cpp:399] norm1 -> norm1
I0803 17:09:10.980172  6069 net.cpp:141] Setting up norm1
I0803 17:09:10.980185  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:10.980190  6069 net.cpp:156] Memory required for data: 495836160
I0803 17:09:10.980197  6069 layer_factory.hpp:77] Creating layer pool1
I0803 17:09:10.980208  6069 net.cpp:91] Creating Layer pool1
I0803 17:09:10.980216  6069 net.cpp:425] pool1 <- norm1
I0803 17:09:10.980224  6069 net.cpp:399] pool1 -> pool1
I0803 17:09:10.980268  6069 net.cpp:141] Setting up pool1
I0803 17:09:10.980280  6069 net.cpp:148] Top shape: 32 96 37 37 (4205568)
I0803 17:09:10.980286  6069 net.cpp:156] Memory required for data: 512658432
I0803 17:09:10.980293  6069 layer_factory.hpp:77] Creating layer conv2
I0803 17:09:10.980305  6069 net.cpp:91] Creating Layer conv2
I0803 17:09:10.980312  6069 net.cpp:425] conv2 <- pool1
I0803 17:09:10.980324  6069 net.cpp:399] conv2 -> conv2
I0803 17:09:10.985987  6069 net.cpp:141] Setting up conv2
I0803 17:09:10.986029  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:10.986038  6069 net.cpp:156] Memory required for data: 548342784
I0803 17:09:10.986052  6069 layer_factory.hpp:77] Creating layer relu2
I0803 17:09:10.986064  6069 net.cpp:91] Creating Layer relu2
I0803 17:09:10.986071  6069 net.cpp:425] relu2 <- conv2
I0803 17:09:10.986081  6069 net.cpp:386] relu2 -> conv2 (in-place)
I0803 17:09:10.986093  6069 net.cpp:141] Setting up relu2
I0803 17:09:10.986100  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:10.986106  6069 net.cpp:156] Memory required for data: 584027136
I0803 17:09:10.986112  6069 layer_factory.hpp:77] Creating layer pool2
I0803 17:09:10.986121  6069 net.cpp:91] Creating Layer pool2
I0803 17:09:10.986129  6069 net.cpp:425] pool2 <- conv2
I0803 17:09:10.986136  6069 net.cpp:399] pool2 -> pool2
I0803 17:09:10.986176  6069 net.cpp:141] Setting up pool2
I0803 17:09:10.986207  6069 net.cpp:148] Top shape: 32 256 17 17 (2367488)
I0803 17:09:10.986213  6069 net.cpp:156] Memory required for data: 593497088
I0803 17:09:10.986220  6069 layer_factory.hpp:77] Creating layer conv3
I0803 17:09:10.986233  6069 net.cpp:91] Creating Layer conv3
I0803 17:09:10.986240  6069 net.cpp:425] conv3 <- pool2
I0803 17:09:10.986250  6069 net.cpp:399] conv3 -> conv3
I0803 17:09:10.997083  6069 net.cpp:141] Setting up conv3
I0803 17:09:10.997125  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:10.997134  6069 net.cpp:156] Memory required for data: 612436992
I0803 17:09:10.997149  6069 layer_factory.hpp:77] Creating layer relu3
I0803 17:09:10.997164  6069 net.cpp:91] Creating Layer relu3
I0803 17:09:10.997171  6069 net.cpp:425] relu3 <- conv3
I0803 17:09:10.997182  6069 net.cpp:386] relu3 -> conv3 (in-place)
I0803 17:09:10.997195  6069 net.cpp:141] Setting up relu3
I0803 17:09:10.997202  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:10.997208  6069 net.cpp:156] Memory required for data: 631376896
I0803 17:09:10.997215  6069 layer_factory.hpp:77] Creating layer conv4
I0803 17:09:10.997228  6069 net.cpp:91] Creating Layer conv4
I0803 17:09:10.997236  6069 net.cpp:425] conv4 <- conv3
I0803 17:09:10.997246  6069 net.cpp:399] conv4 -> conv4
I0803 17:09:11.017187  6069 net.cpp:141] Setting up conv4
I0803 17:09:11.017242  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:11.017251  6069 net.cpp:156] Memory required for data: 650316800
I0803 17:09:11.017266  6069 layer_factory.hpp:77] Creating layer relu4
I0803 17:09:11.017282  6069 net.cpp:91] Creating Layer relu4
I0803 17:09:11.017290  6069 net.cpp:425] relu4 <- conv4
I0803 17:09:11.017302  6069 net.cpp:386] relu4 -> conv4 (in-place)
I0803 17:09:11.017319  6069 net.cpp:141] Setting up relu4
I0803 17:09:11.017328  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:11.017333  6069 net.cpp:156] Memory required for data: 669256704
I0803 17:09:11.017340  6069 layer_factory.hpp:77] Creating layer conv5
I0803 17:09:11.017354  6069 net.cpp:91] Creating Layer conv5
I0803 17:09:11.017361  6069 net.cpp:425] conv5 <- conv4
I0803 17:09:11.017372  6069 net.cpp:399] conv5 -> conv5
I0803 17:09:11.359758  6069 net.cpp:141] Setting up conv5
I0803 17:09:11.359829  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:11.359846  6069 net.cpp:156] Memory required for data: 688196608
I0803 17:09:11.359879  6069 layer_factory.hpp:77] Creating layer relu5
I0803 17:09:11.359910  6069 net.cpp:91] Creating Layer relu5
I0803 17:09:11.359925  6069 net.cpp:425] relu5 <- conv5
I0803 17:09:11.359943  6069 net.cpp:386] relu5 -> conv5 (in-place)
I0803 17:09:11.359969  6069 net.cpp:141] Setting up relu5
I0803 17:09:11.359987  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:11.359998  6069 net.cpp:156] Memory required for data: 707136512
I0803 17:09:11.360011  6069 layer_factory.hpp:77] Creating layer pool3
I0803 17:09:11.360035  6069 net.cpp:91] Creating Layer pool3
I0803 17:09:11.360049  6069 net.cpp:425] pool3 <- conv5
I0803 17:09:11.360067  6069 net.cpp:399] pool3 -> pool3
I0803 17:09:11.360146  6069 net.cpp:141] Setting up pool3
I0803 17:09:11.360169  6069 net.cpp:148] Top shape: 32 512 6 6 (589824)
I0803 17:09:11.360182  6069 net.cpp:156] Memory required for data: 709495808
I0803 17:09:11.360195  6069 layer_factory.hpp:77] Creating layer fc6
I0803 17:09:11.360216  6069 net.cpp:91] Creating Layer fc6
I0803 17:09:11.360229  6069 net.cpp:425] fc6 <- pool3
I0803 17:09:11.360251  6069 net.cpp:399] fc6 -> fc6
I0803 17:09:12.166817  6069 net.cpp:141] Setting up fc6
I0803 17:09:12.166934  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:12.166944  6069 net.cpp:156] Memory required for data: 710020096
I0803 17:09:12.167004  6069 layer_factory.hpp:77] Creating layer relu6
I0803 17:09:12.167059  6069 net.cpp:91] Creating Layer relu6
I0803 17:09:12.167100  6069 net.cpp:425] relu6 <- fc6
I0803 17:09:12.167112  6069 net.cpp:386] relu6 -> fc6 (in-place)
I0803 17:09:12.167129  6069 net.cpp:141] Setting up relu6
I0803 17:09:12.167168  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:12.167178  6069 net.cpp:156] Memory required for data: 710544384
I0803 17:09:12.167192  6069 layer_factory.hpp:77] Creating layer drop6
I0803 17:09:12.167212  6069 net.cpp:91] Creating Layer drop6
I0803 17:09:12.167222  6069 net.cpp:425] drop6 <- fc6
I0803 17:09:12.167232  6069 net.cpp:386] drop6 -> fc6 (in-place)
I0803 17:09:12.167387  6069 net.cpp:141] Setting up drop6
I0803 17:09:12.167403  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:12.167412  6069 net.cpp:156] Memory required for data: 711068672
I0803 17:09:12.167419  6069 layer_factory.hpp:77] Creating layer fc7
I0803 17:09:12.167443  6069 net.cpp:91] Creating Layer fc7
I0803 17:09:12.167453  6069 net.cpp:425] fc7 <- fc6
I0803 17:09:12.167466  6069 net.cpp:399] fc7 -> fc7
I0803 17:09:12.645547  6069 net.cpp:141] Setting up fc7
I0803 17:09:12.645614  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:12.645627  6069 net.cpp:156] Memory required for data: 711592960
I0803 17:09:12.645647  6069 layer_factory.hpp:77] Creating layer relu7
I0803 17:09:12.645673  6069 net.cpp:91] Creating Layer relu7
I0803 17:09:12.645685  6069 net.cpp:425] relu7 <- fc7
I0803 17:09:12.645700  6069 net.cpp:386] relu7 -> fc7 (in-place)
I0803 17:09:12.645720  6069 net.cpp:141] Setting up relu7
I0803 17:09:12.645731  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:12.645740  6069 net.cpp:156] Memory required for data: 712117248
I0803 17:09:12.645751  6069 layer_factory.hpp:77] Creating layer drop7
I0803 17:09:12.645766  6069 net.cpp:91] Creating Layer drop7
I0803 17:09:12.645774  6069 net.cpp:425] drop7 <- fc7
I0803 17:09:12.645787  6069 net.cpp:386] drop7 -> fc7 (in-place)
I0803 17:09:12.645826  6069 net.cpp:141] Setting up drop7
I0803 17:09:12.645843  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:12.645851  6069 net.cpp:156] Memory required for data: 712641536
I0803 17:09:12.645859  6069 layer_factory.hpp:77] Creating layer 68point
I0803 17:09:12.645879  6069 net.cpp:91] Creating Layer 68point
I0803 17:09:12.645887  6069 net.cpp:425] 68point <- fc7
I0803 17:09:12.645901  6069 net.cpp:399] 68point -> 68point
I0803 17:09:12.910853  6069 net.cpp:141] Setting up 68point
I0803 17:09:12.910939  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:12.910959  6069 net.cpp:156] Memory required for data: 712658944
I0803 17:09:12.910987  6069 layer_factory.hpp:77] Creating layer 68point_68point_0_split
I0803 17:09:12.911015  6069 net.cpp:91] Creating Layer 68point_68point_0_split
I0803 17:09:12.911033  6069 net.cpp:425] 68point_68point_0_split <- 68point
I0803 17:09:12.911059  6069 net.cpp:399] 68point_68point_0_split -> 68point_68point_0_split_0
I0803 17:09:12.911108  6069 net.cpp:399] 68point_68point_0_split -> 68point_68point_0_split_1
I0803 17:09:12.911192  6069 net.cpp:141] Setting up 68point_68point_0_split
I0803 17:09:12.911222  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:12.911239  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:12.911253  6069 net.cpp:156] Memory required for data: 712693760
I0803 17:09:12.911268  6069 layer_factory.hpp:77] Creating layer loss
I0803 17:09:12.911304  6069 net.cpp:91] Creating Layer loss
I0803 17:09:12.911324  6069 net.cpp:425] loss <- 68point_68point_0_split_0
I0803 17:09:12.911341  6069 net.cpp:425] loss <- label_facial_point_1_split_0
I0803 17:09:12.911360  6069 net.cpp:399] loss -> loss
I0803 17:09:12.911453  6069 net.cpp:141] Setting up loss
I0803 17:09:12.911478  6069 net.cpp:148] Top shape: (1)
I0803 17:09:12.911494  6069 net.cpp:151]     with loss weight 1
I0803 17:09:12.911553  6069 net.cpp:156] Memory required for data: 712693764
I0803 17:09:12.911571  6069 layer_factory.hpp:77] Creating layer loc_reg_affine
I0803 17:09:12.911599  6069 net.cpp:91] Creating Layer loc_reg_affine
I0803 17:09:12.911617  6069 net.cpp:425] loc_reg_affine <- 68point_68point_0_split_1
I0803 17:09:12.911636  6069 net.cpp:399] loc_reg_affine -> theta
I0803 17:09:13.354123  6069 net.cpp:141] Setting up loc_reg_affine
I0803 17:09:13.354213  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:13.354233  6069 net.cpp:156] Memory required for data: 712694532
I0803 17:09:13.354277  6069 layer_factory.hpp:77] Creating layer theta_loc_reg_affine_0_split
I0803 17:09:13.354307  6069 net.cpp:91] Creating Layer theta_loc_reg_affine_0_split
I0803 17:09:13.354326  6069 net.cpp:425] theta_loc_reg_affine_0_split <- theta
I0803 17:09:13.354353  6069 net.cpp:399] theta_loc_reg_affine_0_split -> theta_loc_reg_affine_0_split_0
I0803 17:09:13.354382  6069 net.cpp:399] theta_loc_reg_affine_0_split -> theta_loc_reg_affine_0_split_1
I0803 17:09:13.354405  6069 net.cpp:399] theta_loc_reg_affine_0_split -> theta_loc_reg_affine_0_split_2
I0803 17:09:13.354527  6069 net.cpp:141] Setting up theta_loc_reg_affine_0_split
I0803 17:09:13.354555  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:13.354573  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:13.354588  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:13.354603  6069 net.cpp:156] Memory required for data: 712696836
I0803 17:09:13.354617  6069 layer_factory.hpp:77] Creating layer theta_loss
I0803 17:09:13.354640  6069 net.cpp:91] Creating Layer theta_loss
I0803 17:09:13.354657  6069 net.cpp:425] theta_loss <- theta_loc_reg_affine_0_split_0
I0803 17:09:13.354677  6069 net.cpp:399] theta_loss -> theta_loss
I0803 17:09:13.354807  6069 net.cpp:141] Setting up theta_loss
I0803 17:09:13.354835  6069 net.cpp:148] Top shape: (1)
I0803 17:09:13.354851  6069 net.cpp:151]     with loss weight 1
I0803 17:09:13.354867  6069 net.cpp:156] Memory required for data: 712696840
I0803 17:09:13.354882  6069 layer_factory.hpp:77] Creating layer st_pts
I0803 17:09:13.354908  6069 net.cpp:91] Creating Layer st_pts
I0803 17:09:13.354925  6069 net.cpp:425] st_pts <- label_facial_point_1_split_1
I0803 17:09:13.354943  6069 net.cpp:425] st_pts <- theta_loc_reg_affine_0_split_1
I0803 17:09:13.354969  6069 net.cpp:399] st_pts -> local/label
I0803 17:09:13.355042  6069 net.cpp:141] Setting up st_pts
I0803 17:09:13.355067  6069 net.cpp:148] Top shape: 32 136 1 1 (4352)
I0803 17:09:13.355082  6069 net.cpp:156] Memory required for data: 712714248
I0803 17:09:13.355098  6069 layer_factory.hpp:77] Creating layer st_layer
I0803 17:09:13.355121  6069 net.cpp:91] Creating Layer st_layer
I0803 17:09:13.355137  6069 net.cpp:425] st_layer <- data_facial_point_0_split_1
I0803 17:09:13.355155  6069 net.cpp:425] st_layer <- theta_loc_reg_affine_0_split_2
I0803 17:09:13.355176  6069 net.cpp:399] st_layer -> local/data
I0803 17:09:13.359269  6069 net.cpp:141] Setting up st_layer
I0803 17:09:13.359307  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:13.359323  6069 net.cpp:156] Memory required for data: 731981832
I0803 17:09:13.359339  6069 layer_factory.hpp:77] Creating layer local_conv1
I0803 17:09:13.359374  6069 net.cpp:91] Creating Layer local_conv1
I0803 17:09:13.359400  6069 net.cpp:425] local_conv1 <- local/data
I0803 17:09:13.359424  6069 net.cpp:399] local_conv1 -> local/conv1
I0803 17:09:13.360138  6069 net.cpp:141] Setting up local_conv1
I0803 17:09:13.360174  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:13.360189  6069 net.cpp:156] Memory required for data: 877975560
I0803 17:09:13.360211  6069 layer_factory.hpp:77] Creating layer local_relu1
I0803 17:09:13.360244  6069 net.cpp:91] Creating Layer local_relu1
I0803 17:09:13.360262  6069 net.cpp:425] local_relu1 <- local/conv1
I0803 17:09:13.360283  6069 net.cpp:386] local_relu1 -> local/conv1 (in-place)
I0803 17:09:13.360307  6069 net.cpp:141] Setting up local_relu1
I0803 17:09:13.360324  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:13.360338  6069 net.cpp:156] Memory required for data: 1023969288
I0803 17:09:13.360352  6069 layer_factory.hpp:77] Creating layer local_norm1
I0803 17:09:13.360376  6069 net.cpp:91] Creating Layer local_norm1
I0803 17:09:13.360391  6069 net.cpp:425] local_norm1 <- local/conv1
I0803 17:09:13.360409  6069 net.cpp:399] local_norm1 -> local/norm1
I0803 17:09:13.360492  6069 net.cpp:141] Setting up local_norm1
I0803 17:09:13.360545  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:13.360560  6069 net.cpp:156] Memory required for data: 1169963016
I0803 17:09:13.360574  6069 layer_factory.hpp:77] Creating layer local_pool1
I0803 17:09:13.360599  6069 net.cpp:91] Creating Layer local_pool1
I0803 17:09:13.360615  6069 net.cpp:425] local_pool1 <- local/norm1
I0803 17:09:13.360632  6069 net.cpp:399] local_pool1 -> local/pool1
I0803 17:09:13.360718  6069 net.cpp:141] Setting up local_pool1
I0803 17:09:13.360740  6069 net.cpp:148] Top shape: 32 96 37 37 (4205568)
I0803 17:09:13.360754  6069 net.cpp:156] Memory required for data: 1186785288
I0803 17:09:13.360769  6069 layer_factory.hpp:77] Creating layer local_conv2
I0803 17:09:13.360797  6069 net.cpp:91] Creating Layer local_conv2
I0803 17:09:13.360813  6069 net.cpp:425] local_conv2 <- local/pool1
I0803 17:09:13.360833  6069 net.cpp:399] local_conv2 -> local/conv2
I0803 17:09:13.372594  6069 net.cpp:141] Setting up local_conv2
I0803 17:09:13.372650  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:13.372666  6069 net.cpp:156] Memory required for data: 1222469640
I0803 17:09:13.372689  6069 layer_factory.hpp:77] Creating layer local_relu2
I0803 17:09:13.372709  6069 net.cpp:91] Creating Layer local_relu2
I0803 17:09:13.372725  6069 net.cpp:425] local_relu2 <- local/conv2
I0803 17:09:13.372745  6069 net.cpp:386] local_relu2 -> local/conv2 (in-place)
I0803 17:09:13.372766  6069 net.cpp:141] Setting up local_relu2
I0803 17:09:13.372784  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:13.372798  6069 net.cpp:156] Memory required for data: 1258153992
I0803 17:09:13.372814  6069 layer_factory.hpp:77] Creating layer local_pool2
I0803 17:09:13.372838  6069 net.cpp:91] Creating Layer local_pool2
I0803 17:09:13.372853  6069 net.cpp:425] local_pool2 <- local/conv2
I0803 17:09:13.372884  6069 net.cpp:399] local_pool2 -> local/pool2
I0803 17:09:13.372973  6069 net.cpp:141] Setting up local_pool2
I0803 17:09:13.373003  6069 net.cpp:148] Top shape: 32 256 17 17 (2367488)
I0803 17:09:13.373018  6069 net.cpp:156] Memory required for data: 1267623944
I0803 17:09:13.373030  6069 layer_factory.hpp:77] Creating layer local_conv3
I0803 17:09:13.373059  6069 net.cpp:91] Creating Layer local_conv3
I0803 17:09:13.373085  6069 net.cpp:425] local_conv3 <- local/pool2
I0803 17:09:13.373103  6069 net.cpp:399] local_conv3 -> local/conv3
I0803 17:09:13.393007  6069 net.cpp:141] Setting up local_conv3
I0803 17:09:13.393067  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:13.393080  6069 net.cpp:156] Memory required for data: 1286563848
I0803 17:09:13.393103  6069 layer_factory.hpp:77] Creating layer local_relu3
I0803 17:09:13.393126  6069 net.cpp:91] Creating Layer local_relu3
I0803 17:09:13.393142  6069 net.cpp:425] local_relu3 <- local/conv3
I0803 17:09:13.393162  6069 net.cpp:386] local_relu3 -> local/conv3 (in-place)
I0803 17:09:13.393191  6069 net.cpp:141] Setting up local_relu3
I0803 17:09:13.393216  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:13.393227  6069 net.cpp:156] Memory required for data: 1305503752
I0803 17:09:13.393242  6069 layer_factory.hpp:77] Creating layer local_conv4
I0803 17:09:13.393266  6069 net.cpp:91] Creating Layer local_conv4
I0803 17:09:13.393280  6069 net.cpp:425] local_conv4 <- local/conv3
I0803 17:09:13.393299  6069 net.cpp:399] local_conv4 -> local/conv4
I0803 17:09:13.426812  6069 net.cpp:141] Setting up local_conv4
I0803 17:09:13.426864  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:13.426877  6069 net.cpp:156] Memory required for data: 1324443656
I0803 17:09:13.426903  6069 layer_factory.hpp:77] Creating layer local_relu4
I0803 17:09:13.426929  6069 net.cpp:91] Creating Layer local_relu4
I0803 17:09:13.426944  6069 net.cpp:425] local_relu4 <- local/conv4
I0803 17:09:13.426967  6069 net.cpp:386] local_relu4 -> local/conv4 (in-place)
I0803 17:09:13.426988  6069 net.cpp:141] Setting up local_relu4
I0803 17:09:13.427002  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:13.427047  6069 net.cpp:156] Memory required for data: 1343383560
I0803 17:09:13.427059  6069 layer_factory.hpp:77] Creating layer local_conv5
I0803 17:09:13.427086  6069 net.cpp:91] Creating Layer local_conv5
I0803 17:09:13.427098  6069 net.cpp:425] local_conv5 <- local/conv4
I0803 17:09:13.427116  6069 net.cpp:399] local_conv5 -> local/conv5
I0803 17:09:13.457027  6069 net.cpp:141] Setting up local_conv5
I0803 17:09:13.457078  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:13.457088  6069 net.cpp:156] Memory required for data: 1362323464
I0803 17:09:13.457106  6069 layer_factory.hpp:77] Creating layer local_relu5
I0803 17:09:13.457126  6069 net.cpp:91] Creating Layer local_relu5
I0803 17:09:13.457139  6069 net.cpp:425] local_relu5 <- local/conv5
I0803 17:09:13.457156  6069 net.cpp:386] local_relu5 -> local/conv5 (in-place)
I0803 17:09:13.457183  6069 net.cpp:141] Setting up local_relu5
I0803 17:09:13.457195  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:13.457203  6069 net.cpp:156] Memory required for data: 1381263368
I0803 17:09:13.457221  6069 layer_factory.hpp:77] Creating layer local_pool3
I0803 17:09:13.457234  6069 net.cpp:91] Creating Layer local_pool3
I0803 17:09:13.457243  6069 net.cpp:425] local_pool3 <- local/conv5
I0803 17:09:13.457257  6069 net.cpp:399] local_pool3 -> local/pool3
I0803 17:09:13.457314  6069 net.cpp:141] Setting up local_pool3
I0803 17:09:13.457329  6069 net.cpp:148] Top shape: 32 512 6 6 (589824)
I0803 17:09:13.457337  6069 net.cpp:156] Memory required for data: 1383622664
I0803 17:09:13.457346  6069 layer_factory.hpp:77] Creating layer local_fc6
I0803 17:09:13.457365  6069 net.cpp:91] Creating Layer local_fc6
I0803 17:09:13.457375  6069 net.cpp:425] local_fc6 <- local/pool3
I0803 17:09:13.457386  6069 net.cpp:399] local_fc6 -> local/fc6
I0803 17:09:14.235937  6069 net.cpp:141] Setting up local_fc6
I0803 17:09:14.236002  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:14.236013  6069 net.cpp:156] Memory required for data: 1384146952
I0803 17:09:14.236047  6069 layer_factory.hpp:77] Creating layer local_relu6
I0803 17:09:14.236073  6069 net.cpp:91] Creating Layer local_relu6
I0803 17:09:14.236124  6069 net.cpp:425] local_relu6 <- local/fc6
I0803 17:09:14.236141  6069 net.cpp:386] local_relu6 -> local/fc6 (in-place)
I0803 17:09:14.236160  6069 net.cpp:141] Setting up local_relu6
I0803 17:09:14.236172  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:14.236182  6069 net.cpp:156] Memory required for data: 1384671240
I0803 17:09:14.236192  6069 layer_factory.hpp:77] Creating layer local_drop6
I0803 17:09:14.236217  6069 net.cpp:91] Creating Layer local_drop6
I0803 17:09:14.236228  6069 net.cpp:425] local_drop6 <- local/fc6
I0803 17:09:14.236239  6069 net.cpp:386] local_drop6 -> local/fc6 (in-place)
I0803 17:09:14.236315  6069 net.cpp:141] Setting up local_drop6
I0803 17:09:14.236333  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:14.236343  6069 net.cpp:156] Memory required for data: 1385195528
I0803 17:09:14.236352  6069 layer_factory.hpp:77] Creating layer local_fc7
I0803 17:09:14.236377  6069 net.cpp:91] Creating Layer local_fc7
I0803 17:09:14.236387  6069 net.cpp:425] local_fc7 <- local/fc6
I0803 17:09:14.236402  6069 net.cpp:399] local_fc7 -> local/fc7
I0803 17:09:14.401922  6069 net.cpp:141] Setting up local_fc7
I0803 17:09:14.401975  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:14.401984  6069 net.cpp:156] Memory required for data: 1385719816
I0803 17:09:14.401999  6069 layer_factory.hpp:77] Creating layer local_relu7
I0803 17:09:14.402025  6069 net.cpp:91] Creating Layer local_relu7
I0803 17:09:14.402036  6069 net.cpp:425] local_relu7 <- local/fc7
I0803 17:09:14.402055  6069 net.cpp:386] local_relu7 -> local/fc7 (in-place)
I0803 17:09:14.402070  6069 net.cpp:141] Setting up local_relu7
I0803 17:09:14.402081  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:14.402089  6069 net.cpp:156] Memory required for data: 1386244104
I0803 17:09:14.402097  6069 layer_factory.hpp:77] Creating layer local_drop7
I0803 17:09:14.402153  6069 net.cpp:91] Creating Layer local_drop7
I0803 17:09:14.402163  6069 net.cpp:425] local_drop7 <- local/fc7
I0803 17:09:14.402175  6069 net.cpp:386] local_drop7 -> local/fc7 (in-place)
I0803 17:09:14.402212  6069 net.cpp:141] Setting up local_drop7
I0803 17:09:14.402226  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:14.402235  6069 net.cpp:156] Memory required for data: 1386768392
I0803 17:09:14.402243  6069 layer_factory.hpp:77] Creating layer local_68point
I0803 17:09:14.402261  6069 net.cpp:91] Creating Layer local_68point
I0803 17:09:14.402268  6069 net.cpp:425] local_68point <- local/fc7
I0803 17:09:14.402290  6069 net.cpp:399] local_68point -> local/68point
I0803 17:09:14.407975  6069 net.cpp:141] Setting up local_68point
I0803 17:09:14.408001  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:14.408010  6069 net.cpp:156] Memory required for data: 1386785800
I0803 17:09:14.408058  6069 layer_factory.hpp:77] Creating layer local_loss
I0803 17:09:14.408088  6069 net.cpp:91] Creating Layer local_loss
I0803 17:09:14.408098  6069 net.cpp:425] local_loss <- local/68point
I0803 17:09:14.408108  6069 net.cpp:425] local_loss <- local/label
I0803 17:09:14.408126  6069 net.cpp:399] local_loss -> local/loss
I0803 17:09:14.408179  6069 net.cpp:141] Setting up local_loss
I0803 17:09:14.408195  6069 net.cpp:148] Top shape: (1)
I0803 17:09:14.408203  6069 net.cpp:151]     with loss weight 1
I0803 17:09:14.408233  6069 net.cpp:156] Memory required for data: 1386785804
I0803 17:09:14.408243  6069 net.cpp:217] local_loss needs backward computation.
I0803 17:09:14.408252  6069 net.cpp:217] local_68point needs backward computation.
I0803 17:09:14.408260  6069 net.cpp:217] local_drop7 needs backward computation.
I0803 17:09:14.408269  6069 net.cpp:217] local_relu7 needs backward computation.
I0803 17:09:14.408277  6069 net.cpp:217] local_fc7 needs backward computation.
I0803 17:09:14.408285  6069 net.cpp:217] local_drop6 needs backward computation.
I0803 17:09:14.408293  6069 net.cpp:217] local_relu6 needs backward computation.
I0803 17:09:14.408303  6069 net.cpp:217] local_fc6 needs backward computation.
I0803 17:09:14.408310  6069 net.cpp:217] local_pool3 needs backward computation.
I0803 17:09:14.408318  6069 net.cpp:217] local_relu5 needs backward computation.
I0803 17:09:14.408327  6069 net.cpp:217] local_conv5 needs backward computation.
I0803 17:09:14.408336  6069 net.cpp:217] local_relu4 needs backward computation.
I0803 17:09:14.408345  6069 net.cpp:217] local_conv4 needs backward computation.
I0803 17:09:14.408354  6069 net.cpp:217] local_relu3 needs backward computation.
I0803 17:09:14.408361  6069 net.cpp:217] local_conv3 needs backward computation.
I0803 17:09:14.408370  6069 net.cpp:217] local_pool2 needs backward computation.
I0803 17:09:14.408377  6069 net.cpp:217] local_relu2 needs backward computation.
I0803 17:09:14.408387  6069 net.cpp:217] local_conv2 needs backward computation.
I0803 17:09:14.408404  6069 net.cpp:217] local_pool1 needs backward computation.
I0803 17:09:14.408413  6069 net.cpp:217] local_norm1 needs backward computation.
I0803 17:09:14.408422  6069 net.cpp:217] local_relu1 needs backward computation.
I0803 17:09:14.408432  6069 net.cpp:217] local_conv1 needs backward computation.
I0803 17:09:14.408450  6069 net.cpp:217] st_layer needs backward computation.
I0803 17:09:14.408462  6069 net.cpp:217] st_pts needs backward computation.
I0803 17:09:14.408470  6069 net.cpp:217] theta_loss needs backward computation.
I0803 17:09:14.408479  6069 net.cpp:217] theta_loc_reg_affine_0_split needs backward computation.
I0803 17:09:14.408488  6069 net.cpp:217] loc_reg_affine needs backward computation.
I0803 17:09:14.408496  6069 net.cpp:217] loss needs backward computation.
I0803 17:09:14.408505  6069 net.cpp:217] 68point_68point_0_split needs backward computation.
I0803 17:09:14.408514  6069 net.cpp:217] 68point needs backward computation.
I0803 17:09:14.408521  6069 net.cpp:217] drop7 needs backward computation.
I0803 17:09:14.408529  6069 net.cpp:217] relu7 needs backward computation.
I0803 17:09:14.408551  6069 net.cpp:217] fc7 needs backward computation.
I0803 17:09:14.408561  6069 net.cpp:217] drop6 needs backward computation.
I0803 17:09:14.408570  6069 net.cpp:217] relu6 needs backward computation.
I0803 17:09:14.408576  6069 net.cpp:217] fc6 needs backward computation.
I0803 17:09:14.408584  6069 net.cpp:217] pool3 needs backward computation.
I0803 17:09:14.408591  6069 net.cpp:217] relu5 needs backward computation.
I0803 17:09:14.408598  6069 net.cpp:217] conv5 needs backward computation.
I0803 17:09:14.408606  6069 net.cpp:217] relu4 needs backward computation.
I0803 17:09:14.408613  6069 net.cpp:217] conv4 needs backward computation.
I0803 17:09:14.408620  6069 net.cpp:217] relu3 needs backward computation.
I0803 17:09:14.408627  6069 net.cpp:217] conv3 needs backward computation.
I0803 17:09:14.408635  6069 net.cpp:217] pool2 needs backward computation.
I0803 17:09:14.408643  6069 net.cpp:217] relu2 needs backward computation.
I0803 17:09:14.408650  6069 net.cpp:217] conv2 needs backward computation.
I0803 17:09:14.408658  6069 net.cpp:217] pool1 needs backward computation.
I0803 17:09:14.408664  6069 net.cpp:217] norm1 needs backward computation.
I0803 17:09:14.408673  6069 net.cpp:217] relu1 needs backward computation.
I0803 17:09:14.408679  6069 net.cpp:217] conv1 needs backward computation.
I0803 17:09:14.408687  6069 net.cpp:219] label_facial_point_1_split does not need backward computation.
I0803 17:09:14.408696  6069 net.cpp:219] data_facial_point_0_split does not need backward computation.
I0803 17:09:14.408704  6069 net.cpp:219] facial_point does not need backward computation.
I0803 17:09:14.408712  6069 net.cpp:261] This network produces output local/loss
I0803 17:09:14.408720  6069 net.cpp:261] This network produces output loss
I0803 17:09:14.408727  6069 net.cpp:261] This network produces output theta_loss
I0803 17:09:14.408766  6069 net.cpp:274] Network initialization done.
I0803 17:09:14.410545  6069 solver.cpp:181] Creating test net (#0) specified by net file: train_val.prototxt
I0803 17:09:14.410629  6069 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer facial_point
I0803 17:09:14.410979  6069 net.cpp:49] Initializing net from parameters: 
name: "facial_point_net"
state {
  phase: TEST
}
layer {
  name: "facial_point"
  type: "NewFacialPointData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
  }
  new_facial_point_data_param {
    source: "test_list.txt"
    batch_size: 32
    new_height: 224
    new_width: 224
    is_color: true
    point_num: 68
    ext_scale: 1.2
    xy_mean: 0.03
    xy_std: 0.03
    wh_mean: 1
    wh_std: 0.03
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv5"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool3"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "68point"
  type: "InnerProduct"
  bottom: "fc7"
  top: "68point"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 136
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "68point"
  bottom: "label"
  top: "loss"
}
layer {
  name: "loc_reg_affine"
  type: "InnerProduct"
  bottom: "68point"
  top: "theta"
  inner_product_param {
    num_output: 6
    weight_filler {
      type: "constant"
      value: 0
    }
    bias_filler {
      type: "file"
      file: "bias_init.txt"
    }
  }
}
layer {
  name: "theta_loss"
  type: "LocLoss"
  bottom: "theta"
  top: "theta_loss"
  loss_weight: 1
  loc_loss_param {
    threshold: 1
  }
}
layer {
  name: "st_pts"
  type: "PointTransformer"
  bottom: "label"
  bottom: "theta"
  top: "local/label"
  propagate_down: false
  propagate_down: false
  pt_param {
    in_width: 224
    in_height: 224
    out_width: 224
    out_height: 224
    transform_type: AFFINE
  }
}
layer {
  name: "st_layer"
  type: "SpatialTransformer"
  bottom: "data"
  bottom: "theta"
  top: "local/data"
  st_param {
    output_H: 224
    output_W: 224
    to_compute_dU: false
  }
}
layer {
  name: "local_conv1"
  type: "Convolution"
  bottom: "local/data"
  top: "local/conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu1"
  type: "ReLU"
  bottom: "local/conv1"
  top: "local/conv1"
}
layer {
  name: "local_norm1"
  type: "LRN"
  bottom: "local/conv1"
  top: "local/norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "local_pool1"
  type: "Pooling"
  bottom: "local/norm1"
  top: "local/pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "local_conv2"
  type: "Convolution"
  bottom: "local/pool1"
  top: "local/conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu2"
  type: "ReLU"
  bottom: "local/conv2"
  top: "local/conv2"
}
layer {
  name: "local_pool2"
  type: "Pooling"
  bottom: "local/conv2"
  top: "local/pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "local_conv3"
  type: "Convolution"
  bottom: "local/pool2"
  top: "local/conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu3"
  type: "ReLU"
  bottom: "local/conv3"
  top: "local/conv3"
}
layer {
  name: "local_conv4"
  type: "Convolution"
  bottom: "local/conv3"
  top: "local/conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu4"
  type: "ReLU"
  bottom: "local/conv4"
  top: "local/conv4"
}
layer {
  name: "local_conv5"
  type: "Convolution"
  bottom: "local/conv4"
  top: "local/conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "local_relu5"
  type: "ReLU"
  bottom: "local/conv5"
  top: "local/conv5"
}
layer {
  name: "local_pool3"
  type: "Pooling"
  bottom: "local/conv5"
  top: "local/pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "local_fc6"
  type: "InnerProduct"
  bottom: "local/pool3"
  top: "local/fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "local_relu6"
  type: "ReLU"
  bottom: "local/fc6"
  top: "local/fc6"
}
layer {
  name: "local_drop6"
  type: "Dropout"
  bottom: "local/fc6"
  top: "local/fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "local_fc7"
  type: "InnerProduct"
  bottom: "local/fc6"
  top: "local/fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "local_relu7"
  type: "ReLU"
  bottom: "local/fc7"
  top: "local/fc7"
}
layer {
  name: "local_drop7"
  type: "Dropout"
  bottom: "local/fc7"
  top: "local/fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "local_68point"
  type: "InnerProduct"
  bottom: "local/fc7"
  top: "local/68point"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 136
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "local_loss"
  type: "EuclideanLoss"
  bottom: "local/68point"
  bottom: "local/label"
  top: "local/loss"
}
I0803 17:09:14.412914  6069 layer_factory.hpp:77] Creating layer facial_point
I0803 17:09:14.412941  6069 net.cpp:91] Creating Layer facial_point
I0803 17:09:14.412951  6069 net.cpp:399] facial_point -> data
I0803 17:09:14.412967  6069 net.cpp:399] facial_point -> label
I0803 17:09:14.412981  6069 new_facial_point_data_layer.cpp:187] Opening file test_list.txt
I0803 17:09:14.413302  6069 new_facial_point_data_layer.cpp:203] A total of 689 images.
I0803 17:09:14.420373  6069 new_facial_point_data_layer.cpp:263] output data size: 32,3,224,224
I0803 17:09:14.467741  6069 net.cpp:141] Setting up facial_point
I0803 17:09:14.467804  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:14.467819  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:14.467830  6069 net.cpp:156] Memory required for data: 19284992
I0803 17:09:14.467842  6069 layer_factory.hpp:77] Creating layer data_facial_point_0_split
I0803 17:09:14.467864  6069 net.cpp:91] Creating Layer data_facial_point_0_split
I0803 17:09:14.467885  6069 net.cpp:425] data_facial_point_0_split <- data
I0803 17:09:14.467900  6069 net.cpp:399] data_facial_point_0_split -> data_facial_point_0_split_0
I0803 17:09:14.467929  6069 net.cpp:399] data_facial_point_0_split -> data_facial_point_0_split_1
I0803 17:09:14.468025  6069 net.cpp:141] Setting up data_facial_point_0_split
I0803 17:09:14.468041  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:14.468050  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:14.468060  6069 net.cpp:156] Memory required for data: 57820160
I0803 17:09:14.468068  6069 layer_factory.hpp:77] Creating layer label_facial_point_1_split
I0803 17:09:14.468080  6069 net.cpp:91] Creating Layer label_facial_point_1_split
I0803 17:09:14.468088  6069 net.cpp:425] label_facial_point_1_split <- label
I0803 17:09:14.468101  6069 net.cpp:399] label_facial_point_1_split -> label_facial_point_1_split_0
I0803 17:09:14.468116  6069 net.cpp:399] label_facial_point_1_split -> label_facial_point_1_split_1
I0803 17:09:14.468204  6069 net.cpp:141] Setting up label_facial_point_1_split
I0803 17:09:14.468221  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:14.468230  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:14.468238  6069 net.cpp:156] Memory required for data: 57854976
I0803 17:09:14.468250  6069 layer_factory.hpp:77] Creating layer conv1
I0803 17:09:14.468273  6069 net.cpp:91] Creating Layer conv1
I0803 17:09:14.468283  6069 net.cpp:425] conv1 <- data_facial_point_0_split_0
I0803 17:09:14.468298  6069 net.cpp:399] conv1 -> conv1
I0803 17:09:14.468698  6069 net.cpp:141] Setting up conv1
I0803 17:09:14.468715  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:14.468727  6069 net.cpp:156] Memory required for data: 203848704
I0803 17:09:14.468744  6069 layer_factory.hpp:77] Creating layer relu1
I0803 17:09:14.468758  6069 net.cpp:91] Creating Layer relu1
I0803 17:09:14.468768  6069 net.cpp:425] relu1 <- conv1
I0803 17:09:14.468780  6069 net.cpp:386] relu1 -> conv1 (in-place)
I0803 17:09:14.468792  6069 net.cpp:141] Setting up relu1
I0803 17:09:14.468802  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:14.468811  6069 net.cpp:156] Memory required for data: 349842432
I0803 17:09:14.468818  6069 layer_factory.hpp:77] Creating layer norm1
I0803 17:09:14.468830  6069 net.cpp:91] Creating Layer norm1
I0803 17:09:14.468838  6069 net.cpp:425] norm1 <- conv1
I0803 17:09:14.468848  6069 net.cpp:399] norm1 -> norm1
I0803 17:09:14.468900  6069 net.cpp:141] Setting up norm1
I0803 17:09:14.468916  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:14.468925  6069 net.cpp:156] Memory required for data: 495836160
I0803 17:09:14.468933  6069 layer_factory.hpp:77] Creating layer pool1
I0803 17:09:14.468946  6069 net.cpp:91] Creating Layer pool1
I0803 17:09:14.468955  6069 net.cpp:425] pool1 <- norm1
I0803 17:09:14.468994  6069 net.cpp:399] pool1 -> pool1
I0803 17:09:14.469043  6069 net.cpp:141] Setting up pool1
I0803 17:09:14.469056  6069 net.cpp:148] Top shape: 32 96 37 37 (4205568)
I0803 17:09:14.469064  6069 net.cpp:156] Memory required for data: 512658432
I0803 17:09:14.469074  6069 layer_factory.hpp:77] Creating layer conv2
I0803 17:09:14.469087  6069 net.cpp:91] Creating Layer conv2
I0803 17:09:14.469097  6069 net.cpp:425] conv2 <- pool1
I0803 17:09:14.469110  6069 net.cpp:399] conv2 -> conv2
I0803 17:09:14.475410  6069 net.cpp:141] Setting up conv2
I0803 17:09:14.475436  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:14.475445  6069 net.cpp:156] Memory required for data: 548342784
I0803 17:09:14.475458  6069 layer_factory.hpp:77] Creating layer relu2
I0803 17:09:14.475471  6069 net.cpp:91] Creating Layer relu2
I0803 17:09:14.475481  6069 net.cpp:425] relu2 <- conv2
I0803 17:09:14.475492  6069 net.cpp:386] relu2 -> conv2 (in-place)
I0803 17:09:14.475503  6069 net.cpp:141] Setting up relu2
I0803 17:09:14.475513  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:14.475520  6069 net.cpp:156] Memory required for data: 584027136
I0803 17:09:14.475528  6069 layer_factory.hpp:77] Creating layer pool2
I0803 17:09:14.475538  6069 net.cpp:91] Creating Layer pool2
I0803 17:09:14.475545  6069 net.cpp:425] pool2 <- conv2
I0803 17:09:14.475556  6069 net.cpp:399] pool2 -> pool2
I0803 17:09:14.475602  6069 net.cpp:141] Setting up pool2
I0803 17:09:14.475613  6069 net.cpp:148] Top shape: 32 256 17 17 (2367488)
I0803 17:09:14.475620  6069 net.cpp:156] Memory required for data: 593497088
I0803 17:09:14.475627  6069 layer_factory.hpp:77] Creating layer conv3
I0803 17:09:14.475641  6069 net.cpp:91] Creating Layer conv3
I0803 17:09:14.475651  6069 net.cpp:425] conv3 <- pool2
I0803 17:09:14.475661  6069 net.cpp:399] conv3 -> conv3
I0803 17:09:14.487257  6069 net.cpp:141] Setting up conv3
I0803 17:09:14.487284  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:14.487293  6069 net.cpp:156] Memory required for data: 612436992
I0803 17:09:14.487311  6069 layer_factory.hpp:77] Creating layer relu3
I0803 17:09:14.487323  6069 net.cpp:91] Creating Layer relu3
I0803 17:09:14.487332  6069 net.cpp:425] relu3 <- conv3
I0803 17:09:14.487344  6069 net.cpp:386] relu3 -> conv3 (in-place)
I0803 17:09:14.487355  6069 net.cpp:141] Setting up relu3
I0803 17:09:14.487365  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:14.487375  6069 net.cpp:156] Memory required for data: 631376896
I0803 17:09:14.487383  6069 layer_factory.hpp:77] Creating layer conv4
I0803 17:09:14.487398  6069 net.cpp:91] Creating Layer conv4
I0803 17:09:14.487407  6069 net.cpp:425] conv4 <- conv3
I0803 17:09:14.487418  6069 net.cpp:399] conv4 -> conv4
I0803 17:09:14.510138  6069 net.cpp:141] Setting up conv4
I0803 17:09:14.510185  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:14.510195  6069 net.cpp:156] Memory required for data: 650316800
I0803 17:09:14.510207  6069 layer_factory.hpp:77] Creating layer relu4
I0803 17:09:14.510224  6069 net.cpp:91] Creating Layer relu4
I0803 17:09:14.510234  6069 net.cpp:425] relu4 <- conv4
I0803 17:09:14.510249  6069 net.cpp:386] relu4 -> conv4 (in-place)
I0803 17:09:14.510263  6069 net.cpp:141] Setting up relu4
I0803 17:09:14.510273  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:14.510282  6069 net.cpp:156] Memory required for data: 669256704
I0803 17:09:14.510290  6069 layer_factory.hpp:77] Creating layer conv5
I0803 17:09:14.510306  6069 net.cpp:91] Creating Layer conv5
I0803 17:09:14.510315  6069 net.cpp:425] conv5 <- conv4
I0803 17:09:14.510329  6069 net.cpp:399] conv5 -> conv5
I0803 17:09:14.533056  6069 net.cpp:141] Setting up conv5
I0803 17:09:14.533088  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:14.533097  6069 net.cpp:156] Memory required for data: 688196608
I0803 17:09:14.533114  6069 layer_factory.hpp:77] Creating layer relu5
I0803 17:09:14.533126  6069 net.cpp:91] Creating Layer relu5
I0803 17:09:14.533135  6069 net.cpp:425] relu5 <- conv5
I0803 17:09:14.533172  6069 net.cpp:386] relu5 -> conv5 (in-place)
I0803 17:09:14.533185  6069 net.cpp:141] Setting up relu5
I0803 17:09:14.533195  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:14.533202  6069 net.cpp:156] Memory required for data: 707136512
I0803 17:09:14.533210  6069 layer_factory.hpp:77] Creating layer pool3
I0803 17:09:14.533223  6069 net.cpp:91] Creating Layer pool3
I0803 17:09:14.533231  6069 net.cpp:425] pool3 <- conv5
I0803 17:09:14.533241  6069 net.cpp:399] pool3 -> pool3
I0803 17:09:14.533288  6069 net.cpp:141] Setting up pool3
I0803 17:09:14.533300  6069 net.cpp:148] Top shape: 32 512 6 6 (589824)
I0803 17:09:14.533308  6069 net.cpp:156] Memory required for data: 709495808
I0803 17:09:14.533314  6069 layer_factory.hpp:77] Creating layer fc6
I0803 17:09:14.533327  6069 net.cpp:91] Creating Layer fc6
I0803 17:09:14.533335  6069 net.cpp:425] fc6 <- pool3
I0803 17:09:14.533345  6069 net.cpp:399] fc6 -> fc6
I0803 17:09:15.747458  6069 net.cpp:141] Setting up fc6
I0803 17:09:15.747526  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:15.747535  6069 net.cpp:156] Memory required for data: 710020096
I0803 17:09:15.747560  6069 layer_factory.hpp:77] Creating layer relu6
I0803 17:09:15.747584  6069 net.cpp:91] Creating Layer relu6
I0803 17:09:15.747596  6069 net.cpp:425] relu6 <- fc6
I0803 17:09:15.747608  6069 net.cpp:386] relu6 -> fc6 (in-place)
I0803 17:09:15.747625  6069 net.cpp:141] Setting up relu6
I0803 17:09:15.747633  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:15.747640  6069 net.cpp:156] Memory required for data: 710544384
I0803 17:09:15.747647  6069 layer_factory.hpp:77] Creating layer drop6
I0803 17:09:15.747660  6069 net.cpp:91] Creating Layer drop6
I0803 17:09:15.747669  6069 net.cpp:425] drop6 <- fc6
I0803 17:09:15.747679  6069 net.cpp:386] drop6 -> fc6 (in-place)
I0803 17:09:15.747735  6069 net.cpp:141] Setting up drop6
I0803 17:09:15.747748  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:15.747756  6069 net.cpp:156] Memory required for data: 711068672
I0803 17:09:15.747761  6069 layer_factory.hpp:77] Creating layer fc7
I0803 17:09:15.747779  6069 net.cpp:91] Creating Layer fc7
I0803 17:09:15.747786  6069 net.cpp:425] fc7 <- fc6
I0803 17:09:15.747797  6069 net.cpp:399] fc7 -> fc7
I0803 17:09:15.900838  6069 net.cpp:141] Setting up fc7
I0803 17:09:15.900943  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:15.900956  6069 net.cpp:156] Memory required for data: 711592960
I0803 17:09:15.900986  6069 layer_factory.hpp:77] Creating layer relu7
I0803 17:09:15.901016  6069 net.cpp:91] Creating Layer relu7
I0803 17:09:15.901031  6069 net.cpp:425] relu7 <- fc7
I0803 17:09:15.901048  6069 net.cpp:386] relu7 -> fc7 (in-place)
I0803 17:09:15.901072  6069 net.cpp:141] Setting up relu7
I0803 17:09:15.901088  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:15.901103  6069 net.cpp:156] Memory required for data: 712117248
I0803 17:09:15.901113  6069 layer_factory.hpp:77] Creating layer drop7
I0803 17:09:15.901136  6069 net.cpp:91] Creating Layer drop7
I0803 17:09:15.901144  6069 net.cpp:425] drop7 <- fc7
I0803 17:09:15.901162  6069 net.cpp:386] drop7 -> fc7 (in-place)
I0803 17:09:15.901212  6069 net.cpp:141] Setting up drop7
I0803 17:09:15.901232  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:15.901242  6069 net.cpp:156] Memory required for data: 712641536
I0803 17:09:15.901247  6069 layer_factory.hpp:77] Creating layer 68point
I0803 17:09:15.901268  6069 net.cpp:91] Creating Layer 68point
I0803 17:09:15.901278  6069 net.cpp:425] 68point <- fc7
I0803 17:09:15.901291  6069 net.cpp:399] 68point -> 68point
I0803 17:09:15.906281  6069 net.cpp:141] Setting up 68point
I0803 17:09:15.906316  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:15.906323  6069 net.cpp:156] Memory required for data: 712658944
I0803 17:09:15.906333  6069 layer_factory.hpp:77] Creating layer 68point_68point_0_split
I0803 17:09:15.906345  6069 net.cpp:91] Creating Layer 68point_68point_0_split
I0803 17:09:15.906352  6069 net.cpp:425] 68point_68point_0_split <- 68point
I0803 17:09:15.906390  6069 net.cpp:399] 68point_68point_0_split -> 68point_68point_0_split_0
I0803 17:09:15.906404  6069 net.cpp:399] 68point_68point_0_split -> 68point_68point_0_split_1
I0803 17:09:15.906448  6069 net.cpp:141] Setting up 68point_68point_0_split
I0803 17:09:15.906472  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:15.906481  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:15.906486  6069 net.cpp:156] Memory required for data: 712693760
I0803 17:09:15.906493  6069 layer_factory.hpp:77] Creating layer loss
I0803 17:09:15.906505  6069 net.cpp:91] Creating Layer loss
I0803 17:09:15.906512  6069 net.cpp:425] loss <- 68point_68point_0_split_0
I0803 17:09:15.906520  6069 net.cpp:425] loss <- label_facial_point_1_split_0
I0803 17:09:15.906533  6069 net.cpp:399] loss -> loss
I0803 17:09:15.906572  6069 net.cpp:141] Setting up loss
I0803 17:09:15.906585  6069 net.cpp:148] Top shape: (1)
I0803 17:09:15.906591  6069 net.cpp:151]     with loss weight 1
I0803 17:09:15.906613  6069 net.cpp:156] Memory required for data: 712693764
I0803 17:09:15.906622  6069 layer_factory.hpp:77] Creating layer loc_reg_affine
I0803 17:09:15.906636  6069 net.cpp:91] Creating Layer loc_reg_affine
I0803 17:09:15.906643  6069 net.cpp:425] loc_reg_affine <- 68point_68point_0_split_1
I0803 17:09:15.906653  6069 net.cpp:399] loc_reg_affine -> theta
I0803 17:09:15.906849  6069 net.cpp:141] Setting up loc_reg_affine
I0803 17:09:15.906865  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:15.906872  6069 net.cpp:156] Memory required for data: 712694532
I0803 17:09:15.906898  6069 layer_factory.hpp:77] Creating layer theta_loc_reg_affine_0_split
I0803 17:09:15.906911  6069 net.cpp:91] Creating Layer theta_loc_reg_affine_0_split
I0803 17:09:15.906926  6069 net.cpp:425] theta_loc_reg_affine_0_split <- theta
I0803 17:09:15.906937  6069 net.cpp:399] theta_loc_reg_affine_0_split -> theta_loc_reg_affine_0_split_0
I0803 17:09:15.906957  6069 net.cpp:399] theta_loc_reg_affine_0_split -> theta_loc_reg_affine_0_split_1
I0803 17:09:15.906970  6069 net.cpp:399] theta_loc_reg_affine_0_split -> theta_loc_reg_affine_0_split_2
I0803 17:09:15.907045  6069 net.cpp:141] Setting up theta_loc_reg_affine_0_split
I0803 17:09:15.907059  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:15.907074  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:15.907083  6069 net.cpp:148] Top shape: 32 6 (192)
I0803 17:09:15.907093  6069 net.cpp:156] Memory required for data: 712696836
I0803 17:09:15.907105  6069 layer_factory.hpp:77] Creating layer theta_loss
I0803 17:09:15.907120  6069 net.cpp:91] Creating Layer theta_loss
I0803 17:09:15.907132  6069 net.cpp:425] theta_loss <- theta_loc_reg_affine_0_split_0
I0803 17:09:15.907145  6069 net.cpp:399] theta_loss -> theta_loss
I0803 17:09:15.907214  6069 net.cpp:141] Setting up theta_loss
I0803 17:09:15.907228  6069 net.cpp:148] Top shape: (1)
I0803 17:09:15.907235  6069 net.cpp:151]     with loss weight 1
I0803 17:09:15.907245  6069 net.cpp:156] Memory required for data: 712696840
I0803 17:09:15.907253  6069 layer_factory.hpp:77] Creating layer st_pts
I0803 17:09:15.907271  6069 net.cpp:91] Creating Layer st_pts
I0803 17:09:15.907280  6069 net.cpp:425] st_pts <- label_facial_point_1_split_1
I0803 17:09:15.907306  6069 net.cpp:425] st_pts <- theta_loc_reg_affine_0_split_1
I0803 17:09:15.907317  6069 net.cpp:399] st_pts -> local/label
I0803 17:09:15.907351  6069 net.cpp:141] Setting up st_pts
I0803 17:09:15.907366  6069 net.cpp:148] Top shape: 32 136 1 1 (4352)
I0803 17:09:15.907373  6069 net.cpp:156] Memory required for data: 712714248
I0803 17:09:15.907378  6069 layer_factory.hpp:77] Creating layer st_layer
I0803 17:09:15.907392  6069 net.cpp:91] Creating Layer st_layer
I0803 17:09:15.907397  6069 net.cpp:425] st_layer <- data_facial_point_0_split_1
I0803 17:09:15.907405  6069 net.cpp:425] st_layer <- theta_loc_reg_affine_0_split_2
I0803 17:09:15.907414  6069 net.cpp:399] st_layer -> local/data
I0803 17:09:15.909497  6069 net.cpp:141] Setting up st_layer
I0803 17:09:15.909523  6069 net.cpp:148] Top shape: 32 3 224 224 (4816896)
I0803 17:09:15.909529  6069 net.cpp:156] Memory required for data: 731981832
I0803 17:09:15.909536  6069 layer_factory.hpp:77] Creating layer local_conv1
I0803 17:09:15.909554  6069 net.cpp:91] Creating Layer local_conv1
I0803 17:09:15.909560  6069 net.cpp:425] local_conv1 <- local/data
I0803 17:09:15.909571  6069 net.cpp:399] local_conv1 -> local/conv1
I0803 17:09:15.909912  6069 net.cpp:141] Setting up local_conv1
I0803 17:09:15.909926  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:15.909932  6069 net.cpp:156] Memory required for data: 877975560
I0803 17:09:15.909942  6069 layer_factory.hpp:77] Creating layer local_relu1
I0803 17:09:15.909960  6069 net.cpp:91] Creating Layer local_relu1
I0803 17:09:15.909967  6069 net.cpp:425] local_relu1 <- local/conv1
I0803 17:09:15.909976  6069 net.cpp:386] local_relu1 -> local/conv1 (in-place)
I0803 17:09:15.909986  6069 net.cpp:141] Setting up local_relu1
I0803 17:09:15.909993  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:15.909999  6069 net.cpp:156] Memory required for data: 1023969288
I0803 17:09:15.910006  6069 layer_factory.hpp:77] Creating layer local_norm1
I0803 17:09:15.910017  6069 net.cpp:91] Creating Layer local_norm1
I0803 17:09:15.910022  6069 net.cpp:425] local_norm1 <- local/conv1
I0803 17:09:15.910032  6069 net.cpp:399] local_norm1 -> local/norm1
I0803 17:09:15.910073  6069 net.cpp:141] Setting up local_norm1
I0803 17:09:15.910084  6069 net.cpp:148] Top shape: 32 96 109 109 (36498432)
I0803 17:09:15.910089  6069 net.cpp:156] Memory required for data: 1169963016
I0803 17:09:15.910095  6069 layer_factory.hpp:77] Creating layer local_pool1
I0803 17:09:15.910106  6069 net.cpp:91] Creating Layer local_pool1
I0803 17:09:15.910112  6069 net.cpp:425] local_pool1 <- local/norm1
I0803 17:09:15.910122  6069 net.cpp:399] local_pool1 -> local/pool1
I0803 17:09:15.910158  6069 net.cpp:141] Setting up local_pool1
I0803 17:09:15.910169  6069 net.cpp:148] Top shape: 32 96 37 37 (4205568)
I0803 17:09:15.910176  6069 net.cpp:156] Memory required for data: 1186785288
I0803 17:09:15.910181  6069 layer_factory.hpp:77] Creating layer local_conv2
I0803 17:09:15.910194  6069 net.cpp:91] Creating Layer local_conv2
I0803 17:09:15.910200  6069 net.cpp:425] local_conv2 <- local/pool1
I0803 17:09:15.910212  6069 net.cpp:399] local_conv2 -> local/conv2
I0803 17:09:15.916225  6069 net.cpp:141] Setting up local_conv2
I0803 17:09:15.916252  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:15.916260  6069 net.cpp:156] Memory required for data: 1222469640
I0803 17:09:15.916271  6069 layer_factory.hpp:77] Creating layer local_relu2
I0803 17:09:15.916285  6069 net.cpp:91] Creating Layer local_relu2
I0803 17:09:15.916292  6069 net.cpp:425] local_relu2 <- local/conv2
I0803 17:09:15.916302  6069 net.cpp:386] local_relu2 -> local/conv2 (in-place)
I0803 17:09:15.916313  6069 net.cpp:141] Setting up local_relu2
I0803 17:09:15.916322  6069 net.cpp:148] Top shape: 32 256 33 33 (8921088)
I0803 17:09:15.916329  6069 net.cpp:156] Memory required for data: 1258153992
I0803 17:09:15.916337  6069 layer_factory.hpp:77] Creating layer local_pool2
I0803 17:09:15.916349  6069 net.cpp:91] Creating Layer local_pool2
I0803 17:09:15.916357  6069 net.cpp:425] local_pool2 <- local/conv2
I0803 17:09:15.916366  6069 net.cpp:399] local_pool2 -> local/pool2
I0803 17:09:15.916419  6069 net.cpp:141] Setting up local_pool2
I0803 17:09:15.916434  6069 net.cpp:148] Top shape: 32 256 17 17 (2367488)
I0803 17:09:15.916440  6069 net.cpp:156] Memory required for data: 1267623944
I0803 17:09:15.916446  6069 layer_factory.hpp:77] Creating layer local_conv3
I0803 17:09:15.916461  6069 net.cpp:91] Creating Layer local_conv3
I0803 17:09:15.916470  6069 net.cpp:425] local_conv3 <- local/pool2
I0803 17:09:15.916481  6069 net.cpp:399] local_conv3 -> local/conv3
I0803 17:09:15.927409  6069 net.cpp:141] Setting up local_conv3
I0803 17:09:15.927467  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:15.927484  6069 net.cpp:156] Memory required for data: 1286563848
I0803 17:09:15.927525  6069 layer_factory.hpp:77] Creating layer local_relu3
I0803 17:09:15.927551  6069 net.cpp:91] Creating Layer local_relu3
I0803 17:09:15.927572  6069 net.cpp:425] local_relu3 <- local/conv3
I0803 17:09:15.927585  6069 net.cpp:386] local_relu3 -> local/conv3 (in-place)
I0803 17:09:15.927608  6069 net.cpp:141] Setting up local_relu3
I0803 17:09:15.927624  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:15.927635  6069 net.cpp:156] Memory required for data: 1305503752
I0803 17:09:15.927647  6069 layer_factory.hpp:77] Creating layer local_conv4
I0803 17:09:15.927670  6069 net.cpp:91] Creating Layer local_conv4
I0803 17:09:15.927678  6069 net.cpp:425] local_conv4 <- local/conv3
I0803 17:09:15.927690  6069 net.cpp:399] local_conv4 -> local/conv4
I0803 17:09:15.953919  6069 net.cpp:141] Setting up local_conv4
I0803 17:09:15.954018  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:15.954028  6069 net.cpp:156] Memory required for data: 1324443656
I0803 17:09:15.954080  6069 layer_factory.hpp:77] Creating layer local_relu4
I0803 17:09:15.954141  6069 net.cpp:91] Creating Layer local_relu4
I0803 17:09:15.954175  6069 net.cpp:425] local_relu4 <- local/conv4
I0803 17:09:15.954191  6069 net.cpp:386] local_relu4 -> local/conv4 (in-place)
I0803 17:09:15.954208  6069 net.cpp:141] Setting up local_relu4
I0803 17:09:15.954217  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:15.954226  6069 net.cpp:156] Memory required for data: 1343383560
I0803 17:09:15.954232  6069 layer_factory.hpp:77] Creating layer local_conv5
I0803 17:09:15.954263  6069 net.cpp:91] Creating Layer local_conv5
I0803 17:09:15.954274  6069 net.cpp:425] local_conv5 <- local/conv4
I0803 17:09:15.954285  6069 net.cpp:399] local_conv5 -> local/conv5
I0803 17:09:15.975741  6069 net.cpp:141] Setting up local_conv5
I0803 17:09:15.975785  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:15.975793  6069 net.cpp:156] Memory required for data: 1362323464
I0803 17:09:15.975808  6069 layer_factory.hpp:77] Creating layer local_relu5
I0803 17:09:15.975826  6069 net.cpp:91] Creating Layer local_relu5
I0803 17:09:15.975836  6069 net.cpp:425] local_relu5 <- local/conv5
I0803 17:09:15.975848  6069 net.cpp:386] local_relu5 -> local/conv5 (in-place)
I0803 17:09:15.975864  6069 net.cpp:141] Setting up local_relu5
I0803 17:09:15.975873  6069 net.cpp:148] Top shape: 32 512 17 17 (4734976)
I0803 17:09:15.975881  6069 net.cpp:156] Memory required for data: 1381263368
I0803 17:09:15.975888  6069 layer_factory.hpp:77] Creating layer local_pool3
I0803 17:09:15.975914  6069 net.cpp:91] Creating Layer local_pool3
I0803 17:09:15.975929  6069 net.cpp:425] local_pool3 <- local/conv5
I0803 17:09:15.975940  6069 net.cpp:399] local_pool3 -> local/pool3
I0803 17:09:15.975989  6069 net.cpp:141] Setting up local_pool3
I0803 17:09:15.976002  6069 net.cpp:148] Top shape: 32 512 6 6 (589824)
I0803 17:09:15.976009  6069 net.cpp:156] Memory required for data: 1383622664
I0803 17:09:15.976016  6069 layer_factory.hpp:77] Creating layer local_fc6
I0803 17:09:15.976032  6069 net.cpp:91] Creating Layer local_fc6
I0803 17:09:15.976039  6069 net.cpp:425] local_fc6 <- local/pool3
I0803 17:09:15.976052  6069 net.cpp:399] local_fc6 -> local/fc6
I0803 17:09:16.648210  6069 net.cpp:141] Setting up local_fc6
I0803 17:09:16.648284  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:16.648298  6069 net.cpp:156] Memory required for data: 1384146952
I0803 17:09:16.648322  6069 layer_factory.hpp:77] Creating layer local_relu6
I0803 17:09:16.648350  6069 net.cpp:91] Creating Layer local_relu6
I0803 17:09:16.648360  6069 net.cpp:425] local_relu6 <- local/fc6
I0803 17:09:16.648375  6069 net.cpp:386] local_relu6 -> local/fc6 (in-place)
I0803 17:09:16.648394  6069 net.cpp:141] Setting up local_relu6
I0803 17:09:16.648403  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:16.648416  6069 net.cpp:156] Memory required for data: 1384671240
I0803 17:09:16.648424  6069 layer_factory.hpp:77] Creating layer local_drop6
I0803 17:09:16.648478  6069 net.cpp:91] Creating Layer local_drop6
I0803 17:09:16.648493  6069 net.cpp:425] local_drop6 <- local/fc6
I0803 17:09:16.648505  6069 net.cpp:386] local_drop6 -> local/fc6 (in-place)
I0803 17:09:16.648617  6069 net.cpp:141] Setting up local_drop6
I0803 17:09:16.648632  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:16.648638  6069 net.cpp:156] Memory required for data: 1385195528
I0803 17:09:16.648645  6069 layer_factory.hpp:77] Creating layer local_fc7
I0803 17:09:16.648664  6069 net.cpp:91] Creating Layer local_fc7
I0803 17:09:16.648672  6069 net.cpp:425] local_fc7 <- local/fc6
I0803 17:09:16.648684  6069 net.cpp:399] local_fc7 -> local/fc7
I0803 17:09:16.799685  6069 net.cpp:141] Setting up local_fc7
I0803 17:09:16.799742  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:16.799752  6069 net.cpp:156] Memory required for data: 1385719816
I0803 17:09:16.799767  6069 layer_factory.hpp:77] Creating layer local_relu7
I0803 17:09:16.799784  6069 net.cpp:91] Creating Layer local_relu7
I0803 17:09:16.799794  6069 net.cpp:425] local_relu7 <- local/fc7
I0803 17:09:16.799808  6069 net.cpp:386] local_relu7 -> local/fc7 (in-place)
I0803 17:09:16.799823  6069 net.cpp:141] Setting up local_relu7
I0803 17:09:16.799831  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:16.799839  6069 net.cpp:156] Memory required for data: 1386244104
I0803 17:09:16.799845  6069 layer_factory.hpp:77] Creating layer local_drop7
I0803 17:09:16.799859  6069 net.cpp:91] Creating Layer local_drop7
I0803 17:09:16.799866  6069 net.cpp:425] local_drop7 <- local/fc7
I0803 17:09:16.799875  6069 net.cpp:386] local_drop7 -> local/fc7 (in-place)
I0803 17:09:16.799913  6069 net.cpp:141] Setting up local_drop7
I0803 17:09:16.799926  6069 net.cpp:148] Top shape: 32 4096 (131072)
I0803 17:09:16.799932  6069 net.cpp:156] Memory required for data: 1386768392
I0803 17:09:16.799938  6069 layer_factory.hpp:77] Creating layer local_68point
I0803 17:09:16.799953  6069 net.cpp:91] Creating Layer local_68point
I0803 17:09:16.799962  6069 net.cpp:425] local_68point <- local/fc7
I0803 17:09:16.799973  6069 net.cpp:399] local_68point -> local/68point
I0803 17:09:16.805052  6069 net.cpp:141] Setting up local_68point
I0803 17:09:16.805074  6069 net.cpp:148] Top shape: 32 136 (4352)
I0803 17:09:16.805083  6069 net.cpp:156] Memory required for data: 1386785800
I0803 17:09:16.805115  6069 layer_factory.hpp:77] Creating layer local_loss
I0803 17:09:16.805130  6069 net.cpp:91] Creating Layer local_loss
I0803 17:09:16.805137  6069 net.cpp:425] local_loss <- local/68point
I0803 17:09:16.805145  6069 net.cpp:425] local_loss <- local/label
I0803 17:09:16.805155  6069 net.cpp:399] local_loss -> local/loss
I0803 17:09:16.805204  6069 net.cpp:141] Setting up local_loss
I0803 17:09:16.805218  6069 net.cpp:148] Top shape: (1)
I0803 17:09:16.805224  6069 net.cpp:151]     with loss weight 1
I0803 17:09:16.805245  6069 net.cpp:156] Memory required for data: 1386785804
I0803 17:09:16.805253  6069 net.cpp:217] local_loss needs backward computation.
I0803 17:09:16.805261  6069 net.cpp:217] local_68point needs backward computation.
I0803 17:09:16.805270  6069 net.cpp:217] local_drop7 needs backward computation.
I0803 17:09:16.805279  6069 net.cpp:217] local_relu7 needs backward computation.
I0803 17:09:16.805289  6069 net.cpp:217] local_fc7 needs backward computation.
I0803 17:09:16.805299  6069 net.cpp:217] local_drop6 needs backward computation.
I0803 17:09:16.805311  6069 net.cpp:217] local_relu6 needs backward computation.
I0803 17:09:16.805322  6069 net.cpp:217] local_fc6 needs backward computation.
I0803 17:09:16.805332  6069 net.cpp:217] local_pool3 needs backward computation.
I0803 17:09:16.805344  6069 net.cpp:217] local_relu5 needs backward computation.
I0803 17:09:16.805354  6069 net.cpp:217] local_conv5 needs backward computation.
I0803 17:09:16.805366  6069 net.cpp:217] local_relu4 needs backward computation.
I0803 17:09:16.805375  6069 net.cpp:217] local_conv4 needs backward computation.
I0803 17:09:16.805382  6069 net.cpp:217] local_relu3 needs backward computation.
I0803 17:09:16.805430  6069 net.cpp:217] local_conv3 needs backward computation.
I0803 17:09:16.805452  6069 net.cpp:217] local_pool2 needs backward computation.
I0803 17:09:16.805461  6069 net.cpp:217] local_relu2 needs backward computation.
I0803 17:09:16.805474  6069 net.cpp:217] local_conv2 needs backward computation.
I0803 17:09:16.805485  6069 net.cpp:217] local_pool1 needs backward computation.
I0803 17:09:16.805492  6069 net.cpp:217] local_norm1 needs backward computation.
I0803 17:09:16.805513  6069 net.cpp:217] local_relu1 needs backward computation.
I0803 17:09:16.805531  6069 net.cpp:217] local_conv1 needs backward computation.
I0803 17:09:16.805546  6069 net.cpp:217] st_layer needs backward computation.
I0803 17:09:16.805558  6069 net.cpp:217] st_pts needs backward computation.
I0803 17:09:16.805568  6069 net.cpp:217] theta_loss needs backward computation.
I0803 17:09:16.805579  6069 net.cpp:217] theta_loc_reg_affine_0_split needs backward computation.
I0803 17:09:16.805596  6069 net.cpp:217] loc_reg_affine needs backward computation.
I0803 17:09:16.805613  6069 net.cpp:217] loss needs backward computation.
I0803 17:09:16.805622  6069 net.cpp:217] 68point_68point_0_split needs backward computation.
I0803 17:09:16.805634  6069 net.cpp:217] 68point needs backward computation.
I0803 17:09:16.805652  6069 net.cpp:217] drop7 needs backward computation.
I0803 17:09:16.805660  6069 net.cpp:217] relu7 needs backward computation.
I0803 17:09:16.805676  6069 net.cpp:217] fc7 needs backward computation.
I0803 17:09:16.805691  6069 net.cpp:217] drop6 needs backward computation.
I0803 17:09:16.805708  6069 net.cpp:217] relu6 needs backward computation.
I0803 17:09:16.805716  6069 net.cpp:217] fc6 needs backward computation.
I0803 17:09:16.805724  6069 net.cpp:217] pool3 needs backward computation.
I0803 17:09:16.805737  6069 net.cpp:217] relu5 needs backward computation.
I0803 17:09:16.805743  6069 net.cpp:217] conv5 needs backward computation.
I0803 17:09:16.805749  6069 net.cpp:217] relu4 needs backward computation.
I0803 17:09:16.805755  6069 net.cpp:217] conv4 needs backward computation.
I0803 17:09:16.805762  6069 net.cpp:217] relu3 needs backward computation.
I0803 17:09:16.805775  6069 net.cpp:217] conv3 needs backward computation.
I0803 17:09:16.805788  6069 net.cpp:217] pool2 needs backward computation.
I0803 17:09:16.805794  6069 net.cpp:217] relu2 needs backward computation.
I0803 17:09:16.805806  6069 net.cpp:217] conv2 needs backward computation.
I0803 17:09:16.805819  6069 net.cpp:217] pool1 needs backward computation.
I0803 17:09:16.805843  6069 net.cpp:217] norm1 needs backward computation.
I0803 17:09:16.805850  6069 net.cpp:217] relu1 needs backward computation.
I0803 17:09:16.805856  6069 net.cpp:217] conv1 needs backward computation.
I0803 17:09:16.805863  6069 net.cpp:219] label_facial_point_1_split does not need backward computation.
I0803 17:09:16.805871  6069 net.cpp:219] data_facial_point_0_split does not need backward computation.
I0803 17:09:16.805878  6069 net.cpp:219] facial_point does not need backward computation.
I0803 17:09:16.805884  6069 net.cpp:261] This network produces output local/loss
I0803 17:09:16.805891  6069 net.cpp:261] This network produces output loss
I0803 17:09:16.805897  6069 net.cpp:261] This network produces output theta_loss
I0803 17:09:16.805930  6069 net.cpp:274] Network initialization done.
I0803 17:09:16.806197  6069 solver.cpp:60] Solver scaffolding done.
I0803 17:09:16.807270  6069 caffe.cpp:129] Finetuning from ./model/init_v2.0.caffemodel
I0803 17:09:21.038686  6069 net.cpp:752] Ignoring source layer input
I0803 17:09:21.038718  6069 net.cpp:752] Ignoring source layer data_input_0_split
I0803 17:09:21.113927  6069 net.cpp:752] Ignoring source layer loc_reg_
I0803 17:09:25.448953  6069 net.cpp:752] Ignoring source layer input
I0803 17:09:25.448998  6069 net.cpp:752] Ignoring source layer data_input_0_split
I0803 17:09:25.524188  6069 net.cpp:752] Ignoring source layer loc_reg_
I0803 17:09:25.620723  6069 caffe.cpp:219] Starting Optimization
I0803 17:09:25.620816  6069 solver.cpp:279] Solving facial_point_net
I0803 17:09:25.620829  6069 solver.cpp:280] Learning Rate Policy: poly
I0803 17:09:25.624275  6069 solver.cpp:337] Iteration 0, Testing net (#0)
I0803 17:09:26.377992  6069 blocking_queue.cpp:50] Data layer prefetch queue empty
I0803 17:10:20.430999  6069 solver.cpp:404]     Test net output #0: local/loss = 3.82979 (* 1 = 3.82979 loss)
I0803 17:10:20.431133  6069 solver.cpp:404]     Test net output #1: loss = 3.82978 (* 1 = 3.82978 loss)
I0803 17:10:20.431145  6069 solver.cpp:404]     Test net output #2: theta_loss = 0 (* 1 = 0 loss)
I0803 17:10:21.257431  6069 solver.cpp:228] Iteration 0, loss = 7.54299
I0803 17:10:21.257477  6069 solver.cpp:244]     Train net output #0: local/loss = 3.75894 (* 1 = 3.75894 loss)
I0803 17:10:21.257488  6069 solver.cpp:244]     Train net output #1: loss = 3.78406 (* 1 = 3.78406 loss)
I0803 17:10:21.257498  6069 solver.cpp:244]     Train net output #2: theta_loss = 0 (* 1 = 0 loss)
I0803 17:10:21.257522  6069 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0803 17:10:21.650944  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.7034 > 10) by scale factor 0.305779
I0803 17:10:22.939707  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 34.1304 > 10) by scale factor 0.292994
I0803 17:10:24.215848  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 35.2133 > 10) by scale factor 0.283984
I0803 17:10:24.965646  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 35.7996 > 10) by scale factor 0.279333
I0803 17:10:25.577056  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 34.9994 > 10) by scale factor 0.285719
I0803 17:10:26.838871  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 34.4766 > 10) by scale factor 0.290052
I0803 17:10:28.100098  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 33.8149 > 10) by scale factor 0.295727
I0803 17:10:28.712246  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 34.8238 > 10) by scale factor 0.28716
I0803 17:10:29.324282  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 35.6487 > 10) by scale factor 0.280515
I0803 17:10:29.936513  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 34.5152 > 10) by scale factor 0.289728
I0803 17:10:30.547477  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 33.4232 > 10) by scale factor 0.299193
I0803 17:10:31.160889  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.6587 > 10) by scale factor 0.306197
I0803 17:10:31.773277  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.5414 > 10) by scale factor 0.307301
I0803 17:10:32.386631  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.0459 > 10) by scale factor 0.312053
I0803 17:10:33.441658  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.2339 > 10) by scale factor 0.310233
I0803 17:10:34.716470  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.3485 > 10) by scale factor 0.329506
I0803 17:10:35.993150  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.5029 > 10) by scale factor 0.327837
I0803 17:10:37.262976  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 29.936 > 10) by scale factor 0.334046
I0803 17:10:38.537472  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 29.8048 > 10) by scale factor 0.335516
I0803 17:10:39.807945  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.5807 > 10) by scale factor 0.349886
I0803 17:10:40.691922  6069 solver.cpp:228] Iteration 20, loss = 6.05361
I0803 17:10:40.691975  6069 solver.cpp:244]     Train net output #0: local/loss = 3.56944 (* 1 = 3.56944 loss)
I0803 17:10:40.691987  6069 solver.cpp:244]     Train net output #1: loss = 2.48417 (* 1 = 2.48417 loss)
I0803 17:10:40.691997  6069 solver.cpp:244]     Train net output #2: theta_loss = 0 (* 1 = 0 loss)
I0803 17:10:40.692008  6069 sgd_solver.cpp:106] Iteration 20, lr = 9.99975e-05
I0803 17:10:41.088032  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.8074 > 10) by scale factor 0.347133
I0803 17:10:42.359841  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.5695 > 10) by scale factor 0.350024
I0803 17:10:43.636067  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.8323 > 10) by scale factor 0.372685
I0803 17:10:44.909252  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.1291 > 10) by scale factor 0.368608
I0803 17:10:45.711002  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.3098 > 10) by scale factor 0.366168
I0803 17:10:46.986436  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.2641 > 10) by scale factor 0.380748
I0803 17:10:48.261504  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.6233 > 10) by scale factor 0.375611
I0803 17:10:49.536329  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 25.4962 > 10) by scale factor 0.392216
I0803 17:10:50.808004  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.444 > 10) by scale factor 0.364378
I0803 17:10:51.976349  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.7612 > 10) by scale factor 0.30524
I0803 17:10:52.588284  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.658 > 10) by scale factor 0.348942
I0803 17:10:53.564332  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.3621 > 10) by scale factor 0.365468
I0803 17:10:54.836941  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.4121 > 10) by scale factor 0.364803
I0803 17:10:56.114822  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.7293 > 10) by scale factor 0.374121
I0803 17:10:57.392750  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.5718 > 10) by scale factor 0.349996
I0803 17:10:58.669114  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 29.1756 > 10) by scale factor 0.342752
I0803 17:10:59.949757  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.3024 > 10) by scale factor 0.366268
I0803 17:11:01.225069  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 25.3744 > 10) by scale factor 0.394099
I0803 17:11:02.497763  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.1848 > 10) by scale factor 0.354801
I0803 17:11:03.328454  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.2307 > 10) by scale factor 0.367232
I0803 17:11:03.773659  6069 solver.cpp:228] Iteration 40, loss = 5.69334
I0803 17:11:03.773710  6069 solver.cpp:244]     Train net output #0: local/loss = 4.26087 (* 1 = 4.26087 loss)
I0803 17:11:03.773721  6069 solver.cpp:244]     Train net output #1: loss = 1.43247 (* 1 = 1.43247 loss)
I0803 17:11:03.773730  6069 solver.cpp:244]     Train net output #2: theta_loss = 0 (* 1 = 0 loss)
I0803 17:11:03.773742  6069 sgd_solver.cpp:106] Iteration 40, lr = 9.9995e-05
I0803 17:11:03.940326  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.1423 > 10) by scale factor 0.33176
I0803 17:11:04.551815  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.247 > 10) by scale factor 0.367012
I0803 17:11:05.371373  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.5915 > 10) by scale factor 0.423881
I0803 17:11:06.650827  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.2535 > 10) by scale factor 0.412312
I0803 17:11:07.916669  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 25.384 > 10) by scale factor 0.393948
I0803 17:11:09.185793  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.5982 > 10) by scale factor 0.406534
I0803 17:11:10.451769  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.3392 > 10) by scale factor 0.41086
I0803 17:11:11.729423  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.383 > 10) by scale factor 0.427661
I0803 17:11:12.752209  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 35.9677 > 10) by scale factor 0.278028
I0803 17:11:13.364157  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.1693 > 10) by scale factor 0.451075
I0803 17:11:13.978464  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.6734 > 10) by scale factor 0.422416
I0803 17:11:14.591616  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.0144 > 10) by scale factor 0.454248
I0803 17:11:15.764730  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.1924 > 10) by scale factor 0.471867
I0803 17:11:17.033308  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.7127 > 10) by scale factor 0.460559
I0803 17:11:18.311480  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.3677 > 10) by scale factor 0.516324
I0803 17:11:19.582784  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.7103 > 10) by scale factor 0.374387
I0803 17:11:20.867254  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.5711 > 10) by scale factor 0.510958
I0803 17:11:22.134395  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.1589 > 10) by scale factor 0.451287
I0803 17:11:23.398078  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.4494 > 10) by scale factor 0.466215
I0803 17:11:24.660509  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.807 > 10) by scale factor 0.438462
I0803 17:11:25.536084  6069 solver.cpp:228] Iteration 60, loss = 2.16368
I0803 17:11:25.536134  6069 solver.cpp:244]     Train net output #0: local/loss = 1.53554 (* 1 = 1.53554 loss)
I0803 17:11:25.536145  6069 solver.cpp:244]     Train net output #1: loss = 0.62814 (* 1 = 0.62814 loss)
I0803 17:11:25.536154  6069 solver.cpp:244]     Train net output #2: theta_loss = 0 (* 1 = 0 loss)
I0803 17:11:25.536167  6069 sgd_solver.cpp:106] Iteration 60, lr = 9.99925e-05
I0803 17:11:25.923985  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.033 > 10) by scale factor 0.499175
I0803 17:11:27.187531  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.4791 > 10) by scale factor 0.444858
I0803 17:11:28.452956  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.4951 > 10) by scale factor 0.444541
I0803 17:11:29.716363  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.8492 > 10) by scale factor 0.419301
I0803 17:11:30.979907  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.417 > 10) by scale factor 0.57415
I0803 17:11:32.246907  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.6668 > 10) by scale factor 0.566033
I0803 17:11:33.517659  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.4265 > 10) by scale factor 0.445902
I0803 17:11:34.787766  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.3087 > 10) by scale factor 0.577745
I0803 17:11:36.061684  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.5869 > 10) by scale factor 0.510546
I0803 17:11:37.332128  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.0715 > 10) by scale factor 0.498218
I0803 17:11:38.063163  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.352 > 10) by scale factor 0.379477
I0803 17:11:39.328830  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.3864 > 10) by scale factor 0.467587
I0803 17:11:40.598675  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.9238 > 10) by scale factor 0.401223
I0803 17:11:41.869508  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.8043 > 10) by scale factor 0.420093
I0803 17:11:43.139941  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 25.3173 > 10) by scale factor 0.394987
I0803 17:11:44.415761  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.6174 > 10) by scale factor 0.56762
I0803 17:11:45.694732  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.65 > 10) by scale factor 0.638978
I0803 17:11:46.978214  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.1567 > 10) by scale factor 0.618937
I0803 17:11:48.250083  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.8244 > 10) by scale factor 0.779766
I0803 17:11:49.525197  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7549 > 10) by scale factor 0.929812
I0803 17:11:50.401310  6069 solver.cpp:228] Iteration 80, loss = 1.41909
I0803 17:11:50.401374  6069 solver.cpp:244]     Train net output #0: local/loss = 0.985621 (* 1 = 0.985621 loss)
I0803 17:11:50.401387  6069 solver.cpp:244]     Train net output #1: loss = 0.433465 (* 1 = 0.433465 loss)
I0803 17:11:50.401398  6069 solver.cpp:244]     Train net output #2: theta_loss = 0 (* 1 = 0 loss)
I0803 17:11:50.401412  6069 sgd_solver.cpp:106] Iteration 80, lr = 9.999e-05
I0803 17:11:50.793946  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.6567 > 10) by scale factor 0.535999
I0803 17:11:52.064267  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4668 > 10) by scale factor 0.955403
I0803 17:11:53.338541  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.5013 > 10) by scale factor 0.799914
I0803 17:11:54.605115  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.5889 > 10) by scale factor 0.56854
I0803 17:11:55.872243  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.9586 > 10) by scale factor 0.716405
I0803 17:11:57.151983  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4116 > 10) by scale factor 0.960471
I0803 17:11:59.684167  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.4667 > 10) by scale factor 0.802134
I0803 17:12:00.723954  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.2534 > 10) by scale factor 0.888617
I0803 17:12:01.336125  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.1064 > 10) by scale factor 0.552291
I0803 17:12:03.165647  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4545 > 10) by scale factor 0.956529
I0803 17:12:05.687561  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.677 > 10) by scale factor 0.788832
I0803 17:12:06.335140  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.8487 > 10) by scale factor 0.630966
I0803 17:12:09.465426  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8135 > 10) by scale factor 0.846491
I0803 17:12:11.192075  6069 solver.cpp:228] Iteration 100, loss = 0.635838
I0803 17:12:11.192142  6069 solver.cpp:244]     Train net output #0: local/loss = 0.27723 (* 1 = 0.27723 loss)
I0803 17:12:11.192157  6069 solver.cpp:244]     Train net output #1: loss = 0.357793 (* 1 = 0.357793 loss)
I0803 17:12:11.192167  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000813987 (* 1 = 0.000813987 loss)
I0803 17:12:11.192178  6069 sgd_solver.cpp:106] Iteration 100, lr = 9.99875e-05
I0803 17:12:11.595098  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.0576 > 10) by scale factor 0.829352
I0803 17:12:16.644982  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2143 > 10) by scale factor 0.979019
I0803 17:12:21.696118  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2111 > 10) by scale factor 0.979324
I0803 17:12:22.967892  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.7932 > 10) by scale factor 0.847945
I0803 17:12:25.492383  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.4113 > 10) by scale factor 0.805718
I0803 17:12:26.419790  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.4644 > 10) by scale factor 0.691353
I0803 17:12:27.643183  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.3366 > 10) by scale factor 0.810598
I0803 17:12:28.908311  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.8323 > 10) by scale factor 0.779281
I0803 17:12:30.172945  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.9756 > 10) by scale factor 0.667751
I0803 17:12:30.826287  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.0748 > 10) by scale factor 0.710488
I0803 17:12:31.438684  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.1208 > 10) by scale factor 0.661342
I0803 17:12:32.052065  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.7551 > 10) by scale factor 0.677733
I0803 17:12:33.021123  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 25.3878 > 10) by scale factor 0.39389
I0803 17:12:33.895368  6069 solver.cpp:228] Iteration 120, loss = 1.11407
I0803 17:12:33.895433  6069 solver.cpp:244]     Train net output #0: local/loss = 0.859989 (* 1 = 0.859989 loss)
I0803 17:12:33.895447  6069 solver.cpp:244]     Train net output #1: loss = 0.253868 (* 1 = 0.253868 loss)
I0803 17:12:33.895457  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000207794 (* 1 = 0.000207794 loss)
I0803 17:12:33.895468  6069 sgd_solver.cpp:106] Iteration 120, lr = 9.9985e-05
I0803 17:12:34.296887  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.9931 > 10) by scale factor 0.500174
I0803 17:12:35.574297  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.6636 > 10) by scale factor 0.405456
I0803 17:12:36.846016  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.6756 > 10) by scale factor 0.405258
I0803 17:12:37.723825  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 41.5703 > 10) by scale factor 0.240557
I0803 17:12:38.917758  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 39.3819 > 10) by scale factor 0.253924
I0803 17:12:40.182612  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 33.4418 > 10) by scale factor 0.299027
I0803 17:12:41.452488  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.3937 > 10) by scale factor 0.329016
I0803 17:12:42.125329  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 41.1604 > 10) by scale factor 0.242952
I0803 17:12:43.391706  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.1999 > 10) by scale factor 0.495053
I0803 17:12:44.664520  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.3574 > 10) by scale factor 0.696504
I0803 17:12:45.932287  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.5872 > 10) by scale factor 0.863023
I0803 17:12:47.206972  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.3413 > 10) by scale factor 0.966993
I0803 17:12:49.733770  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.501 > 10) by scale factor 0.740685
I0803 17:12:51.006314  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.3695 > 10) by scale factor 0.74797
I0803 17:12:52.279857  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.1789 > 10) by scale factor 0.821095
I0803 17:12:53.497823  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.3563 > 10) by scale factor 0.611385
I0803 17:12:54.644768  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.3979 > 10) by scale factor 0.877358
I0803 17:12:55.913125  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.5021 > 10) by scale factor 0.869405
I0803 17:12:57.183812  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.1926 > 10) by scale factor 0.549675
I0803 17:12:58.060164  6069 solver.cpp:228] Iteration 140, loss = 0.540152
I0803 17:12:58.060221  6069 solver.cpp:244]     Train net output #0: local/loss = 0.31787 (* 1 = 0.31787 loss)
I0803 17:12:58.060235  6069 solver.cpp:244]     Train net output #1: loss = 0.217524 (* 1 = 0.217524 loss)
I0803 17:12:58.060245  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00475805 (* 1 = 0.00475805 loss)
I0803 17:12:58.060259  6069 sgd_solver.cpp:106] Iteration 140, lr = 9.99825e-05
I0803 17:12:58.453673  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.9339 > 10) by scale factor 0.557605
I0803 17:12:59.722976  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.167 > 10) by scale factor 0.618543
I0803 17:13:00.995693  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.4615 > 10) by scale factor 0.64677
I0803 17:13:02.276389  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.8481 > 10) by scale factor 0.778325
I0803 17:13:03.549634  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.9801 > 10) by scale factor 0.770408
I0803 17:13:04.818894  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.5755 > 10) by scale factor 0.686083
I0803 17:13:06.090091  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.6645 > 10) by scale factor 0.731822
I0803 17:13:07.360841  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.6252 > 10) by scale factor 0.63999
I0803 17:13:08.629091  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.7593 > 10) by scale factor 0.783743
I0803 17:13:09.898654  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.9679 > 10) by scale factor 0.715927
I0803 17:13:11.163760  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.5134 > 10) by scale factor 0.689018
I0803 17:13:12.421340  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.3158 > 10) by scale factor 0.698528
I0803 17:13:13.687367  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.6272 > 10) by scale factor 0.601425
I0803 17:13:14.943300  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.3348 > 10) by scale factor 0.697602
I0803 17:13:16.200278  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.0373 > 10) by scale factor 0.665013
I0803 17:13:17.462908  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.7935 > 10) by scale factor 0.675972
I0803 17:13:18.729367  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.9148 > 10) by scale factor 0.628348
I0803 17:13:19.991612  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.4463 > 10) by scale factor 0.647404
I0803 17:13:21.264056  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.5387 > 10) by scale factor 0.643554
I0803 17:13:22.533181  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.8017 > 10) by scale factor 0.595177
I0803 17:13:23.447868  6069 solver.cpp:228] Iteration 160, loss = 0.856173
I0803 17:13:23.447921  6069 solver.cpp:244]     Train net output #0: local/loss = 0.639311 (* 1 = 0.639311 loss)
I0803 17:13:23.447932  6069 solver.cpp:244]     Train net output #1: loss = 0.20935 (* 1 = 0.20935 loss)
I0803 17:13:23.447942  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00751173 (* 1 = 0.00751173 loss)
I0803 17:13:23.447952  6069 sgd_solver.cpp:106] Iteration 160, lr = 9.998e-05
I0803 17:13:23.799599  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.6361 > 10) by scale factor 0.509266
I0803 17:13:25.078007  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.6626 > 10) by scale factor 0.566168
I0803 17:13:26.351408  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.9332 > 10) by scale factor 0.528172
I0803 17:13:27.624009  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.0364 > 10) by scale factor 0.416035
I0803 17:13:28.902156  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 25.666 > 10) by scale factor 0.38962
I0803 17:13:30.167502  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.7515 > 10) by scale factor 0.325188
I0803 17:13:31.438247  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.0697 > 10) by scale factor 0.41546
I0803 17:13:32.705642  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.8263 > 10) by scale factor 0.402799
I0803 17:13:33.971200  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.823 > 10) by scale factor 0.402853
I0803 17:13:35.241433  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.9557 > 10) by scale factor 0.477198
I0803 17:13:36.516047  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.9759 > 10) by scale factor 0.345115
I0803 17:13:37.796834  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.5368 > 10) by scale factor 0.424867
I0803 17:13:39.071048  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.0889 > 10) by scale factor 0.497788
I0803 17:13:40.337977  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.7169 > 10) by scale factor 0.534277
I0803 17:13:41.604427  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.5441 > 10) by scale factor 0.569993
I0803 17:13:42.880288  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.8374 > 10) by scale factor 0.778975
I0803 17:13:44.153518  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.8672 > 10) by scale factor 0.777172
I0803 17:13:45.425745  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.4062 > 10) by scale factor 0.378699
I0803 17:13:46.704773  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.1661 > 10) by scale factor 0.431665
I0803 17:13:47.982446  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.8517 > 10) by scale factor 0.346601
I0803 17:13:48.876554  6069 solver.cpp:228] Iteration 180, loss = 0.83337
I0803 17:13:48.876610  6069 solver.cpp:244]     Train net output #0: local/loss = 0.63774 (* 1 = 0.63774 loss)
I0803 17:13:48.876622  6069 solver.cpp:244]     Train net output #1: loss = 0.191834 (* 1 = 0.191834 loss)
I0803 17:13:48.876631  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00379614 (* 1 = 0.00379614 loss)
I0803 17:13:48.876643  6069 sgd_solver.cpp:106] Iteration 180, lr = 9.99775e-05
I0803 17:13:49.263761  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 29.7633 > 10) by scale factor 0.335985
I0803 17:13:50.541841  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.598 > 10) by scale factor 0.362345
I0803 17:13:51.813765  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.6792 > 10) by scale factor 0.325954
I0803 17:13:53.083248  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 35.9026 > 10) by scale factor 0.278531
I0803 17:13:54.354873  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.1864 > 10) by scale factor 0.31069
I0803 17:13:55.623301  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.5387 > 10) by scale factor 0.350402
I0803 17:13:56.896155  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.0645 > 10) by scale factor 0.415549
I0803 17:13:58.163699  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.3275 > 10) by scale factor 0.447877
I0803 17:13:59.437111  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.0312 > 10) by scale factor 0.475484
I0803 17:14:00.706099  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.7374 > 10) by scale factor 0.563779
I0803 17:14:03.244760  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.7468 > 10) by scale factor 0.784511
I0803 17:14:12.029834  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4712 > 10) by scale factor 0.955002
I0803 17:14:14.142222  6069 solver.cpp:228] Iteration 200, loss = 0.421811
I0803 17:14:14.142287  6069 solver.cpp:244]     Train net output #0: local/loss = 0.228724 (* 1 = 0.228724 loss)
I0803 17:14:14.142300  6069 solver.cpp:244]     Train net output #1: loss = 0.184552 (* 1 = 0.184552 loss)
I0803 17:14:14.142310  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00853532 (* 1 = 0.00853532 loss)
I0803 17:14:14.142321  6069 sgd_solver.cpp:106] Iteration 200, lr = 9.9975e-05
I0803 17:14:23.322791  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7979 > 10) by scale factor 0.926103
I0803 17:14:24.591476  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.1288 > 10) by scale factor 0.89857
I0803 17:14:27.119218  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.0359 > 10) by scale factor 0.906138
I0803 17:14:30.905848  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8144 > 10) by scale factor 0.846426
I0803 17:14:32.176750  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4807 > 10) by scale factor 0.954134
I0803 17:14:39.343149  6069 solver.cpp:228] Iteration 220, loss = 0.392883
I0803 17:14:39.343204  6069 solver.cpp:244]     Train net output #0: local/loss = 0.198702 (* 1 = 0.198702 loss)
I0803 17:14:39.343216  6069 solver.cpp:244]     Train net output #1: loss = 0.188368 (* 1 = 0.188368 loss)
I0803 17:14:39.343227  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00581311 (* 1 = 0.00581311 loss)
I0803 17:14:39.343241  6069 sgd_solver.cpp:106] Iteration 220, lr = 9.99725e-05
I0803 17:14:51.092648  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.9062 > 10) by scale factor 0.916912
I0803 17:15:04.549834  6069 solver.cpp:228] Iteration 240, loss = 0.472589
I0803 17:15:04.550037  6069 solver.cpp:244]     Train net output #0: local/loss = 0.258971 (* 1 = 0.258971 loss)
I0803 17:15:04.550071  6069 solver.cpp:244]     Train net output #1: loss = 0.211194 (* 1 = 0.211194 loss)
I0803 17:15:04.550088  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00242371 (* 1 = 0.00242371 loss)
I0803 17:15:04.550112  6069 sgd_solver.cpp:106] Iteration 240, lr = 9.997e-05
I0803 17:15:08.727157  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7365 > 10) by scale factor 0.931406
I0803 17:15:29.716678  6069 solver.cpp:228] Iteration 260, loss = 0.294874
I0803 17:15:29.716748  6069 solver.cpp:244]     Train net output #0: local/loss = 0.147793 (* 1 = 0.147793 loss)
I0803 17:15:29.716761  6069 solver.cpp:244]     Train net output #1: loss = 0.1424 (* 1 = 0.1424 loss)
I0803 17:15:29.716773  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00468156 (* 1 = 0.00468156 loss)
I0803 17:15:29.716785  6069 sgd_solver.cpp:106] Iteration 260, lr = 9.99675e-05
I0803 17:15:54.960072  6069 solver.cpp:228] Iteration 280, loss = 0.367635
I0803 17:15:54.960325  6069 solver.cpp:244]     Train net output #0: local/loss = 0.217448 (* 1 = 0.217448 loss)
I0803 17:15:54.960356  6069 solver.cpp:244]     Train net output #1: loss = 0.14796 (* 1 = 0.14796 loss)
I0803 17:15:54.960372  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00222664 (* 1 = 0.00222664 loss)
I0803 17:15:54.960389  6069 sgd_solver.cpp:106] Iteration 280, lr = 9.9965e-05
I0803 17:16:20.153355  6069 solver.cpp:228] Iteration 300, loss = 0.303164
I0803 17:16:20.153409  6069 solver.cpp:244]     Train net output #0: local/loss = 0.181051 (* 1 = 0.181051 loss)
I0803 17:16:20.153421  6069 solver.cpp:244]     Train net output #1: loss = 0.11994 (* 1 = 0.11994 loss)
I0803 17:16:20.153430  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00217332 (* 1 = 0.00217332 loss)
I0803 17:16:20.153441  6069 sgd_solver.cpp:106] Iteration 300, lr = 9.99625e-05
I0803 17:16:49.141329  6069 solver.cpp:228] Iteration 320, loss = 0.39077
I0803 17:16:49.141542  6069 solver.cpp:244]     Train net output #0: local/loss = 0.204577 (* 1 = 0.204577 loss)
I0803 17:16:49.141587  6069 solver.cpp:244]     Train net output #1: loss = 0.182655 (* 1 = 0.182655 loss)
I0803 17:16:49.141603  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00353791 (* 1 = 0.00353791 loss)
I0803 17:16:49.141624  6069 sgd_solver.cpp:106] Iteration 320, lr = 9.996e-05
I0803 17:18:42.425179  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.458 > 10) by scale factor 0.956201
I0803 17:18:44.560562  6069 solver.cpp:228] Iteration 340, loss = 0.369901
I0803 17:18:44.560616  6069 solver.cpp:244]     Train net output #0: local/loss = 0.238352 (* 1 = 0.238352 loss)
I0803 17:18:44.560628  6069 solver.cpp:244]     Train net output #1: loss = 0.130038 (* 1 = 0.130038 loss)
I0803 17:18:44.560637  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00151053 (* 1 = 0.00151053 loss)
I0803 17:18:44.560648  6069 sgd_solver.cpp:106] Iteration 340, lr = 9.99575e-05
I0803 17:18:46.222256  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.1429 > 10) by scale factor 0.823526
I0803 17:18:47.503445  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.9353 > 10) by scale factor 0.773079
I0803 17:18:48.780459  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.1186 > 10) by scale factor 0.899391
I0803 17:18:50.045505  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.8877 > 10) by scale factor 0.720064
I0803 17:19:31.862010  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.3136 > 10) by scale factor 0.698635
I0803 17:19:33.080726  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.6188 > 10) by scale factor 0.860671
I0803 17:19:34.324163  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.6507 > 10) by scale factor 0.858321
I0803 17:19:35.577832  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.3497 > 10) by scale factor 0.696878
I0803 17:19:36.828397  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.3421 > 10) by scale factor 0.88167
I0803 17:19:38.089855  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.1349 > 10) by scale factor 0.89808
I0803 17:19:39.355679  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.318 > 10) by scale factor 0.698423
I0803 17:19:40.633819  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.2032 > 10) by scale factor 0.617161
I0803 17:19:47.889752  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.1698 > 10) by scale factor 0.983301
I0803 17:20:25.305621  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.5278 > 10) by scale factor 0.949862
I0803 17:20:56.909206  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4058 > 10) by scale factor 0.961002
I0803 17:20:57.699364  6069 solver.cpp:228] Iteration 360, loss = 0.335416
I0803 17:20:57.699429  6069 solver.cpp:244]     Train net output #0: local/loss = 0.193451 (* 1 = 0.193451 loss)
I0803 17:20:57.699441  6069 solver.cpp:244]     Train net output #1: loss = 0.137949 (* 1 = 0.137949 loss)
I0803 17:20:57.699451  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00401582 (* 1 = 0.00401582 loss)
I0803 17:20:57.699463  6069 sgd_solver.cpp:106] Iteration 360, lr = 9.9955e-05
I0803 17:20:58.058001  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.1872 > 10) by scale factor 0.981626
I0803 17:20:59.210291  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8505 > 10) by scale factor 0.843845
I0803 17:21:39.039471  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0796 > 10) by scale factor 0.992102
I0803 17:21:40.263082  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.3685 > 10) by scale factor 0.879621
I0803 17:22:38.142982  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.6613 > 10) by scale factor 0.857535
I0803 17:22:40.681190  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7795 > 10) by scale factor 0.927684
I0803 17:22:41.952707  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.5371 > 10) by scale factor 0.949032
I0803 17:22:43.228689  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.7961 > 10) by scale factor 0.781491
I0803 17:22:44.502758  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8026 > 10) by scale factor 0.847269
I0803 17:22:45.775218  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.7796 > 10) by scale factor 0.848922
I0803 17:22:47.050778  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.9303 > 10) by scale factor 0.914887
I0803 17:22:48.328531  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 29.8203 > 10) by scale factor 0.335342
I0803 17:22:49.600826  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.5014 > 10) by scale factor 0.35086
I0803 17:22:50.873744  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.4323 > 10) by scale factor 0.409293
I0803 17:22:52.140188  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.1081 > 10) by scale factor 0.355769
I0803 17:24:36.904634  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.1554 > 10) by scale factor 0.550802
I0803 17:25:23.648131  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.415 > 10) by scale factor 0.515065
I0803 17:26:11.582479  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.3328 > 10) by scale factor 0.882391
I0803 17:26:12.816360  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.7289 > 10) by scale factor 0.785611
I0803 17:26:13.681921  6069 solver.cpp:228] Iteration 380, loss = 0.598065
I0803 17:26:13.681972  6069 solver.cpp:244]     Train net output #0: local/loss = 0.425967 (* 1 = 0.425967 loss)
I0803 17:26:13.681988  6069 solver.cpp:244]     Train net output #1: loss = 0.168332 (* 1 = 0.168332 loss)
I0803 17:26:13.682001  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00376587 (* 1 = 0.00376587 loss)
I0803 17:26:13.682016  6069 sgd_solver.cpp:106] Iteration 380, lr = 9.99525e-05
I0803 17:26:14.068392  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.4906 > 10) by scale factor 0.690102
I0803 17:26:15.328585  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.3806 > 10) by scale factor 0.74735
I0803 17:26:16.600253  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.3114 > 10) by scale factor 0.884064
I0803 17:26:17.871486  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.0174 > 10) by scale factor 0.713401
I0803 17:26:19.143329  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.965 > 10) by scale factor 0.668224
I0803 17:26:20.418716  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.6593 > 10) by scale factor 0.566274
I0803 17:26:21.691692  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.4947 > 10) by scale factor 0.606254
I0803 17:26:22.970383  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.1712 > 10) by scale factor 0.58237
I0803 17:26:24.241463  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.4857 > 10) by scale factor 0.870649
I0803 17:27:03.658437  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.6713 > 10) by scale factor 0.731459
I0803 17:27:04.906819  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.9747 > 10) by scale factor 0.911187
I0803 17:27:41.351074  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2516 > 10) by scale factor 0.97546
I0803 17:27:43.697504  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2349 > 10) by scale factor 0.977051
I0803 17:29:02.763950  6069 solver.cpp:228] Iteration 400, loss = 0.407289
I0803 17:29:02.764163  6069 solver.cpp:244]     Train net output #0: local/loss = 0.247499 (* 1 = 0.247499 loss)
I0803 17:29:02.764195  6069 solver.cpp:244]     Train net output #1: loss = 0.145202 (* 1 = 0.145202 loss)
I0803 17:29:02.764221  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0145883 (* 1 = 0.0145883 loss)
I0803 17:29:02.764236  6069 sgd_solver.cpp:106] Iteration 400, lr = 9.995e-05
I0803 17:31:34.859017  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.142 > 10) by scale factor 0.986
I0803 17:32:12.257464  6069 solver.cpp:228] Iteration 420, loss = 0.321299
I0803 17:32:12.257673  6069 solver.cpp:244]     Train net output #0: local/loss = 0.169556 (* 1 = 0.169556 loss)
I0803 17:32:12.257707  6069 solver.cpp:244]     Train net output #1: loss = 0.139122 (* 1 = 0.139122 loss)
I0803 17:32:12.257725  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0126213 (* 1 = 0.0126213 loss)
I0803 17:32:12.257747  6069 sgd_solver.cpp:106] Iteration 420, lr = 9.99475e-05
I0803 17:33:03.611552  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.6569 > 10) by scale factor 0.938355
I0803 17:33:43.889076  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.8831 > 10) by scale factor 0.918859
I0803 17:33:46.062855  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.3322 > 10) by scale factor 0.967847
I0803 17:33:47.689163  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4425 > 10) by scale factor 0.957623
I0803 17:34:33.666909  6069 solver.cpp:228] Iteration 440, loss = 0.529292
I0803 17:34:33.667124  6069 solver.cpp:244]     Train net output #0: local/loss = 0.362987 (* 1 = 0.362987 loss)
I0803 17:34:33.667155  6069 solver.cpp:244]     Train net output #1: loss = 0.158979 (* 1 = 0.158979 loss)
I0803 17:34:33.667171  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00732621 (* 1 = 0.00732621 loss)
I0803 17:34:33.667187  6069 sgd_solver.cpp:106] Iteration 440, lr = 9.9945e-05
I0803 17:34:35.312232  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.0086 > 10) by scale factor 0.713849
I0803 17:34:36.192410  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.1471 > 10) by scale factor 0.897098
I0803 17:34:37.339437  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.4747 > 10) by scale factor 0.87148
I0803 17:35:17.961485  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.6204 > 10) by scale factor 0.734193
I0803 17:35:19.237244  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.1358 > 10) by scale factor 0.660683
I0803 17:35:20.512167  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0295 > 10) by scale factor 0.997062
I0803 17:35:21.781697  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8697 > 10) by scale factor 0.842479
I0803 17:35:23.054008  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8147 > 10) by scale factor 0.846403
I0803 17:35:24.322803  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.0959 > 10) by scale factor 0.662429
I0803 17:36:44.643302  6069 solver.cpp:228] Iteration 460, loss = 0.313333
I0803 17:36:44.643542  6069 solver.cpp:244]     Train net output #0: local/loss = 0.175505 (* 1 = 0.175505 loss)
I0803 17:36:44.643561  6069 solver.cpp:244]     Train net output #1: loss = 0.130128 (* 1 = 0.130128 loss)
I0803 17:36:44.643570  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0077002 (* 1 = 0.0077002 loss)
I0803 17:36:44.643582  6069 sgd_solver.cpp:106] Iteration 460, lr = 9.99425e-05
I0803 17:38:42.374058  6069 solver.cpp:228] Iteration 480, loss = 0.260294
I0803 17:38:42.374310  6069 solver.cpp:244]     Train net output #0: local/loss = 0.128954 (* 1 = 0.128954 loss)
I0803 17:38:42.374342  6069 solver.cpp:244]     Train net output #1: loss = 0.123639 (* 1 = 0.123639 loss)
I0803 17:38:42.374359  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0077013 (* 1 = 0.0077013 loss)
I0803 17:38:42.374377  6069 sgd_solver.cpp:106] Iteration 480, lr = 9.994e-05
I0803 17:39:02.918121  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.8295 > 10) by scale factor 0.923403
I0803 17:39:07.582545  6069 solver.cpp:228] Iteration 500, loss = 0.488315
I0803 17:39:07.582613  6069 solver.cpp:244]     Train net output #0: local/loss = 0.346977 (* 1 = 0.346977 loss)
I0803 17:39:07.582626  6069 solver.cpp:244]     Train net output #1: loss = 0.136977 (* 1 = 0.136977 loss)
I0803 17:39:07.582636  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00436138 (* 1 = 0.00436138 loss)
I0803 17:39:07.582648  6069 sgd_solver.cpp:106] Iteration 500, lr = 9.99375e-05
I0803 17:39:16.020826  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.0495 > 10) by scale factor 0.623074
I0803 17:39:18.561036  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2592 > 10) by scale factor 0.974732
I0803 17:39:21.088482  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.9789 > 10) by scale factor 0.770482
I0803 17:39:22.369184  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.503 > 10) by scale factor 0.540452
I0803 17:39:23.645079  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.7476 > 10) by scale factor 0.851234
I0803 17:39:24.912773  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.2136 > 10) by scale factor 0.657307
I0803 17:39:26.184619  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.062 > 10) by scale factor 0.663921
I0803 17:39:27.460527  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.354 > 10) by scale factor 0.88075
I0803 17:39:28.736340  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.3984 > 10) by scale factor 0.806553
I0803 17:39:30.006032  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7007 > 10) by scale factor 0.934516
I0803 17:39:31.289777  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.8877 > 10) by scale factor 0.775934
I0803 17:39:32.558238  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.2229 > 10) by scale factor 0.818137
I0803 17:39:37.341195  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0379 > 10) by scale factor 0.99622
I0803 17:39:38.214228  6069 solver.cpp:228] Iteration 520, loss = 0.313445
I0803 17:39:38.214318  6069 solver.cpp:244]     Train net output #0: local/loss = 0.179745 (* 1 = 0.179745 loss)
I0803 17:39:38.214331  6069 solver.cpp:244]     Train net output #1: loss = 0.128383 (* 1 = 0.128383 loss)
I0803 17:39:38.214342  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00531664 (* 1 = 0.00531664 loss)
I0803 17:39:38.214355  6069 sgd_solver.cpp:106] Iteration 520, lr = 9.9935e-05
I0803 17:39:38.619138  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.2338 > 10) by scale factor 0.890169
I0803 17:39:43.591476  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.8044 > 10) by scale factor 0.925552
I0803 17:39:44.869699  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0038 > 10) by scale factor 0.99962
I0803 17:39:47.403596  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.6679 > 10) by scale factor 0.789397
I0803 17:39:48.678828  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.8551 > 10) by scale factor 0.721758
I0803 17:39:49.955404  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.5378 > 10) by scale factor 0.948966
I0803 17:39:51.232928  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.2363 > 10) by scale factor 0.81724
I0803 17:39:52.503237  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.6108 > 10) by scale factor 0.734711
I0803 17:39:53.777384  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.9398 > 10) by scale factor 0.77281
I0803 17:39:55.046363  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.6948 > 10) by scale factor 0.787721
I0803 17:39:56.315692  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.5109 > 10) by scale factor 0.487545
I0803 17:39:57.581429  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.7286 > 10) by scale factor 0.533944
I0803 17:39:58.846114  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.8371 > 10) by scale factor 0.504106
I0803 17:40:00.110033  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.3647 > 10) by scale factor 0.365435
I0803 17:40:01.374689  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 38.7861 > 10) by scale factor 0.257825
I0803 17:40:02.638666  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 36.2046 > 10) by scale factor 0.276208
I0803 17:40:03.904150  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.7152 > 10) by scale factor 0.440233
I0803 17:40:05.167990  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.7558 > 10) by scale factor 0.506181
I0803 17:40:06.434679  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.6655 > 10) by scale factor 0.638345
I0803 17:40:07.348538  6069 solver.cpp:228] Iteration 540, loss = 0.466107
I0803 17:40:07.348610  6069 solver.cpp:244]     Train net output #0: local/loss = 0.348627 (* 1 = 0.348627 loss)
I0803 17:40:07.348623  6069 solver.cpp:244]     Train net output #1: loss = 0.112912 (* 1 = 0.112912 loss)
I0803 17:40:07.348634  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0045689 (* 1 = 0.0045689 loss)
I0803 17:40:07.348644  6069 sgd_solver.cpp:106] Iteration 540, lr = 9.99325e-05
I0803 17:40:07.707242  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.1533 > 10) by scale factor 0.706549
I0803 17:40:08.978518  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.7133 > 10) by scale factor 0.460548
I0803 17:40:10.252914  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.4604 > 10) by scale factor 0.742923
I0803 17:40:11.525507  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.972 > 10) by scale factor 0.667911
I0803 17:40:12.797701  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.8641 > 10) by scale factor 0.479292
I0803 17:40:14.069772  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.3205 > 10) by scale factor 0.411175
I0803 17:40:15.347833  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.585 > 10) by scale factor 0.349834
I0803 17:40:16.615536  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.696 > 10) by scale factor 0.305848
I0803 17:40:17.888849  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.302 > 10) by scale factor 0.330011
I0803 17:40:19.161058  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.5827 > 10) by scale factor 0.376184
I0803 17:40:20.434479  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 25.2851 > 10) by scale factor 0.39549
I0803 17:40:21.712872  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.4533 > 10) by scale factor 0.466128
I0803 17:40:22.983928  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.0449 > 10) by scale factor 0.49888
I0803 17:40:24.256878  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.414 > 10) by scale factor 0.543066
I0803 17:40:25.514421  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.9707 > 10) by scale factor 0.715783
I0803 17:40:26.796061  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.0438 > 10) by scale factor 0.905488
I0803 17:40:28.065289  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.3566 > 10) by scale factor 0.809282
I0803 17:40:32.752146  6069 solver.cpp:228] Iteration 560, loss = 0.284546
I0803 17:40:32.752221  6069 solver.cpp:244]     Train net output #0: local/loss = 0.137316 (* 1 = 0.137316 loss)
I0803 17:40:32.752241  6069 solver.cpp:244]     Train net output #1: loss = 0.133139 (* 1 = 0.133139 loss)
I0803 17:40:32.752255  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0140906 (* 1 = 0.0140906 loss)
I0803 17:40:32.752272  6069 sgd_solver.cpp:106] Iteration 560, lr = 9.993e-05
I0803 17:40:57.926232  6069 solver.cpp:228] Iteration 580, loss = 0.357007
I0803 17:40:57.926482  6069 solver.cpp:244]     Train net output #0: local/loss = 0.220287 (* 1 = 0.220287 loss)
I0803 17:40:57.926515  6069 solver.cpp:244]     Train net output #1: loss = 0.121114 (* 1 = 0.121114 loss)
I0803 17:40:57.926532  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0156059 (* 1 = 0.0156059 loss)
I0803 17:40:57.926553  6069 sgd_solver.cpp:106] Iteration 580, lr = 9.99275e-05
I0803 17:41:12.223543  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.0625 > 10) by scale factor 0.663902
I0803 17:41:21.045402  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0023 > 10) by scale factor 0.99977
I0803 17:41:23.567934  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.6036 > 10) by scale factor 0.943077
I0803 17:41:25.687842  6069 solver.cpp:228] Iteration 600, loss = 0.23071
I0803 17:41:25.687923  6069 solver.cpp:244]     Train net output #0: local/loss = 0.111243 (* 1 = 0.111243 loss)
I0803 17:41:25.687937  6069 solver.cpp:244]     Train net output #1: loss = 0.107669 (* 1 = 0.107669 loss)
I0803 17:41:25.687947  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0117971 (* 1 = 0.0117971 loss)
I0803 17:41:25.687960  6069 sgd_solver.cpp:106] Iteration 600, lr = 9.9925e-05
I0803 17:41:50.842290  6069 solver.cpp:228] Iteration 620, loss = 0.224798
I0803 17:41:50.842412  6069 solver.cpp:244]     Train net output #0: local/loss = 0.103851 (* 1 = 0.103851 loss)
I0803 17:41:50.842427  6069 solver.cpp:244]     Train net output #1: loss = 0.109928 (* 1 = 0.109928 loss)
I0803 17:41:50.842437  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0110189 (* 1 = 0.0110189 loss)
I0803 17:41:50.842449  6069 sgd_solver.cpp:106] Iteration 620, lr = 9.99225e-05
I0803 17:42:07.606590  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4233 > 10) by scale factor 0.95939
I0803 17:42:08.876734  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.061 > 10) by scale factor 0.993942
I0803 17:42:13.918354  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.6257 > 10) by scale factor 0.860166
I0803 17:42:15.190448  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.0838 > 10) by scale factor 0.474297
I0803 17:42:16.066203  6069 solver.cpp:228] Iteration 640, loss = 0.656042
I0803 17:42:16.066262  6069 solver.cpp:244]     Train net output #0: local/loss = 0.527107 (* 1 = 0.527107 loss)
I0803 17:42:16.066277  6069 solver.cpp:244]     Train net output #1: loss = 0.124283 (* 1 = 0.124283 loss)
I0803 17:42:16.066289  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00465207 (* 1 = 0.00465207 loss)
I0803 17:42:16.066306  6069 sgd_solver.cpp:106] Iteration 640, lr = 9.992e-05
I0803 17:42:16.466769  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.761 > 10) by scale factor 0.596623
I0803 17:42:17.743582  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.7772 > 10) by scale factor 0.53256
I0803 17:42:19.007241  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.0644 > 10) by scale factor 0.663815
I0803 17:42:20.281168  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.766 > 10) by scale factor 0.634277
I0803 17:42:21.549443  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.8037 > 10) by scale factor 0.531811
I0803 17:42:22.822396  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.3914 > 10) by scale factor 0.574998
I0803 17:42:24.098630  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.0752 > 10) by scale factor 0.585644
I0803 17:42:25.366219  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.8462 > 10) by scale factor 0.503874
I0803 17:42:26.643116  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.0418 > 10) by scale factor 0.62337
I0803 17:42:27.917678  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.8365 > 10) by scale factor 0.560649
I0803 17:42:29.196504  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.9696 > 10) by scale factor 0.589289
I0803 17:42:30.470953  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.6538 > 10) by scale factor 0.536084
I0803 17:42:31.744549  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.5295 > 10) by scale factor 0.604978
I0803 17:42:33.020776  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.7651 > 10) by scale factor 0.459452
I0803 17:42:34.294291  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.3879 > 10) by scale factor 0.543836
I0803 17:42:35.571439  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.8118 > 10) by scale factor 0.59482
I0803 17:42:36.843514  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.6791 > 10) by scale factor 0.483581
I0803 17:42:38.118343  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.8265 > 10) by scale factor 0.723247
I0803 17:42:39.385396  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.0577 > 10) by scale factor 0.829345
I0803 17:42:40.657186  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.7361 > 10) by scale factor 0.852071
I0803 17:42:41.534631  6069 solver.cpp:228] Iteration 660, loss = 0.349642
I0803 17:42:41.534698  6069 solver.cpp:244]     Train net output #0: local/loss = 0.202662 (* 1 = 0.202662 loss)
I0803 17:42:41.534709  6069 solver.cpp:244]     Train net output #1: loss = 0.139453 (* 1 = 0.139453 loss)
I0803 17:42:41.534718  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00752709 (* 1 = 0.00752709 loss)
I0803 17:42:41.534731  6069 sgd_solver.cpp:106] Iteration 660, lr = 9.99175e-05
I0803 17:42:41.931155  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8246 > 10) by scale factor 0.845693
I0803 17:42:44.471891  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.8339 > 10) by scale factor 0.923027
I0803 17:42:48.276113  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2553 > 10) by scale factor 0.975107
I0803 17:43:06.813544  6069 solver.cpp:228] Iteration 680, loss = 0.385994
I0803 17:43:06.813761  6069 solver.cpp:244]     Train net output #0: local/loss = 0.258505 (* 1 = 0.258505 loss)
I0803 17:43:06.813777  6069 solver.cpp:244]     Train net output #1: loss = 0.118984 (* 1 = 0.118984 loss)
I0803 17:43:06.813788  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00850535 (* 1 = 0.00850535 loss)
I0803 17:43:06.813802  6069 sgd_solver.cpp:106] Iteration 680, lr = 9.9915e-05
I0803 17:43:07.206051  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2814 > 10) by scale factor 0.972627
I0803 17:43:08.478041  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0471 > 10) by scale factor 0.995313
I0803 17:43:09.745942  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0716 > 10) by scale factor 0.992886
I0803 17:43:11.022630  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.3048 > 10) by scale factor 0.812694
I0803 17:43:12.295351  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.719 > 10) by scale factor 0.786224
I0803 17:43:13.577982  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.4688 > 10) by scale factor 0.607207
I0803 17:43:14.845897  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.9368 > 10) by scale factor 0.627478
I0803 17:43:16.110723  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.9229 > 10) by scale factor 0.590915
I0803 17:43:17.375283  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.0821 > 10) by scale factor 0.332424
I0803 17:43:18.636670  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.0541 > 10) by scale factor 0.433763
I0803 17:43:19.901486  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.5104 > 10) by scale factor 0.444239
I0803 17:43:21.166896  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.0803 > 10) by scale factor 0.452893
I0803 17:43:22.431619  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.1171 > 10) by scale factor 0.452139
I0803 17:43:23.697511  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.5213 > 10) by scale factor 0.487298
I0803 17:43:24.974629  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.2174 > 10) by scale factor 0.520363
I0803 17:43:26.249708  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.5132 > 10) by scale factor 0.377171
I0803 17:43:27.524211  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.3265 > 10) by scale factor 0.447898
I0803 17:43:28.794991  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 26.6512 > 10) by scale factor 0.375218
I0803 17:43:30.064779  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.6484 > 10) by scale factor 0.306294
I0803 17:43:31.337524  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 31.7928 > 10) by scale factor 0.314537
I0803 17:43:32.252943  6069 solver.cpp:228] Iteration 700, loss = 0.550769
I0803 17:43:32.253011  6069 solver.cpp:244]     Train net output #0: local/loss = 0.393159 (* 1 = 0.393159 loss)
I0803 17:43:32.253026  6069 solver.cpp:244]     Train net output #1: loss = 0.150066 (* 1 = 0.150066 loss)
I0803 17:43:32.253036  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00754304 (* 1 = 0.00754304 loss)
I0803 17:43:32.253048  6069 sgd_solver.cpp:106] Iteration 700, lr = 9.99125e-05
I0803 17:43:32.607729  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 27.8704 > 10) by scale factor 0.358804
I0803 17:43:33.880146  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 28.6573 > 10) by scale factor 0.348951
I0803 17:43:35.154322  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.6489 > 10) by scale factor 0.326276
I0803 17:43:36.428522  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.4418 > 10) by scale factor 0.426588
I0803 17:43:37.707079  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.9807 > 10) by scale factor 0.526851
I0803 17:43:38.979339  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.0702 > 10) by scale factor 0.553398
I0803 17:43:40.251797  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.4948 > 10) by scale factor 0.571598
I0803 17:43:41.522378  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.761 > 10) by scale factor 0.726693
I0803 17:43:42.797226  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.6789 > 10) by scale factor 0.731055
I0803 17:43:44.065359  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.0679 > 10) by scale factor 0.828641
I0803 17:43:57.560822  6069 solver.cpp:228] Iteration 720, loss = 0.229714
I0803 17:43:57.560958  6069 solver.cpp:244]     Train net output #0: local/loss = 0.102777 (* 1 = 0.102777 loss)
I0803 17:43:57.560989  6069 solver.cpp:244]     Train net output #1: loss = 0.108529 (* 1 = 0.108529 loss)
I0803 17:43:57.561010  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0184078 (* 1 = 0.0184078 loss)
I0803 17:43:57.561033  6069 sgd_solver.cpp:106] Iteration 720, lr = 9.991e-05
I0803 17:44:20.592175  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.3932 > 10) by scale factor 0.806893
I0803 17:44:21.859877  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.6524 > 10) by scale factor 0.938759
I0803 17:44:22.767786  6069 solver.cpp:228] Iteration 740, loss = 0.441667
I0803 17:44:22.767865  6069 solver.cpp:244]     Train net output #0: local/loss = 0.307399 (* 1 = 0.307399 loss)
I0803 17:44:22.767880  6069 solver.cpp:244]     Train net output #1: loss = 0.11977 (* 1 = 0.11977 loss)
I0803 17:44:22.767892  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0144972 (* 1 = 0.0144972 loss)
I0803 17:44:22.767905  6069 sgd_solver.cpp:106] Iteration 740, lr = 9.99075e-05
I0803 17:44:23.127882  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2605 > 10) by scale factor 0.974611
I0803 17:44:28.187078  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.1355 > 10) by scale factor 0.496635
I0803 17:44:30.720437  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.1389 > 10) by scale factor 0.986301
I0803 17:44:33.263322  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0627 > 10) by scale factor 0.993771
I0803 17:44:34.538736  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.5039 > 10) by scale factor 0.95203
I0803 17:44:35.809275  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2408 > 10) by scale factor 0.976487
I0803 17:44:40.384850  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0353 > 10) by scale factor 0.996482
I0803 17:44:58.682420  6069 solver.cpp:228] Iteration 760, loss = 0.281318
I0803 17:44:58.682639  6069 solver.cpp:244]     Train net output #0: local/loss = 0.123757 (* 1 = 0.123757 loss)
I0803 17:44:58.682673  6069 solver.cpp:244]     Train net output #1: loss = 0.142259 (* 1 = 0.142259 loss)
I0803 17:44:58.682689  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0153019 (* 1 = 0.0153019 loss)
I0803 17:44:58.682708  6069 sgd_solver.cpp:106] Iteration 760, lr = 9.9905e-05
I0803 17:45:23.882098  6069 solver.cpp:228] Iteration 780, loss = 0.272888
I0803 17:45:23.882167  6069 solver.cpp:244]     Train net output #0: local/loss = 0.128537 (* 1 = 0.128537 loss)
I0803 17:45:23.882180  6069 solver.cpp:244]     Train net output #1: loss = 0.132238 (* 1 = 0.132238 loss)
I0803 17:45:23.882189  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0121134 (* 1 = 0.0121134 loss)
I0803 17:45:23.882202  6069 sgd_solver.cpp:106] Iteration 780, lr = 9.99024e-05
I0803 17:45:49.096843  6069 solver.cpp:228] Iteration 800, loss = 0.259336
I0803 17:45:49.097095  6069 solver.cpp:244]     Train net output #0: local/loss = 0.127501 (* 1 = 0.127501 loss)
I0803 17:45:49.097131  6069 solver.cpp:244]     Train net output #1: loss = 0.122602 (* 1 = 0.122602 loss)
I0803 17:45:49.097146  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00923363 (* 1 = 0.00923363 loss)
I0803 17:45:49.097165  6069 sgd_solver.cpp:106] Iteration 800, lr = 9.98999e-05
I0803 17:46:10.034809  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.5966 > 10) by scale factor 0.943696
I0803 17:46:14.683992  6069 solver.cpp:228] Iteration 820, loss = 0.346099
I0803 17:46:14.684061  6069 solver.cpp:244]     Train net output #0: local/loss = 0.218376 (* 1 = 0.218376 loss)
I0803 17:46:14.684074  6069 solver.cpp:244]     Train net output #1: loss = 0.123461 (* 1 = 0.123461 loss)
I0803 17:46:14.684084  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00426171 (* 1 = 0.00426171 loss)
I0803 17:46:14.684097  6069 sgd_solver.cpp:106] Iteration 820, lr = 9.98974e-05
I0803 17:46:17.598880  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.2827 > 10) by scale factor 0.97251
I0803 17:46:18.872143  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.3244 > 10) by scale factor 0.652553
I0803 17:46:20.149627  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.1913 > 10) by scale factor 0.65827
I0803 17:46:21.426836  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.6438 > 10) by scale factor 0.63923
I0803 17:46:22.703603  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.2957 > 10) by scale factor 0.813295
I0803 17:46:23.976439  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.9717 > 10) by scale factor 0.770907
I0803 17:46:25.243590  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.8202 > 10) by scale factor 0.723576
I0803 17:46:26.513464  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.2201 > 10) by scale factor 0.703232
I0803 17:46:27.785514  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.5199 > 10) by scale factor 0.688711
I0803 17:46:30.307350  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.1616 > 10) by scale factor 0.759788
I0803 17:46:31.573580  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.943 > 10) by scale factor 0.837309
I0803 17:46:32.840559  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.64 > 10) by scale factor 0.939849
I0803 17:46:35.364404  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7733 > 10) by scale factor 0.928222
I0803 17:46:40.030575  6069 solver.cpp:228] Iteration 840, loss = 0.282802
I0803 17:46:40.030625  6069 solver.cpp:244]     Train net output #0: local/loss = 0.142496 (* 1 = 0.142496 loss)
I0803 17:46:40.030637  6069 solver.cpp:244]     Train net output #1: loss = 0.134532 (* 1 = 0.134532 loss)
I0803 17:46:40.030647  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00577418 (* 1 = 0.00577418 loss)
I0803 17:46:40.030658  6069 sgd_solver.cpp:106] Iteration 840, lr = 9.98949e-05
I0803 17:46:51.200608  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.411 > 10) by scale factor 0.960522
I0803 17:46:52.472491  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.718 > 10) by scale factor 0.85339
I0803 17:46:53.737241  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.9536 > 10) by scale factor 0.771989
I0803 17:46:55.005187  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.1396 > 10) by scale factor 0.761059
I0803 17:46:57.531247  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.8093 > 10) by scale factor 0.780683
I0803 17:47:00.061105  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.1758 > 10) by scale factor 0.705427
I0803 17:47:01.333379  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.1784 > 10) by scale factor 0.582125
I0803 17:47:02.598325  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.1189 > 10) by scale factor 0.452103
I0803 17:47:03.867602  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.0341 > 10) by scale factor 0.623672
I0803 17:47:05.132725  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.2355 > 10) by scale factor 0.412618
I0803 17:47:06.042981  6069 solver.cpp:228] Iteration 860, loss = 0.840822
I0803 17:47:06.043045  6069 solver.cpp:244]     Train net output #0: local/loss = 0.714083 (* 1 = 0.714083 loss)
I0803 17:47:06.043057  6069 solver.cpp:244]     Train net output #1: loss = 0.122467 (* 1 = 0.122467 loss)
I0803 17:47:06.043068  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00427159 (* 1 = 0.00427159 loss)
I0803 17:47:06.043081  6069 sgd_solver.cpp:106] Iteration 860, lr = 9.98924e-05
I0803 17:47:06.399121  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.6974 > 10) by scale factor 0.507682
I0803 17:47:07.670476  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 32.0663 > 10) by scale factor 0.311853
I0803 17:47:08.943568  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 37.8402 > 10) by scale factor 0.26427
I0803 17:47:10.766388  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.9131 > 10) by scale factor 0.502181
I0803 17:47:12.032780  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.0628 > 10) by scale factor 0.433598
I0803 17:47:13.297921  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.2311 > 10) by scale factor 0.471006
I0803 17:47:14.571723  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.7069 > 10) by scale factor 0.421818
I0803 17:47:15.851678  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.1485 > 10) by scale factor 0.331692
I0803 17:47:17.126338  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 30.3277 > 10) by scale factor 0.329732
I0803 17:47:18.393528  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.0209 > 10) by scale factor 0.475717
I0803 17:47:19.669576  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 31.54 > 10) by scale factor 0.317057
I0803 17:47:20.945937  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 33.3033 > 10) by scale factor 0.300271
I0803 17:47:22.228188  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 35.2048 > 10) by scale factor 0.284052
I0803 17:47:23.499891  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 29.3403 > 10) by scale factor 0.340828
I0803 17:47:24.784098  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 29.2542 > 10) by scale factor 0.341832
I0803 17:47:26.059631  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 24.2245 > 10) by scale factor 0.412805
I0803 17:47:27.333966  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.6316 > 10) by scale factor 0.462287
I0803 17:47:28.599802  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.6241 > 10) by scale factor 0.48487
I0803 17:47:29.873958  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.7582 > 10) by scale factor 0.85047
I0803 17:47:31.146699  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.7229 > 10) by scale factor 0.785982
I0803 17:47:32.016553  6069 solver.cpp:228] Iteration 880, loss = 0.300825
I0803 17:47:32.016616  6069 solver.cpp:244]     Train net output #0: local/loss = 0.166192 (* 1 = 0.166192 loss)
I0803 17:47:32.016628  6069 solver.cpp:244]     Train net output #1: loss = 0.122599 (* 1 = 0.122599 loss)
I0803 17:47:32.016638  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0120347 (* 1 = 0.0120347 loss)
I0803 17:47:32.016651  6069 sgd_solver.cpp:106] Iteration 880, lr = 9.98899e-05
I0803 17:47:32.424480  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.2444 > 10) by scale factor 0.816699
I0803 17:47:57.197578  6069 solver.cpp:228] Iteration 900, loss = 0.271556
I0803 17:47:57.197729  6069 solver.cpp:244]     Train net output #0: local/loss = 0.1208 (* 1 = 0.1208 loss)
I0803 17:47:57.197744  6069 solver.cpp:244]     Train net output #1: loss = 0.129608 (* 1 = 0.129608 loss)
I0803 17:47:57.197765  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0211475 (* 1 = 0.0211475 loss)
I0803 17:47:57.197779  6069 sgd_solver.cpp:106] Iteration 900, lr = 9.98874e-05
I0803 17:48:22.377846  6069 solver.cpp:228] Iteration 920, loss = 0.347974
I0803 17:48:22.377920  6069 solver.cpp:244]     Train net output #0: local/loss = 0.173534 (* 1 = 0.173534 loss)
I0803 17:48:22.377934  6069 solver.cpp:244]     Train net output #1: loss = 0.158659 (* 1 = 0.158659 loss)
I0803 17:48:22.377944  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0157801 (* 1 = 0.0157801 loss)
I0803 17:48:22.377956  6069 sgd_solver.cpp:106] Iteration 920, lr = 9.98849e-05
I0803 17:48:53.314364  6069 solver.cpp:228] Iteration 940, loss = 0.361005
I0803 17:48:53.314580  6069 solver.cpp:244]     Train net output #0: local/loss = 0.189896 (* 1 = 0.189896 loss)
I0803 17:48:53.314612  6069 solver.cpp:244]     Train net output #1: loss = 0.159728 (* 1 = 0.159728 loss)
I0803 17:48:53.314625  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0113801 (* 1 = 0.0113801 loss)
I0803 17:48:53.314641  6069 sgd_solver.cpp:106] Iteration 940, lr = 9.98824e-05
I0803 17:49:19.073204  6069 solver.cpp:228] Iteration 960, loss = 0.285113
I0803 17:49:19.073295  6069 solver.cpp:244]     Train net output #0: local/loss = 0.136255 (* 1 = 0.136255 loss)
I0803 17:49:19.073309  6069 solver.cpp:244]     Train net output #1: loss = 0.139856 (* 1 = 0.139856 loss)
I0803 17:49:19.073320  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00900171 (* 1 = 0.00900171 loss)
I0803 17:49:19.073333  6069 sgd_solver.cpp:106] Iteration 960, lr = 9.98799e-05
I0803 17:49:44.269696  6069 solver.cpp:228] Iteration 980, loss = 0.199136
I0803 17:49:44.269884  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0979974 (* 1 = 0.0979974 loss)
I0803 17:49:44.269906  6069 solver.cpp:244]     Train net output #1: loss = 0.0930734 (* 1 = 0.0930734 loss)
I0803 17:49:44.269912  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00806519 (* 1 = 0.00806519 loss)
I0803 17:49:44.269923  6069 sgd_solver.cpp:106] Iteration 980, lr = 9.98774e-05
I0803 17:50:08.587540  6069 solver.cpp:337] Iteration 1000, Testing net (#0)
I0803 17:51:08.004395  6069 solver.cpp:404]     Test net output #0: local/loss = 0.114917 (* 1 = 0.114917 loss)
I0803 17:51:08.004598  6069 solver.cpp:404]     Test net output #1: loss = 0.117837 (* 1 = 0.117837 loss)
I0803 17:51:08.004629  6069 solver.cpp:404]     Test net output #2: theta_loss = 0.00522879 (* 1 = 0.00522879 loss)
I0803 17:51:08.798940  6069 solver.cpp:228] Iteration 1000, loss = 0.270333
I0803 17:51:08.799005  6069 solver.cpp:244]     Train net output #0: local/loss = 0.131984 (* 1 = 0.131984 loss)
I0803 17:51:08.799021  6069 solver.cpp:244]     Train net output #1: loss = 0.132856 (* 1 = 0.132856 loss)
I0803 17:51:08.799031  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00549298 (* 1 = 0.00549298 loss)
I0803 17:51:08.799047  6069 sgd_solver.cpp:106] Iteration 1000, lr = 9.98749e-05
I0803 17:51:31.621919  6069 solver.cpp:228] Iteration 1020, loss = 0.230044
I0803 17:51:31.621994  6069 solver.cpp:244]     Train net output #0: local/loss = 0.111599 (* 1 = 0.111599 loss)
I0803 17:51:31.622009  6069 solver.cpp:244]     Train net output #1: loss = 0.115305 (* 1 = 0.115305 loss)
I0803 17:51:31.622019  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00314027 (* 1 = 0.00314027 loss)
I0803 17:51:31.622031  6069 sgd_solver.cpp:106] Iteration 1020, lr = 9.98724e-05
I0803 17:51:55.795729  6069 solver.cpp:228] Iteration 1040, loss = 0.261927
I0803 17:51:55.796007  6069 solver.cpp:244]     Train net output #0: local/loss = 0.124692 (* 1 = 0.124692 loss)
I0803 17:51:55.796037  6069 solver.cpp:244]     Train net output #1: loss = 0.135301 (* 1 = 0.135301 loss)
I0803 17:51:55.796052  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00193319 (* 1 = 0.00193319 loss)
I0803 17:51:55.796068  6069 sgd_solver.cpp:106] Iteration 1040, lr = 9.98699e-05
I0803 17:52:21.012338  6069 solver.cpp:228] Iteration 1060, loss = 0.211965
I0803 17:52:21.012418  6069 solver.cpp:244]     Train net output #0: local/loss = 0.10374 (* 1 = 0.10374 loss)
I0803 17:52:21.012434  6069 solver.cpp:244]     Train net output #1: loss = 0.105968 (* 1 = 0.105968 loss)
I0803 17:52:21.012444  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00225764 (* 1 = 0.00225764 loss)
I0803 17:52:21.012459  6069 sgd_solver.cpp:106] Iteration 1060, lr = 9.98674e-05
I0803 17:52:46.185277  6069 solver.cpp:228] Iteration 1080, loss = 0.228222
I0803 17:52:46.185475  6069 solver.cpp:244]     Train net output #0: local/loss = 0.117063 (* 1 = 0.117063 loss)
I0803 17:52:46.185502  6069 solver.cpp:244]     Train net output #1: loss = 0.109301 (* 1 = 0.109301 loss)
I0803 17:52:46.185513  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00185781 (* 1 = 0.00185781 loss)
I0803 17:52:46.185528  6069 sgd_solver.cpp:106] Iteration 1080, lr = 9.98649e-05
I0803 17:53:01.358491  6069 solver.cpp:228] Iteration 1100, loss = 0.317865
I0803 17:53:01.358582  6069 solver.cpp:244]     Train net output #0: local/loss = 0.177796 (* 1 = 0.177796 loss)
I0803 17:53:01.358597  6069 solver.cpp:244]     Train net output #1: loss = 0.138797 (* 1 = 0.138797 loss)
I0803 17:53:01.358608  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00127265 (* 1 = 0.00127265 loss)
I0803 17:53:01.358621  6069 sgd_solver.cpp:106] Iteration 1100, lr = 9.98624e-05
I0803 17:53:14.262061  6069 solver.cpp:228] Iteration 1120, loss = 0.248273
I0803 17:53:14.262135  6069 solver.cpp:244]     Train net output #0: local/loss = 0.123519 (* 1 = 0.123519 loss)
I0803 17:53:14.262147  6069 solver.cpp:244]     Train net output #1: loss = 0.123068 (* 1 = 0.123068 loss)
I0803 17:53:14.262157  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00168602 (* 1 = 0.00168602 loss)
I0803 17:53:14.262169  6069 sgd_solver.cpp:106] Iteration 1120, lr = 9.98599e-05
I0803 17:53:27.093830  6069 solver.cpp:228] Iteration 1140, loss = 0.250322
I0803 17:53:27.094040  6069 solver.cpp:244]     Train net output #0: local/loss = 0.117988 (* 1 = 0.117988 loss)
I0803 17:53:27.094081  6069 solver.cpp:244]     Train net output #1: loss = 0.130532 (* 1 = 0.130532 loss)
I0803 17:53:27.094099  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0018025 (* 1 = 0.0018025 loss)
I0803 17:53:27.094122  6069 sgd_solver.cpp:106] Iteration 1140, lr = 9.98574e-05
I0803 17:53:46.850029  6069 solver.cpp:228] Iteration 1160, loss = 0.305631
I0803 17:53:46.850108  6069 solver.cpp:244]     Train net output #0: local/loss = 0.176756 (* 1 = 0.176756 loss)
I0803 17:53:46.850122  6069 solver.cpp:244]     Train net output #1: loss = 0.128027 (* 1 = 0.128027 loss)
I0803 17:53:46.850131  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000848776 (* 1 = 0.000848776 loss)
I0803 17:53:46.850143  6069 sgd_solver.cpp:106] Iteration 1160, lr = 9.98549e-05
I0803 17:53:56.160842  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.4318 > 10) by scale factor 0.874751
I0803 17:53:59.955909  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7922 > 10) by scale factor 0.926591
I0803 17:54:02.913058  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.5197 > 10) by scale factor 0.798739
I0803 17:54:03.526346  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.56 > 10) by scale factor 0.946973
I0803 17:54:04.139622  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.9768 > 10) by scale factor 0.911012
I0803 17:54:04.752534  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.7285 > 10) by scale factor 0.785639
I0803 17:54:05.365228  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.0602 > 10) by scale factor 0.664004
I0803 17:54:05.811425  6069 solver.cpp:228] Iteration 1180, loss = 0.315119
I0803 17:54:05.811496  6069 solver.cpp:244]     Train net output #0: local/loss = 0.209422 (* 1 = 0.209422 loss)
I0803 17:54:05.811514  6069 solver.cpp:244]     Train net output #1: loss = 0.104682 (* 1 = 0.104682 loss)
I0803 17:54:05.811527  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00101571 (* 1 = 0.00101571 loss)
I0803 17:54:05.811540  6069 sgd_solver.cpp:106] Iteration 1180, lr = 9.98524e-05
I0803 17:54:05.978070  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.3784 > 10) by scale factor 0.695487
I0803 17:54:06.591871  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.7255 > 10) by scale factor 0.852841
I0803 17:54:07.204099  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.5855 > 10) by scale factor 0.794564
I0803 17:54:07.816642  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.6262 > 10) by scale factor 0.941073
I0803 17:54:08.429239  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.0576 > 10) by scale factor 0.904354
I0803 17:54:09.041424  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.4897 > 10) by scale factor 0.870342
I0803 17:54:10.261327  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.642 > 10) by scale factor 0.858961
I0803 17:54:18.181372  6069 solver.cpp:228] Iteration 1200, loss = 0.403025
I0803 17:54:18.181430  6069 solver.cpp:244]     Train net output #0: local/loss = 0.209383 (* 1 = 0.209383 loss)
I0803 17:54:18.181442  6069 solver.cpp:244]     Train net output #1: loss = 0.185105 (* 1 = 0.185105 loss)
I0803 17:54:18.181452  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00853754 (* 1 = 0.00853754 loss)
I0803 17:54:18.181464  6069 sgd_solver.cpp:106] Iteration 1200, lr = 9.98499e-05
I0803 17:54:24.686561  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.6388 > 10) by scale factor 0.791215
I0803 17:54:26.737324  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.6649 > 10) by scale factor 0.789586
I0803 17:54:28.605132  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0131 > 10) by scale factor 0.998688
I0803 17:54:29.865339  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.1418 > 10) by scale factor 0.760933
I0803 17:54:30.478304  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.715 > 10) by scale factor 0.786474
I0803 17:54:30.922684  6069 solver.cpp:228] Iteration 1220, loss = 0.459033
I0803 17:54:30.922741  6069 solver.cpp:244]     Train net output #0: local/loss = 0.307335 (* 1 = 0.307335 loss)
I0803 17:54:30.922754  6069 solver.cpp:244]     Train net output #1: loss = 0.144402 (* 1 = 0.144402 loss)
I0803 17:54:30.922763  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00729505 (* 1 = 0.00729505 loss)
I0803 17:54:30.922776  6069 sgd_solver.cpp:106] Iteration 1220, lr = 9.98474e-05
I0803 17:54:31.091789  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.7796 > 10) by scale factor 0.782496
I0803 17:54:31.704030  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.2958 > 10) by scale factor 0.613656
I0803 17:54:32.438204  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.9304 > 10) by scale factor 0.62773
I0803 17:54:33.049963  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.8797 > 10) by scale factor 0.841776
I0803 17:54:33.874156  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.4907 > 10) by scale factor 0.6901
I0803 17:54:34.515775  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.3114 > 10) by scale factor 0.751237
I0803 17:54:35.127656  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.3524 > 10) by scale factor 0.809561
I0803 17:54:36.498289  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7238 > 10) by scale factor 0.932503
I0803 17:54:43.697095  6069 solver.cpp:228] Iteration 1240, loss = 0.258299
I0803 17:54:43.697154  6069 solver.cpp:244]     Train net output #0: local/loss = 0.13589 (* 1 = 0.13589 loss)
I0803 17:54:43.697167  6069 solver.cpp:244]     Train net output #1: loss = 0.109995 (* 1 = 0.109995 loss)
I0803 17:54:43.697177  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0124139 (* 1 = 0.0124139 loss)
I0803 17:54:43.697190  6069 sgd_solver.cpp:106] Iteration 1240, lr = 9.98449e-05
I0803 17:54:56.501404  6069 solver.cpp:228] Iteration 1260, loss = 0.330612
I0803 17:54:56.501463  6069 solver.cpp:244]     Train net output #0: local/loss = 0.186349 (* 1 = 0.186349 loss)
I0803 17:54:56.501477  6069 solver.cpp:244]     Train net output #1: loss = 0.134041 (* 1 = 0.134041 loss)
I0803 17:54:56.501485  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0102225 (* 1 = 0.0102225 loss)
I0803 17:54:56.501499  6069 sgd_solver.cpp:106] Iteration 1260, lr = 9.98424e-05
I0803 17:55:08.536607  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.0008 > 10) by scale factor 0.999916
I0803 17:55:09.656271  6069 solver.cpp:228] Iteration 1280, loss = 0.343915
I0803 17:55:09.656363  6069 solver.cpp:244]     Train net output #0: local/loss = 0.230189 (* 1 = 0.230189 loss)
I0803 17:55:09.656378  6069 solver.cpp:244]     Train net output #1: loss = 0.107607 (* 1 = 0.107607 loss)
I0803 17:55:09.656388  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00611837 (* 1 = 0.00611837 loss)
I0803 17:55:09.656401  6069 sgd_solver.cpp:106] Iteration 1280, lr = 9.98399e-05
I0803 17:55:13.721004  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.1315 > 10) by scale factor 0.987021
I0803 17:55:14.332581  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.2323 > 10) by scale factor 0.81751
I0803 17:55:15.607020  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.3616 > 10) by scale factor 0.9651
I0803 17:55:16.218960  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7224 > 10) by scale factor 0.93263
I0803 17:55:17.699617  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.1901 > 10) by scale factor 0.820339
I0803 17:55:22.593267  6069 solver.cpp:228] Iteration 1300, loss = 0.25246
I0803 17:55:22.593346  6069 solver.cpp:244]     Train net output #0: local/loss = 0.126427 (* 1 = 0.126427 loss)
I0803 17:55:22.593361  6069 solver.cpp:244]     Train net output #1: loss = 0.118881 (* 1 = 0.118881 loss)
I0803 17:55:22.593371  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00715152 (* 1 = 0.00715152 loss)
I0803 17:55:22.593385  6069 sgd_solver.cpp:106] Iteration 1300, lr = 9.98374e-05
I0803 17:55:35.777406  6069 solver.cpp:228] Iteration 1320, loss = 0.358411
I0803 17:55:35.777480  6069 solver.cpp:244]     Train net output #0: local/loss = 0.234002 (* 1 = 0.234002 loss)
I0803 17:55:35.777498  6069 solver.cpp:244]     Train net output #1: loss = 0.119368 (* 1 = 0.119368 loss)
I0803 17:55:35.777510  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00504061 (* 1 = 0.00504061 loss)
I0803 17:55:35.777524  6069 sgd_solver.cpp:106] Iteration 1320, lr = 9.98349e-05
I0803 17:55:39.688630  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.4945 > 10) by scale factor 0.869985
I0803 17:55:40.301410  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.3787 > 10) by scale factor 0.575416
I0803 17:55:42.739421  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.6883 > 10) by scale factor 0.935603
I0803 17:55:43.351678  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.5747 > 10) by scale factor 0.795249
I0803 17:55:43.965227  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.1898 > 10) by scale factor 0.658337
I0803 17:55:44.673197  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.5442 > 10) by scale factor 0.797182
I0803 17:55:45.286054  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.6044 > 10) by scale factor 0.943002
I0803 17:55:45.897423  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.7703 > 10) by scale factor 0.928482
I0803 17:55:46.626767  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.0177 > 10) by scale factor 0.768185
I0803 17:55:47.239553  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.2576 > 10) by scale factor 0.815818
I0803 17:55:47.852013  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.0128 > 10) by scale factor 0.768474
I0803 17:55:48.303946  6069 solver.cpp:228] Iteration 1340, loss = 0.350865
I0803 17:55:48.304013  6069 solver.cpp:244]     Train net output #0: local/loss = 0.222167 (* 1 = 0.222167 loss)
I0803 17:55:48.304026  6069 solver.cpp:244]     Train net output #1: loss = 0.124807 (* 1 = 0.124807 loss)
I0803 17:55:48.304036  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00389113 (* 1 = 0.00389113 loss)
I0803 17:55:48.304052  6069 sgd_solver.cpp:106] Iteration 1340, lr = 9.98324e-05
I0803 17:55:48.472573  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.1813 > 10) by scale factor 0.894353
I0803 17:55:49.084903  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.0148 > 10) by scale factor 0.907866
I0803 17:55:51.845298  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.6798 > 10) by scale factor 0.936345
I0803 17:55:52.539881  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.1004 > 10) by scale factor 0.990055
I0803 17:55:57.035895  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.4643 > 10) by scale factor 0.955629
I0803 17:55:58.298837  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.8078 > 10) by scale factor 0.724231
I0803 17:55:59.115103  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.0239 > 10) by scale factor 0.831678
I0803 17:55:59.725476  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 21.5793 > 10) by scale factor 0.463408
I0803 17:56:00.365387  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.3522 > 10) by scale factor 0.748942
I0803 17:56:01.102061  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 14.6627 > 10) by scale factor 0.682002
I0803 17:56:01.546303  6069 solver.cpp:228] Iteration 1360, loss = 0.530897
I0803 17:56:01.546375  6069 solver.cpp:244]     Train net output #0: local/loss = 0.398172 (* 1 = 0.398172 loss)
I0803 17:56:01.546389  6069 solver.cpp:244]     Train net output #1: loss = 0.126191 (* 1 = 0.126191 loss)
I0803 17:56:01.546401  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00653398 (* 1 = 0.00653398 loss)
I0803 17:56:01.546412  6069 sgd_solver.cpp:106] Iteration 1360, lr = 9.98298e-05
I0803 17:56:01.714129  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.0357 > 10) by scale factor 0.665085
I0803 17:56:02.371650  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 15.1957 > 10) by scale factor 0.658082
I0803 17:56:03.246207  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.3029 > 10) by scale factor 0.546361
I0803 17:56:03.859063  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.2834 > 10) by scale factor 0.518581
I0803 17:56:04.542099  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.9286 > 10) by scale factor 0.5283
I0803 17:56:05.154815  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.7044 > 10) by scale factor 0.421862
I0803 17:56:05.766633  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 23.1325 > 10) by scale factor 0.432292
I0803 17:56:06.379473  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.9943 > 10) by scale factor 0.434891
I0803 17:56:07.039767  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 20.7359 > 10) by scale factor 0.482256
I0803 17:56:07.682294  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 22.4347 > 10) by scale factor 0.445739
I0803 17:56:08.298301  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.0809 > 10) by scale factor 0.58545
I0803 17:56:08.911257  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 19.0229 > 10) by scale factor 0.525682
I0803 17:56:09.523393  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 16.5736 > 10) by scale factor 0.60337
I0803 17:56:10.202478  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 17.6585 > 10) by scale factor 0.5663
I0803 17:56:10.815224  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 18.2591 > 10) by scale factor 0.547672
I0803 17:56:11.428154  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.1991 > 10) by scale factor 0.757629
I0803 17:56:12.041604  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 13.601 > 10) by scale factor 0.735238
I0803 17:56:12.873029  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 12.6839 > 10) by scale factor 0.788398
I0803 17:56:14.093228  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 11.5256 > 10) by scale factor 0.867633
I0803 17:56:14.721292  6069 solver.cpp:228] Iteration 1380, loss = 0.347087
I0803 17:56:14.721365  6069 solver.cpp:244]     Train net output #0: local/loss = 0.177435 (* 1 = 0.177435 loss)
I0803 17:56:14.721377  6069 solver.cpp:244]     Train net output #1: loss = 0.159649 (* 1 = 0.159649 loss)
I0803 17:56:14.721388  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0100032 (* 1 = 0.0100032 loss)
I0803 17:56:14.721402  6069 sgd_solver.cpp:106] Iteration 1380, lr = 9.98273e-05
I0803 17:56:14.888953  6069 sgd_solver.cpp:92] Gradient clipping: scaling down gradients (L2 norm 10.5308 > 10) by scale factor 0.949595
I0803 17:56:27.657742  6069 solver.cpp:228] Iteration 1400, loss = 0.273164
I0803 17:56:27.657822  6069 solver.cpp:244]     Train net output #0: local/loss = 0.160317 (* 1 = 0.160317 loss)
I0803 17:56:27.657842  6069 solver.cpp:244]     Train net output #1: loss = 0.0999098 (* 1 = 0.0999098 loss)
I0803 17:56:27.657857  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0129376 (* 1 = 0.0129376 loss)
I0803 17:56:27.657876  6069 sgd_solver.cpp:106] Iteration 1400, lr = 9.98248e-05
I0803 17:56:40.639600  6069 solver.cpp:228] Iteration 1420, loss = 0.28833
I0803 17:56:40.639765  6069 solver.cpp:244]     Train net output #0: local/loss = 0.153767 (* 1 = 0.153767 loss)
I0803 17:56:40.639783  6069 solver.cpp:244]     Train net output #1: loss = 0.1248 (* 1 = 0.1248 loss)
I0803 17:56:40.639796  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00976247 (* 1 = 0.00976247 loss)
I0803 17:56:40.639811  6069 sgd_solver.cpp:106] Iteration 1420, lr = 9.98223e-05
I0803 17:56:54.581153  6069 solver.cpp:228] Iteration 1440, loss = 0.21976
I0803 17:56:54.581238  6069 solver.cpp:244]     Train net output #0: local/loss = 0.119092 (* 1 = 0.119092 loss)
I0803 17:56:54.581253  6069 solver.cpp:244]     Train net output #1: loss = 0.0944629 (* 1 = 0.0944629 loss)
I0803 17:56:54.581262  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00620555 (* 1 = 0.00620555 loss)
I0803 17:56:54.581287  6069 sgd_solver.cpp:106] Iteration 1440, lr = 9.98198e-05
I0803 17:57:08.138947  6069 solver.cpp:228] Iteration 1460, loss = 0.19572
I0803 17:57:08.139013  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0935068 (* 1 = 0.0935068 loss)
I0803 17:57:08.139026  6069 solver.cpp:244]     Train net output #1: loss = 0.0970678 (* 1 = 0.0970678 loss)
I0803 17:57:08.139036  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00514574 (* 1 = 0.00514574 loss)
I0803 17:57:08.139060  6069 sgd_solver.cpp:106] Iteration 1460, lr = 9.98173e-05
I0803 17:57:20.612117  6069 solver.cpp:228] Iteration 1480, loss = 0.211058
I0803 17:57:20.612366  6069 solver.cpp:244]     Train net output #0: local/loss = 0.106119 (* 1 = 0.106119 loss)
I0803 17:57:20.612411  6069 solver.cpp:244]     Train net output #1: loss = 0.101324 (* 1 = 0.101324 loss)
I0803 17:57:20.612426  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00361527 (* 1 = 0.00361527 loss)
I0803 17:57:20.612444  6069 sgd_solver.cpp:106] Iteration 1480, lr = 9.98148e-05
I0803 17:57:33.907523  6069 solver.cpp:228] Iteration 1500, loss = 0.204536
I0803 17:57:33.907611  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0993502 (* 1 = 0.0993502 loss)
I0803 17:57:33.907629  6069 solver.cpp:244]     Train net output #1: loss = 0.102541 (* 1 = 0.102541 loss)
I0803 17:57:33.907641  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00264434 (* 1 = 0.00264434 loss)
I0803 17:57:33.907665  6069 sgd_solver.cpp:106] Iteration 1500, lr = 9.98123e-05
I0803 17:57:46.390933  6069 solver.cpp:228] Iteration 1520, loss = 0.211756
I0803 17:57:46.391011  6069 solver.cpp:244]     Train net output #0: local/loss = 0.103417 (* 1 = 0.103417 loss)
I0803 17:57:46.391026  6069 solver.cpp:244]     Train net output #1: loss = 0.106365 (* 1 = 0.106365 loss)
I0803 17:57:46.391036  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00197356 (* 1 = 0.00197356 loss)
I0803 17:57:46.391047  6069 sgd_solver.cpp:106] Iteration 1520, lr = 9.98098e-05
I0803 17:57:59.886579  6069 solver.cpp:228] Iteration 1540, loss = 0.259487
I0803 17:57:59.886751  6069 solver.cpp:244]     Train net output #0: local/loss = 0.14111 (* 1 = 0.14111 loss)
I0803 17:57:59.886770  6069 solver.cpp:244]     Train net output #1: loss = 0.117313 (* 1 = 0.117313 loss)
I0803 17:57:59.886780  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00106387 (* 1 = 0.00106387 loss)
I0803 17:57:59.886795  6069 sgd_solver.cpp:106] Iteration 1540, lr = 9.98073e-05
I0803 17:58:13.366736  6069 solver.cpp:228] Iteration 1560, loss = 0.249128
I0803 17:58:13.366807  6069 solver.cpp:244]     Train net output #0: local/loss = 0.128605 (* 1 = 0.128605 loss)
I0803 17:58:13.366823  6069 solver.cpp:244]     Train net output #1: loss = 0.119246 (* 1 = 0.119246 loss)
I0803 17:58:13.366835  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.0012763 (* 1 = 0.0012763 loss)
I0803 17:58:13.366852  6069 sgd_solver.cpp:106] Iteration 1560, lr = 9.98048e-05
I0803 17:58:26.063508  6069 solver.cpp:228] Iteration 1580, loss = 0.278329
I0803 17:58:26.063578  6069 solver.cpp:244]     Train net output #0: local/loss = 0.14477 (* 1 = 0.14477 loss)
I0803 17:58:26.063591  6069 solver.cpp:244]     Train net output #1: loss = 0.13289 (* 1 = 0.13289 loss)
I0803 17:58:26.063601  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000669583 (* 1 = 0.000669583 loss)
I0803 17:58:26.063614  6069 sgd_solver.cpp:106] Iteration 1580, lr = 9.98023e-05
I0803 17:58:39.159715  6069 solver.cpp:228] Iteration 1600, loss = 0.304408
I0803 17:58:39.159971  6069 solver.cpp:244]     Train net output #0: local/loss = 0.155272 (* 1 = 0.155272 loss)
I0803 17:58:39.160007  6069 solver.cpp:244]     Train net output #1: loss = 0.14831 (* 1 = 0.14831 loss)
I0803 17:58:39.160023  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000826075 (* 1 = 0.000826075 loss)
I0803 17:58:39.160040  6069 sgd_solver.cpp:106] Iteration 1600, lr = 9.97998e-05
I0803 17:58:51.854457  6069 solver.cpp:228] Iteration 1620, loss = 0.257211
I0803 17:58:51.854526  6069 solver.cpp:244]     Train net output #0: local/loss = 0.136299 (* 1 = 0.136299 loss)
I0803 17:58:51.854547  6069 solver.cpp:244]     Train net output #1: loss = 0.120242 (* 1 = 0.120242 loss)
I0803 17:58:51.854564  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00067007 (* 1 = 0.00067007 loss)
I0803 17:58:51.854583  6069 sgd_solver.cpp:106] Iteration 1620, lr = 9.97973e-05
I0803 17:59:04.699980  6069 solver.cpp:228] Iteration 1640, loss = 0.237024
I0803 17:59:04.700053  6069 solver.cpp:244]     Train net output #0: local/loss = 0.1237 (* 1 = 0.1237 loss)
I0803 17:59:04.700067  6069 solver.cpp:244]     Train net output #1: loss = 0.112909 (* 1 = 0.112909 loss)
I0803 17:59:04.700076  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000414625 (* 1 = 0.000414625 loss)
I0803 17:59:04.700089  6069 sgd_solver.cpp:106] Iteration 1640, lr = 9.97948e-05
I0803 17:59:17.838438  6069 solver.cpp:228] Iteration 1660, loss = 0.19033
I0803 17:59:17.846020  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0946579 (* 1 = 0.0946579 loss)
I0803 17:59:17.846055  6069 solver.cpp:244]     Train net output #1: loss = 0.0952527 (* 1 = 0.0952527 loss)
I0803 17:59:17.846071  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000419385 (* 1 = 0.000419385 loss)
I0803 17:59:17.846086  6069 sgd_solver.cpp:106] Iteration 1660, lr = 9.97923e-05
I0803 17:59:30.722340  6069 solver.cpp:228] Iteration 1680, loss = 0.217091
I0803 17:59:30.722410  6069 solver.cpp:244]     Train net output #0: local/loss = 0.108438 (* 1 = 0.108438 loss)
I0803 17:59:30.722424  6069 solver.cpp:244]     Train net output #1: loss = 0.108483 (* 1 = 0.108483 loss)
I0803 17:59:30.722434  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000169525 (* 1 = 0.000169525 loss)
I0803 17:59:30.722446  6069 sgd_solver.cpp:106] Iteration 1680, lr = 9.97898e-05
I0803 17:59:43.794457  6069 solver.cpp:228] Iteration 1700, loss = 0.21666
I0803 17:59:43.794548  6069 solver.cpp:244]     Train net output #0: local/loss = 0.112023 (* 1 = 0.112023 loss)
I0803 17:59:43.794562  6069 solver.cpp:244]     Train net output #1: loss = 0.104494 (* 1 = 0.104494 loss)
I0803 17:59:43.794572  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000143294 (* 1 = 0.000143294 loss)
I0803 17:59:43.794585  6069 sgd_solver.cpp:106] Iteration 1700, lr = 9.97873e-05
I0803 17:59:57.266758  6069 solver.cpp:228] Iteration 1720, loss = 0.240449
I0803 17:59:57.266954  6069 solver.cpp:244]     Train net output #0: local/loss = 0.118482 (* 1 = 0.118482 loss)
I0803 17:59:57.266988  6069 solver.cpp:244]     Train net output #1: loss = 0.121884 (* 1 = 0.121884 loss)
I0803 17:59:57.267004  6069 solver.cpp:244]     Train net output #2: theta_loss = 8.26085e-05 (* 1 = 8.26085e-05 loss)
I0803 17:59:57.267020  6069 sgd_solver.cpp:106] Iteration 1720, lr = 9.97848e-05
I0803 18:00:10.407104  6069 solver.cpp:228] Iteration 1740, loss = 0.22243
I0803 18:00:10.407168  6069 solver.cpp:244]     Train net output #0: local/loss = 0.114944 (* 1 = 0.114944 loss)
I0803 18:00:10.407181  6069 solver.cpp:244]     Train net output #1: loss = 0.10742 (* 1 = 0.10742 loss)
I0803 18:00:10.407191  6069 solver.cpp:244]     Train net output #2: theta_loss = 6.57984e-05 (* 1 = 6.57984e-05 loss)
I0803 18:00:10.407203  6069 sgd_solver.cpp:106] Iteration 1740, lr = 9.97823e-05
I0803 18:00:23.083443  6069 solver.cpp:228] Iteration 1760, loss = 0.217838
I0803 18:00:23.083526  6069 solver.cpp:244]     Train net output #0: local/loss = 0.114658 (* 1 = 0.114658 loss)
I0803 18:00:23.083540  6069 solver.cpp:244]     Train net output #1: loss = 0.102944 (* 1 = 0.102944 loss)
I0803 18:00:23.083550  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000235498 (* 1 = 0.000235498 loss)
I0803 18:00:23.083562  6069 sgd_solver.cpp:106] Iteration 1760, lr = 9.97798e-05
I0803 18:00:36.078856  6069 solver.cpp:228] Iteration 1780, loss = 0.18689
I0803 18:00:36.079121  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0922738 (* 1 = 0.0922738 loss)
I0803 18:00:36.079155  6069 solver.cpp:244]     Train net output #1: loss = 0.0943731 (* 1 = 0.0943731 loss)
I0803 18:00:36.079170  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00024338 (* 1 = 0.00024338 loss)
I0803 18:00:36.079186  6069 sgd_solver.cpp:106] Iteration 1780, lr = 9.97773e-05
I0803 18:00:48.706430  6069 solver.cpp:228] Iteration 1800, loss = 0.221472
I0803 18:00:48.706504  6069 solver.cpp:244]     Train net output #0: local/loss = 0.111464 (* 1 = 0.111464 loss)
I0803 18:00:48.706517  6069 solver.cpp:244]     Train net output #1: loss = 0.10983 (* 1 = 0.10983 loss)
I0803 18:00:48.706527  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000178021 (* 1 = 0.000178021 loss)
I0803 18:00:48.706539  6069 sgd_solver.cpp:106] Iteration 1800, lr = 9.97747e-05
I0803 18:01:01.742475  6069 solver.cpp:228] Iteration 1820, loss = 0.24333
I0803 18:01:01.742534  6069 solver.cpp:244]     Train net output #0: local/loss = 0.121996 (* 1 = 0.121996 loss)
I0803 18:01:01.742547  6069 solver.cpp:244]     Train net output #1: loss = 0.121064 (* 1 = 0.121064 loss)
I0803 18:01:01.742558  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000269854 (* 1 = 0.000269854 loss)
I0803 18:01:01.742569  6069 sgd_solver.cpp:106] Iteration 1820, lr = 9.97722e-05
I0803 18:01:14.200104  6069 solver.cpp:228] Iteration 1840, loss = 0.242311
I0803 18:01:14.200333  6069 solver.cpp:244]     Train net output #0: local/loss = 0.121637 (* 1 = 0.121637 loss)
I0803 18:01:14.200368  6069 solver.cpp:244]     Train net output #1: loss = 0.120407 (* 1 = 0.120407 loss)
I0803 18:01:14.200382  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000267467 (* 1 = 0.000267467 loss)
I0803 18:01:14.200399  6069 sgd_solver.cpp:106] Iteration 1840, lr = 9.97697e-05
I0803 18:01:27.707700  6069 solver.cpp:228] Iteration 1860, loss = 0.223518
I0803 18:01:27.707767  6069 solver.cpp:244]     Train net output #0: local/loss = 0.105805 (* 1 = 0.105805 loss)
I0803 18:01:27.707785  6069 solver.cpp:244]     Train net output #1: loss = 0.117469 (* 1 = 0.117469 loss)
I0803 18:01:27.707799  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000243275 (* 1 = 0.000243275 loss)
I0803 18:01:27.707818  6069 sgd_solver.cpp:106] Iteration 1860, lr = 9.97672e-05
I0803 18:01:40.323926  6069 solver.cpp:228] Iteration 1880, loss = 0.213827
I0803 18:01:40.324007  6069 solver.cpp:244]     Train net output #0: local/loss = 0.10794 (* 1 = 0.10794 loss)
I0803 18:01:40.324025  6069 solver.cpp:244]     Train net output #1: loss = 0.105708 (* 1 = 0.105708 loss)
I0803 18:01:40.324043  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000178855 (* 1 = 0.000178855 loss)
I0803 18:01:40.324061  6069 sgd_solver.cpp:106] Iteration 1880, lr = 9.97647e-05
I0803 18:01:53.664835  6069 solver.cpp:228] Iteration 1900, loss = 0.222178
I0803 18:01:53.665056  6069 solver.cpp:244]     Train net output #0: local/loss = 0.108814 (* 1 = 0.108814 loss)
I0803 18:01:53.665094  6069 solver.cpp:244]     Train net output #1: loss = 0.113199 (* 1 = 0.113199 loss)
I0803 18:01:53.665112  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00016593 (* 1 = 0.00016593 loss)
I0803 18:01:53.665130  6069 sgd_solver.cpp:106] Iteration 1900, lr = 9.97622e-05
I0803 18:02:09.451303  6069 solver.cpp:228] Iteration 1920, loss = 0.208929
I0803 18:02:09.451387  6069 solver.cpp:244]     Train net output #0: local/loss = 0.105868 (* 1 = 0.105868 loss)
I0803 18:02:09.451403  6069 solver.cpp:244]     Train net output #1: loss = 0.102736 (* 1 = 0.102736 loss)
I0803 18:02:09.451416  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000324527 (* 1 = 0.000324527 loss)
I0803 18:02:09.451431  6069 sgd_solver.cpp:106] Iteration 1920, lr = 9.97597e-05
I0803 18:02:23.282713  6069 solver.cpp:228] Iteration 1940, loss = 0.232315
I0803 18:02:23.282784  6069 solver.cpp:244]     Train net output #0: local/loss = 0.114547 (* 1 = 0.114547 loss)
I0803 18:02:23.282799  6069 solver.cpp:244]     Train net output #1: loss = 0.117445 (* 1 = 0.117445 loss)
I0803 18:02:23.282810  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000322268 (* 1 = 0.000322268 loss)
I0803 18:02:23.282824  6069 sgd_solver.cpp:106] Iteration 1940, lr = 9.97572e-05
I0803 18:02:36.642273  6069 solver.cpp:228] Iteration 1960, loss = 0.212324
I0803 18:02:36.642441  6069 solver.cpp:244]     Train net output #0: local/loss = 0.110381 (* 1 = 0.110381 loss)
I0803 18:02:36.642472  6069 solver.cpp:244]     Train net output #1: loss = 0.101425 (* 1 = 0.101425 loss)
I0803 18:02:36.642482  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00051925 (* 1 = 0.00051925 loss)
I0803 18:02:36.642495  6069 sgd_solver.cpp:106] Iteration 1960, lr = 9.97547e-05
I0803 18:02:50.059485  6069 solver.cpp:228] Iteration 1980, loss = 0.245838
I0803 18:02:50.059562  6069 solver.cpp:244]     Train net output #0: local/loss = 0.117012 (* 1 = 0.117012 loss)
I0803 18:02:50.059576  6069 solver.cpp:244]     Train net output #1: loss = 0.128482 (* 1 = 0.128482 loss)
I0803 18:02:50.059587  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000343307 (* 1 = 0.000343307 loss)
I0803 18:02:50.059598  6069 sgd_solver.cpp:106] Iteration 1980, lr = 9.97522e-05
I0803 18:03:02.953351  6069 solver.cpp:337] Iteration 2000, Testing net (#0)
I0803 18:03:55.373888  6069 solver.cpp:404]     Test net output #0: local/loss = 0.109831 (* 1 = 0.109831 loss)
I0803 18:03:55.374035  6069 solver.cpp:404]     Test net output #1: loss = 0.106165 (* 1 = 0.106165 loss)
I0803 18:03:55.374050  6069 solver.cpp:404]     Test net output #2: theta_loss = 0.000180474 (* 1 = 0.000180474 loss)
I0803 18:03:55.739100  6069 solver.cpp:228] Iteration 2000, loss = 0.22983
I0803 18:03:55.739166  6069 solver.cpp:244]     Train net output #0: local/loss = 0.115917 (* 1 = 0.115917 loss)
I0803 18:03:55.739184  6069 solver.cpp:244]     Train net output #1: loss = 0.113749 (* 1 = 0.113749 loss)
I0803 18:03:55.739202  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000163573 (* 1 = 0.000163573 loss)
I0803 18:03:55.739226  6069 sgd_solver.cpp:106] Iteration 2000, lr = 9.97497e-05
I0803 18:04:08.116341  6069 solver.cpp:228] Iteration 2020, loss = 0.206492
I0803 18:04:08.116411  6069 solver.cpp:244]     Train net output #0: local/loss = 0.103646 (* 1 = 0.103646 loss)
I0803 18:04:08.116435  6069 solver.cpp:244]     Train net output #1: loss = 0.102802 (* 1 = 0.102802 loss)
I0803 18:04:08.116447  6069 solver.cpp:244]     Train net output #2: theta_loss = 4.44019e-05 (* 1 = 4.44019e-05 loss)
I0803 18:04:08.116459  6069 sgd_solver.cpp:106] Iteration 2020, lr = 9.97472e-05
I0803 18:04:23.599956  6069 solver.cpp:228] Iteration 2040, loss = 0.280438
I0803 18:04:23.600025  6069 solver.cpp:244]     Train net output #0: local/loss = 0.139944 (* 1 = 0.139944 loss)
I0803 18:04:23.600039  6069 solver.cpp:244]     Train net output #1: loss = 0.140434 (* 1 = 0.140434 loss)
I0803 18:04:23.600049  6069 solver.cpp:244]     Train net output #2: theta_loss = 5.94186e-05 (* 1 = 5.94186e-05 loss)
I0803 18:04:23.600061  6069 sgd_solver.cpp:106] Iteration 2040, lr = 9.97447e-05
I0803 18:04:37.281728  6069 solver.cpp:228] Iteration 2060, loss = 0.201238
I0803 18:04:37.281934  6069 solver.cpp:244]     Train net output #0: local/loss = 0.103297 (* 1 = 0.103297 loss)
I0803 18:04:37.281970  6069 solver.cpp:244]     Train net output #1: loss = 0.0978903 (* 1 = 0.0978903 loss)
I0803 18:04:37.281985  6069 solver.cpp:244]     Train net output #2: theta_loss = 5.052e-05 (* 1 = 5.052e-05 loss)
I0803 18:04:37.282002  6069 sgd_solver.cpp:106] Iteration 2060, lr = 9.97422e-05
I0803 18:04:52.704162  6069 solver.cpp:228] Iteration 2080, loss = 0.23791
I0803 18:04:52.704226  6069 solver.cpp:244]     Train net output #0: local/loss = 0.123721 (* 1 = 0.123721 loss)
I0803 18:04:52.704242  6069 solver.cpp:244]     Train net output #1: loss = 0.114111 (* 1 = 0.114111 loss)
I0803 18:04:52.704257  6069 solver.cpp:244]     Train net output #2: theta_loss = 7.83098e-05 (* 1 = 7.83098e-05 loss)
I0803 18:04:52.704273  6069 sgd_solver.cpp:106] Iteration 2080, lr = 9.97397e-05
I0803 18:05:17.970970  6069 solver.cpp:228] Iteration 2100, loss = 0.25233
I0803 18:05:17.971207  6069 solver.cpp:244]     Train net output #0: local/loss = 0.115896 (* 1 = 0.115896 loss)
I0803 18:05:17.971230  6069 solver.cpp:244]     Train net output #1: loss = 0.136148 (* 1 = 0.136148 loss)
I0803 18:05:17.971236  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000285222 (* 1 = 0.000285222 loss)
I0803 18:05:17.971247  6069 sgd_solver.cpp:106] Iteration 2100, lr = 9.97372e-05
I0803 18:05:34.684275  6069 solver.cpp:228] Iteration 2120, loss = 0.207302
I0803 18:05:34.684357  6069 solver.cpp:244]     Train net output #0: local/loss = 0.108087 (* 1 = 0.108087 loss)
I0803 18:05:34.684372  6069 solver.cpp:244]     Train net output #1: loss = 0.0989926 (* 1 = 0.0989926 loss)
I0803 18:05:34.684382  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000222755 (* 1 = 0.000222755 loss)
I0803 18:05:34.684396  6069 sgd_solver.cpp:106] Iteration 2120, lr = 9.97346e-05
I0803 18:05:48.827241  6069 solver.cpp:228] Iteration 2140, loss = 0.207538
I0803 18:05:48.827508  6069 solver.cpp:244]     Train net output #0: local/loss = 0.112275 (* 1 = 0.112275 loss)
I0803 18:05:48.827564  6069 solver.cpp:244]     Train net output #1: loss = 0.0950501 (* 1 = 0.0950501 loss)
I0803 18:05:48.827580  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000212707 (* 1 = 0.000212707 loss)
I0803 18:05:48.827602  6069 sgd_solver.cpp:106] Iteration 2140, lr = 9.97321e-05
I0803 18:06:04.169461  6069 solver.cpp:228] Iteration 2160, loss = 0.228326
I0803 18:06:04.169528  6069 solver.cpp:244]     Train net output #0: local/loss = 0.113655 (* 1 = 0.113655 loss)
I0803 18:06:04.169540  6069 solver.cpp:244]     Train net output #1: loss = 0.114446 (* 1 = 0.114446 loss)
I0803 18:06:04.169551  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000225101 (* 1 = 0.000225101 loss)
I0803 18:06:04.169564  6069 sgd_solver.cpp:106] Iteration 2160, lr = 9.97296e-05
I0803 18:06:19.862316  6069 solver.cpp:228] Iteration 2180, loss = 0.211429
I0803 18:06:19.862519  6069 solver.cpp:244]     Train net output #0: local/loss = 0.105668 (* 1 = 0.105668 loss)
I0803 18:06:19.862555  6069 solver.cpp:244]     Train net output #1: loss = 0.105395 (* 1 = 0.105395 loss)
I0803 18:06:19.862571  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000365439 (* 1 = 0.000365439 loss)
I0803 18:06:19.862596  6069 sgd_solver.cpp:106] Iteration 2180, lr = 9.97271e-05
I0803 18:06:33.784364  6069 solver.cpp:228] Iteration 2200, loss = 0.242215
I0803 18:06:33.784432  6069 solver.cpp:244]     Train net output #0: local/loss = 0.11943 (* 1 = 0.11943 loss)
I0803 18:06:33.784446  6069 solver.cpp:244]     Train net output #1: loss = 0.12258 (* 1 = 0.12258 loss)
I0803 18:06:33.784456  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00020475 (* 1 = 0.00020475 loss)
I0803 18:06:33.784468  6069 sgd_solver.cpp:106] Iteration 2200, lr = 9.97246e-05
I0803 18:06:53.863122  6069 solver.cpp:228] Iteration 2220, loss = 0.233969
I0803 18:06:53.863258  6069 solver.cpp:244]     Train net output #0: local/loss = 0.119893 (* 1 = 0.119893 loss)
I0803 18:06:53.863273  6069 solver.cpp:244]     Train net output #1: loss = 0.113867 (* 1 = 0.113867 loss)
I0803 18:06:53.863284  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000209496 (* 1 = 0.000209496 loss)
I0803 18:06:53.863298  6069 sgd_solver.cpp:106] Iteration 2220, lr = 9.97221e-05
I0803 18:07:19.051815  6069 solver.cpp:228] Iteration 2240, loss = 0.213346
I0803 18:07:19.051893  6069 solver.cpp:244]     Train net output #0: local/loss = 0.103758 (* 1 = 0.103758 loss)
I0803 18:07:19.051908  6069 solver.cpp:244]     Train net output #1: loss = 0.109195 (* 1 = 0.109195 loss)
I0803 18:07:19.051918  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000393372 (* 1 = 0.000393372 loss)
I0803 18:07:19.051928  6069 sgd_solver.cpp:106] Iteration 2240, lr = 9.97196e-05
I0803 18:07:43.016710  6069 solver.cpp:228] Iteration 2260, loss = 0.244815
I0803 18:07:43.016994  6069 solver.cpp:244]     Train net output #0: local/loss = 0.122653 (* 1 = 0.122653 loss)
I0803 18:07:43.017040  6069 solver.cpp:244]     Train net output #1: loss = 0.121771 (* 1 = 0.121771 loss)
I0803 18:07:43.017055  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000391814 (* 1 = 0.000391814 loss)
I0803 18:07:43.017072  6069 sgd_solver.cpp:106] Iteration 2260, lr = 9.97171e-05
I0803 18:08:08.196444  6069 solver.cpp:228] Iteration 2280, loss = 0.250241
I0803 18:08:08.196509  6069 solver.cpp:244]     Train net output #0: local/loss = 0.135269 (* 1 = 0.135269 loss)
I0803 18:08:08.196522  6069 solver.cpp:244]     Train net output #1: loss = 0.114847 (* 1 = 0.114847 loss)
I0803 18:08:08.196532  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000125455 (* 1 = 0.000125455 loss)
I0803 18:08:08.196544  6069 sgd_solver.cpp:106] Iteration 2280, lr = 9.97146e-05
I0803 18:08:33.372661  6069 solver.cpp:228] Iteration 2300, loss = 0.251148
I0803 18:08:33.372884  6069 solver.cpp:244]     Train net output #0: local/loss = 0.139395 (* 1 = 0.139395 loss)
I0803 18:08:33.372921  6069 solver.cpp:244]     Train net output #1: loss = 0.111721 (* 1 = 0.111721 loss)
I0803 18:08:33.372937  6069 solver.cpp:244]     Train net output #2: theta_loss = 3.13441e-05 (* 1 = 3.13441e-05 loss)
I0803 18:08:33.372962  6069 sgd_solver.cpp:106] Iteration 2300, lr = 9.97121e-05
I0803 18:08:52.361706  6069 solver.cpp:228] Iteration 2320, loss = 0.224685
I0803 18:08:52.361788  6069 solver.cpp:244]     Train net output #0: local/loss = 0.113698 (* 1 = 0.113698 loss)
I0803 18:08:52.361804  6069 solver.cpp:244]     Train net output #1: loss = 0.110716 (* 1 = 0.110716 loss)
I0803 18:08:52.361812  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00027186 (* 1 = 0.00027186 loss)
I0803 18:08:52.361832  6069 sgd_solver.cpp:106] Iteration 2320, lr = 9.97096e-05
I0803 18:09:14.274076  6069 solver.cpp:228] Iteration 2340, loss = 0.17316
I0803 18:09:14.274235  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0849128 (* 1 = 0.0849128 loss)
I0803 18:09:14.274250  6069 solver.cpp:244]     Train net output #1: loss = 0.0875528 (* 1 = 0.0875528 loss)
I0803 18:09:14.274260  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000694347 (* 1 = 0.000694347 loss)
I0803 18:09:14.274272  6069 sgd_solver.cpp:106] Iteration 2340, lr = 9.97071e-05
I0803 18:09:36.096604  6069 solver.cpp:228] Iteration 2360, loss = 0.236437
I0803 18:09:36.096673  6069 solver.cpp:244]     Train net output #0: local/loss = 0.129347 (* 1 = 0.129347 loss)
I0803 18:09:36.096686  6069 solver.cpp:244]     Train net output #1: loss = 0.106762 (* 1 = 0.106762 loss)
I0803 18:09:36.096695  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000327243 (* 1 = 0.000327243 loss)
I0803 18:09:36.096707  6069 sgd_solver.cpp:106] Iteration 2360, lr = 9.97046e-05
I0803 18:09:52.895002  6069 solver.cpp:228] Iteration 2380, loss = 0.233552
I0803 18:09:52.895226  6069 solver.cpp:244]     Train net output #0: local/loss = 0.121977 (* 1 = 0.121977 loss)
I0803 18:09:52.895261  6069 solver.cpp:244]     Train net output #1: loss = 0.111327 (* 1 = 0.111327 loss)
I0803 18:09:52.895275  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000248301 (* 1 = 0.000248301 loss)
I0803 18:09:52.895292  6069 sgd_solver.cpp:106] Iteration 2380, lr = 9.97021e-05
I0803 18:10:08.859935  6069 solver.cpp:228] Iteration 2400, loss = 0.304547
I0803 18:10:08.860005  6069 solver.cpp:244]     Train net output #0: local/loss = 0.145458 (* 1 = 0.145458 loss)
I0803 18:10:08.860018  6069 solver.cpp:244]     Train net output #1: loss = 0.157377 (* 1 = 0.157377 loss)
I0803 18:10:08.860029  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00171214 (* 1 = 0.00171214 loss)
I0803 18:10:08.860041  6069 sgd_solver.cpp:106] Iteration 2400, lr = 9.96995e-05
I0803 18:10:24.059746  6069 solver.cpp:228] Iteration 2420, loss = 0.227161
I0803 18:10:24.059938  6069 solver.cpp:244]     Train net output #0: local/loss = 0.125544 (* 1 = 0.125544 loss)
I0803 18:10:24.059955  6069 solver.cpp:244]     Train net output #1: loss = 0.10025 (* 1 = 0.10025 loss)
I0803 18:10:24.059967  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00136661 (* 1 = 0.00136661 loss)
I0803 18:10:24.059979  6069 sgd_solver.cpp:106] Iteration 2420, lr = 9.9697e-05
I0803 18:10:44.277094  6069 solver.cpp:228] Iteration 2440, loss = 0.220232
I0803 18:10:44.277184  6069 solver.cpp:244]     Train net output #0: local/loss = 0.116656 (* 1 = 0.116656 loss)
I0803 18:10:44.277199  6069 solver.cpp:244]     Train net output #1: loss = 0.103049 (* 1 = 0.103049 loss)
I0803 18:10:44.277209  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000527471 (* 1 = 0.000527471 loss)
I0803 18:10:44.277222  6069 sgd_solver.cpp:106] Iteration 2440, lr = 9.96945e-05
I0803 18:11:04.638453  6069 solver.cpp:228] Iteration 2460, loss = 0.278757
I0803 18:11:04.638686  6069 solver.cpp:244]     Train net output #0: local/loss = 0.149972 (* 1 = 0.149972 loss)
I0803 18:11:04.638722  6069 solver.cpp:244]     Train net output #1: loss = 0.127638 (* 1 = 0.127638 loss)
I0803 18:11:04.638738  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00114654 (* 1 = 0.00114654 loss)
I0803 18:11:04.638756  6069 sgd_solver.cpp:106] Iteration 2460, lr = 9.9692e-05
I0803 18:11:28.848945  6069 solver.cpp:228] Iteration 2480, loss = 0.242085
I0803 18:11:28.849028  6069 solver.cpp:244]     Train net output #0: local/loss = 0.117176 (* 1 = 0.117176 loss)
I0803 18:11:28.849042  6069 solver.cpp:244]     Train net output #1: loss = 0.123249 (* 1 = 0.123249 loss)
I0803 18:11:28.849052  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00165981 (* 1 = 0.00165981 loss)
I0803 18:11:28.849064  6069 sgd_solver.cpp:106] Iteration 2480, lr = 9.96895e-05
I0803 18:11:46.482964  6069 solver.cpp:228] Iteration 2500, loss = 0.235086
I0803 18:11:46.483168  6069 solver.cpp:244]     Train net output #0: local/loss = 0.111514 (* 1 = 0.111514 loss)
I0803 18:11:46.483201  6069 solver.cpp:244]     Train net output #1: loss = 0.122641 (* 1 = 0.122641 loss)
I0803 18:11:46.483216  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00093078 (* 1 = 0.00093078 loss)
I0803 18:11:46.483232  6069 sgd_solver.cpp:106] Iteration 2500, lr = 9.9687e-05
I0803 18:12:10.258417  6069 solver.cpp:228] Iteration 2520, loss = 0.209793
I0803 18:12:10.258488  6069 solver.cpp:244]     Train net output #0: local/loss = 0.104373 (* 1 = 0.104373 loss)
I0803 18:12:10.258502  6069 solver.cpp:244]     Train net output #1: loss = 0.104994 (* 1 = 0.104994 loss)
I0803 18:12:10.258512  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000425888 (* 1 = 0.000425888 loss)
I0803 18:12:10.258522  6069 sgd_solver.cpp:106] Iteration 2520, lr = 9.96845e-05
I0803 18:12:29.689750  6069 solver.cpp:228] Iteration 2540, loss = 0.22057
I0803 18:12:29.689980  6069 solver.cpp:244]     Train net output #0: local/loss = 0.112195 (* 1 = 0.112195 loss)
I0803 18:12:29.690014  6069 solver.cpp:244]     Train net output #1: loss = 0.108188 (* 1 = 0.108188 loss)
I0803 18:12:29.690029  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000186431 (* 1 = 0.000186431 loss)
I0803 18:12:29.690047  6069 sgd_solver.cpp:106] Iteration 2540, lr = 9.9682e-05
I0803 18:12:51.694555  6069 solver.cpp:228] Iteration 2560, loss = 0.212704
I0803 18:12:51.694623  6069 solver.cpp:244]     Train net output #0: local/loss = 0.118637 (* 1 = 0.118637 loss)
I0803 18:12:51.694636  6069 solver.cpp:244]     Train net output #1: loss = 0.0937959 (* 1 = 0.0937959 loss)
I0803 18:12:51.694646  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000270614 (* 1 = 0.000270614 loss)
I0803 18:12:51.694658  6069 sgd_solver.cpp:106] Iteration 2560, lr = 9.96795e-05
I0803 18:13:12.567266  6069 solver.cpp:228] Iteration 2580, loss = 0.199184
I0803 18:13:12.567546  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0988763 (* 1 = 0.0988763 loss)
I0803 18:13:12.567584  6069 solver.cpp:244]     Train net output #1: loss = 0.0998962 (* 1 = 0.0998962 loss)
I0803 18:13:12.567602  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000411764 (* 1 = 0.000411764 loss)
I0803 18:13:12.567627  6069 sgd_solver.cpp:106] Iteration 2580, lr = 9.9677e-05
I0803 18:13:36.683624  6069 solver.cpp:228] Iteration 2600, loss = 0.22972
I0803 18:13:36.683687  6069 solver.cpp:244]     Train net output #0: local/loss = 0.113724 (* 1 = 0.113724 loss)
I0803 18:13:36.683701  6069 solver.cpp:244]     Train net output #1: loss = 0.115562 (* 1 = 0.115562 loss)
I0803 18:13:36.683712  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000432848 (* 1 = 0.000432848 loss)
I0803 18:13:36.683727  6069 sgd_solver.cpp:106] Iteration 2600, lr = 9.96745e-05
I0803 18:14:01.846009  6069 solver.cpp:228] Iteration 2620, loss = 0.270339
I0803 18:14:01.846227  6069 solver.cpp:244]     Train net output #0: local/loss = 0.14054 (* 1 = 0.14054 loss)
I0803 18:14:01.846262  6069 solver.cpp:244]     Train net output #1: loss = 0.129558 (* 1 = 0.129558 loss)
I0803 18:14:01.846278  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000241739 (* 1 = 0.000241739 loss)
I0803 18:14:01.846295  6069 sgd_solver.cpp:106] Iteration 2620, lr = 9.9672e-05
I0803 18:14:22.700569  6069 solver.cpp:228] Iteration 2640, loss = 0.22094
I0803 18:14:22.700652  6069 solver.cpp:244]     Train net output #0: local/loss = 0.109321 (* 1 = 0.109321 loss)
I0803 18:14:22.700666  6069 solver.cpp:244]     Train net output #1: loss = 0.111429 (* 1 = 0.111429 loss)
I0803 18:14:22.700676  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000190272 (* 1 = 0.000190272 loss)
I0803 18:14:22.700688  6069 sgd_solver.cpp:106] Iteration 2640, lr = 9.96695e-05
I0803 18:14:46.262790  6069 solver.cpp:228] Iteration 2660, loss = 0.217816
I0803 18:14:46.262989  6069 solver.cpp:244]     Train net output #0: local/loss = 0.111293 (* 1 = 0.111293 loss)
I0803 18:14:46.263021  6069 solver.cpp:244]     Train net output #1: loss = 0.106376 (* 1 = 0.106376 loss)
I0803 18:14:46.263036  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00014728 (* 1 = 0.00014728 loss)
I0803 18:14:46.263058  6069 sgd_solver.cpp:106] Iteration 2660, lr = 9.96669e-05
I0803 18:15:11.280583  6069 solver.cpp:228] Iteration 2680, loss = 0.193019
I0803 18:15:11.280660  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0994963 (* 1 = 0.0994963 loss)
I0803 18:15:11.280673  6069 solver.cpp:244]     Train net output #1: loss = 0.0934388 (* 1 = 0.0934388 loss)
I0803 18:15:11.280684  6069 solver.cpp:244]     Train net output #2: theta_loss = 8.43923e-05 (* 1 = 8.43923e-05 loss)
I0803 18:15:11.280697  6069 sgd_solver.cpp:106] Iteration 2680, lr = 9.96644e-05
I0803 18:15:26.033094  6069 solver.cpp:228] Iteration 2700, loss = 0.204555
I0803 18:15:26.033221  6069 solver.cpp:244]     Train net output #0: local/loss = 0.104809 (* 1 = 0.104809 loss)
I0803 18:15:26.033237  6069 solver.cpp:244]     Train net output #1: loss = 0.0996463 (* 1 = 0.0996463 loss)
I0803 18:15:26.033247  6069 solver.cpp:244]     Train net output #2: theta_loss = 9.94881e-05 (* 1 = 9.94881e-05 loss)
I0803 18:15:26.033260  6069 sgd_solver.cpp:106] Iteration 2700, lr = 9.96619e-05
I0803 18:15:41.958474  6069 solver.cpp:228] Iteration 2720, loss = 0.209883
I0803 18:15:41.958551  6069 solver.cpp:244]     Train net output #0: local/loss = 0.107789 (* 1 = 0.107789 loss)
I0803 18:15:41.958565  6069 solver.cpp:244]     Train net output #1: loss = 0.101965 (* 1 = 0.101965 loss)
I0803 18:15:41.958575  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000128862 (* 1 = 0.000128862 loss)
I0803 18:15:41.958587  6069 sgd_solver.cpp:106] Iteration 2720, lr = 9.96594e-05
I0803 18:16:03.785029  6069 solver.cpp:228] Iteration 2740, loss = 0.185834
I0803 18:16:03.785243  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0906054 (* 1 = 0.0906054 loss)
I0803 18:16:03.785261  6069 solver.cpp:244]     Train net output #1: loss = 0.0951084 (* 1 = 0.0951084 loss)
I0803 18:16:03.785271  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000119979 (* 1 = 0.000119979 loss)
I0803 18:16:03.785284  6069 sgd_solver.cpp:106] Iteration 2740, lr = 9.96569e-05
I0803 18:16:23.416477  6069 solver.cpp:228] Iteration 2760, loss = 0.206642
I0803 18:16:23.416553  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0976751 (* 1 = 0.0976751 loss)
I0803 18:16:23.416568  6069 solver.cpp:244]     Train net output #1: loss = 0.108736 (* 1 = 0.108736 loss)
I0803 18:16:23.416579  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000231396 (* 1 = 0.000231396 loss)
I0803 18:16:23.416591  6069 sgd_solver.cpp:106] Iteration 2760, lr = 9.96544e-05
I0803 18:16:48.384591  6069 solver.cpp:228] Iteration 2780, loss = 0.225591
I0803 18:16:48.384798  6069 solver.cpp:244]     Train net output #0: local/loss = 0.115952 (* 1 = 0.115952 loss)
I0803 18:16:48.384836  6069 solver.cpp:244]     Train net output #1: loss = 0.109392 (* 1 = 0.109392 loss)
I0803 18:16:48.384851  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000247626 (* 1 = 0.000247626 loss)
I0803 18:16:48.384886  6069 sgd_solver.cpp:106] Iteration 2780, lr = 9.96519e-05
I0803 18:17:07.377038  6069 solver.cpp:228] Iteration 2800, loss = 0.204319
I0803 18:17:07.377109  6069 solver.cpp:244]     Train net output #0: local/loss = 0.107183 (* 1 = 0.107183 loss)
I0803 18:17:07.377121  6069 solver.cpp:244]     Train net output #1: loss = 0.0969212 (* 1 = 0.0969212 loss)
I0803 18:17:07.377132  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000213943 (* 1 = 0.000213943 loss)
I0803 18:17:07.377143  6069 sgd_solver.cpp:106] Iteration 2800, lr = 9.96494e-05
I0803 18:17:24.533128  6069 solver.cpp:228] Iteration 2820, loss = 0.212215
I0803 18:17:24.533308  6069 solver.cpp:244]     Train net output #0: local/loss = 0.110499 (* 1 = 0.110499 loss)
I0803 18:17:24.533325  6069 solver.cpp:244]     Train net output #1: loss = 0.101597 (* 1 = 0.101597 loss)
I0803 18:17:24.533336  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000118742 (* 1 = 0.000118742 loss)
I0803 18:17:24.533350  6069 sgd_solver.cpp:106] Iteration 2820, lr = 9.96469e-05
I0803 18:17:39.896944  6069 solver.cpp:228] Iteration 2840, loss = 0.208072
I0803 18:17:39.897013  6069 solver.cpp:244]     Train net output #0: local/loss = 0.103292 (* 1 = 0.103292 loss)
I0803 18:17:39.897028  6069 solver.cpp:244]     Train net output #1: loss = 0.104678 (* 1 = 0.104678 loss)
I0803 18:17:39.897040  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00010158 (* 1 = 0.00010158 loss)
I0803 18:17:39.897054  6069 sgd_solver.cpp:106] Iteration 2840, lr = 9.96444e-05
I0803 18:18:02.138417  6069 solver.cpp:228] Iteration 2860, loss = 0.212823
I0803 18:18:02.138547  6069 solver.cpp:244]     Train net output #0: local/loss = 0.102239 (* 1 = 0.102239 loss)
I0803 18:18:02.138561  6069 solver.cpp:244]     Train net output #1: loss = 0.110486 (* 1 = 0.110486 loss)
I0803 18:18:02.138571  6069 solver.cpp:244]     Train net output #2: theta_loss = 9.81987e-05 (* 1 = 9.81987e-05 loss)
I0803 18:18:02.138584  6069 sgd_solver.cpp:106] Iteration 2860, lr = 9.96419e-05
I0803 18:18:25.888183  6069 solver.cpp:228] Iteration 2880, loss = 0.243532
I0803 18:18:25.888258  6069 solver.cpp:244]     Train net output #0: local/loss = 0.122631 (* 1 = 0.122631 loss)
I0803 18:18:25.888272  6069 solver.cpp:244]     Train net output #1: loss = 0.120668 (* 1 = 0.120668 loss)
I0803 18:18:25.888281  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00023386 (* 1 = 0.00023386 loss)
I0803 18:18:25.888294  6069 sgd_solver.cpp:106] Iteration 2880, lr = 9.96394e-05
I0803 18:18:38.664155  6069 solver.cpp:228] Iteration 2900, loss = 0.227261
I0803 18:18:38.664441  6069 solver.cpp:244]     Train net output #0: local/loss = 0.114553 (* 1 = 0.114553 loss)
I0803 18:18:38.664477  6069 solver.cpp:244]     Train net output #1: loss = 0.11236 (* 1 = 0.11236 loss)
I0803 18:18:38.664494  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000348062 (* 1 = 0.000348062 loss)
I0803 18:18:38.664510  6069 sgd_solver.cpp:106] Iteration 2900, lr = 9.96368e-05
I0803 18:19:08.131098  6069 solver.cpp:228] Iteration 2920, loss = 0.23566
I0803 18:19:08.131170  6069 solver.cpp:244]     Train net output #0: local/loss = 0.122264 (* 1 = 0.122264 loss)
I0803 18:19:08.131184  6069 solver.cpp:244]     Train net output #1: loss = 0.113262 (* 1 = 0.113262 loss)
I0803 18:19:08.131206  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00013408 (* 1 = 0.00013408 loss)
I0803 18:19:08.131222  6069 sgd_solver.cpp:106] Iteration 2920, lr = 9.96343e-05
I0803 18:19:26.555438  6069 solver.cpp:228] Iteration 2940, loss = 0.207149
I0803 18:19:26.555667  6069 solver.cpp:244]     Train net output #0: local/loss = 0.102171 (* 1 = 0.102171 loss)
I0803 18:19:26.555704  6069 solver.cpp:244]     Train net output #1: loss = 0.10476 (* 1 = 0.10476 loss)
I0803 18:19:26.555719  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00021892 (* 1 = 0.00021892 loss)
I0803 18:19:26.555737  6069 sgd_solver.cpp:106] Iteration 2940, lr = 9.96318e-05
I0803 18:19:49.941558  6069 solver.cpp:228] Iteration 2960, loss = 0.231674
I0803 18:19:49.941649  6069 solver.cpp:244]     Train net output #0: local/loss = 0.121403 (* 1 = 0.121403 loss)
I0803 18:19:49.941663  6069 solver.cpp:244]     Train net output #1: loss = 0.110036 (* 1 = 0.110036 loss)
I0803 18:19:49.941674  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000234949 (* 1 = 0.000234949 loss)
I0803 18:19:49.941686  6069 sgd_solver.cpp:106] Iteration 2960, lr = 9.96293e-05
I0803 18:20:21.631659  6069 solver.cpp:228] Iteration 2980, loss = 0.224263
I0803 18:20:21.631788  6069 solver.cpp:244]     Train net output #0: local/loss = 0.114851 (* 1 = 0.114851 loss)
I0803 18:20:21.631803  6069 solver.cpp:244]     Train net output #1: loss = 0.109294 (* 1 = 0.109294 loss)
I0803 18:20:21.631813  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000118632 (* 1 = 0.000118632 loss)
I0803 18:20:21.631824  6069 sgd_solver.cpp:106] Iteration 2980, lr = 9.96268e-05
I0803 18:20:50.385133  6069 solver.cpp:337] Iteration 3000, Testing net (#0)
I0803 18:22:23.923946  6069 solver.cpp:404]     Test net output #0: local/loss = 0.107852 (* 1 = 0.107852 loss)
I0803 18:22:23.924149  6069 solver.cpp:404]     Test net output #1: loss = 0.0993926 (* 1 = 0.0993926 loss)
I0803 18:22:23.924185  6069 solver.cpp:404]     Test net output #2: theta_loss = 8.65905e-05 (* 1 = 8.65905e-05 loss)
I0803 18:22:24.328081  6069 solver.cpp:228] Iteration 3000, loss = 0.279377
I0803 18:22:24.328148  6069 solver.cpp:244]     Train net output #0: local/loss = 0.146295 (* 1 = 0.146295 loss)
I0803 18:22:24.328166  6069 solver.cpp:244]     Train net output #1: loss = 0.133004 (* 1 = 0.133004 loss)
I0803 18:22:24.328178  6069 solver.cpp:244]     Train net output #2: theta_loss = 7.84773e-05 (* 1 = 7.84773e-05 loss)
I0803 18:22:24.328194  6069 sgd_solver.cpp:106] Iteration 3000, lr = 9.96243e-05
I0803 18:22:41.832101  6069 solver.cpp:228] Iteration 3020, loss = 0.231607
I0803 18:22:41.832172  6069 solver.cpp:244]     Train net output #0: local/loss = 0.119958 (* 1 = 0.119958 loss)
I0803 18:22:41.832186  6069 solver.cpp:244]     Train net output #1: loss = 0.11151 (* 1 = 0.11151 loss)
I0803 18:22:41.832196  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000138441 (* 1 = 0.000138441 loss)
I0803 18:22:41.832208  6069 sgd_solver.cpp:106] Iteration 3020, lr = 9.96218e-05
I0803 18:23:08.826584  6069 solver.cpp:228] Iteration 3040, loss = 0.215256
I0803 18:23:08.826746  6069 solver.cpp:244]     Train net output #0: local/loss = 0.106834 (* 1 = 0.106834 loss)
I0803 18:23:08.826764  6069 solver.cpp:244]     Train net output #1: loss = 0.1081 (* 1 = 0.1081 loss)
I0803 18:23:08.826776  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000322142 (* 1 = 0.000322142 loss)
I0803 18:23:08.826787  6069 sgd_solver.cpp:106] Iteration 3040, lr = 9.96193e-05
I0803 18:23:25.128880  6069 solver.cpp:228] Iteration 3060, loss = 0.19734
I0803 18:23:25.128948  6069 solver.cpp:244]     Train net output #0: local/loss = 0.094325 (* 1 = 0.094325 loss)
I0803 18:23:25.128962  6069 solver.cpp:244]     Train net output #1: loss = 0.102671 (* 1 = 0.102671 loss)
I0803 18:23:25.128971  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00034401 (* 1 = 0.00034401 loss)
I0803 18:23:25.128984  6069 sgd_solver.cpp:106] Iteration 3060, lr = 9.96168e-05
I0803 18:23:55.550101  6069 solver.cpp:228] Iteration 3080, loss = 0.211668
I0803 18:23:55.550400  6069 solver.cpp:244]     Train net output #0: local/loss = 0.108172 (* 1 = 0.108172 loss)
I0803 18:23:55.550449  6069 solver.cpp:244]     Train net output #1: loss = 0.103337 (* 1 = 0.103337 loss)
I0803 18:23:55.550482  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000158651 (* 1 = 0.000158651 loss)
I0803 18:23:55.550529  6069 sgd_solver.cpp:106] Iteration 3080, lr = 9.96143e-05
I0803 18:24:19.125804  6069 solver.cpp:228] Iteration 3100, loss = 0.210864
I0803 18:24:19.125888  6069 solver.cpp:244]     Train net output #0: local/loss = 0.106096 (* 1 = 0.106096 loss)
I0803 18:24:19.125900  6069 solver.cpp:244]     Train net output #1: loss = 0.104619 (* 1 = 0.104619 loss)
I0803 18:24:19.125910  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000148232 (* 1 = 0.000148232 loss)
I0803 18:24:19.125923  6069 sgd_solver.cpp:106] Iteration 3100, lr = 9.96117e-05
I0803 18:24:33.951567  6069 solver.cpp:228] Iteration 3120, loss = 0.207027
I0803 18:24:33.951779  6069 solver.cpp:244]     Train net output #0: local/loss = 0.105438 (* 1 = 0.105438 loss)
I0803 18:24:33.951817  6069 solver.cpp:244]     Train net output #1: loss = 0.101376 (* 1 = 0.101376 loss)
I0803 18:24:33.951833  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000213957 (* 1 = 0.000213957 loss)
I0803 18:24:33.951851  6069 sgd_solver.cpp:106] Iteration 3120, lr = 9.96092e-05
I0803 18:24:59.749845  6069 solver.cpp:228] Iteration 3140, loss = 0.203322
I0803 18:24:59.749919  6069 solver.cpp:244]     Train net output #0: local/loss = 0.109121 (* 1 = 0.109121 loss)
I0803 18:24:59.749933  6069 solver.cpp:244]     Train net output #1: loss = 0.0940609 (* 1 = 0.0940609 loss)
I0803 18:24:59.749943  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000139969 (* 1 = 0.000139969 loss)
I0803 18:24:59.749954  6069 sgd_solver.cpp:106] Iteration 3140, lr = 9.96067e-05
I0803 18:25:24.369290  6069 solver.cpp:228] Iteration 3160, loss = 0.194783
I0803 18:25:24.369498  6069 solver.cpp:244]     Train net output #0: local/loss = 0.100301 (* 1 = 0.100301 loss)
I0803 18:25:24.369534  6069 solver.cpp:244]     Train net output #1: loss = 0.0942874 (* 1 = 0.0942874 loss)
I0803 18:25:24.369549  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000194081 (* 1 = 0.000194081 loss)
I0803 18:25:24.369566  6069 sgd_solver.cpp:106] Iteration 3160, lr = 9.96042e-05
I0803 18:25:49.558207  6069 solver.cpp:228] Iteration 3180, loss = 0.189646
I0803 18:25:49.558289  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0987756 (* 1 = 0.0987756 loss)
I0803 18:25:49.558300  6069 solver.cpp:244]     Train net output #1: loss = 0.090605 (* 1 = 0.090605 loss)
I0803 18:25:49.558310  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000265815 (* 1 = 0.000265815 loss)
I0803 18:25:49.558322  6069 sgd_solver.cpp:106] Iteration 3180, lr = 9.96017e-05
I0803 18:26:11.714758  6069 solver.cpp:228] Iteration 3200, loss = 0.184753
I0803 18:26:11.714963  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0924312 (* 1 = 0.0924312 loss)
I0803 18:26:11.715004  6069 solver.cpp:244]     Train net output #1: loss = 0.0922166 (* 1 = 0.0922166 loss)
I0803 18:26:11.715020  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000105097 (* 1 = 0.000105097 loss)
I0803 18:26:11.715037  6069 sgd_solver.cpp:106] Iteration 3200, lr = 9.95992e-05
I0803 18:26:37.317243  6069 solver.cpp:228] Iteration 3220, loss = 0.199721
I0803 18:26:37.317339  6069 solver.cpp:244]     Train net output #0: local/loss = 0.104287 (* 1 = 0.104287 loss)
I0803 18:26:37.317355  6069 solver.cpp:244]     Train net output #1: loss = 0.0953033 (* 1 = 0.0953033 loss)
I0803 18:26:37.317365  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00013099 (* 1 = 0.00013099 loss)
I0803 18:26:37.317379  6069 sgd_solver.cpp:106] Iteration 3220, lr = 9.95967e-05
I0803 18:27:00.619520  6069 solver.cpp:228] Iteration 3240, loss = 0.225683
I0803 18:27:00.619807  6069 solver.cpp:244]     Train net output #0: local/loss = 0.111531 (* 1 = 0.111531 loss)
I0803 18:27:00.619840  6069 solver.cpp:244]     Train net output #1: loss = 0.114047 (* 1 = 0.114047 loss)
I0803 18:27:00.619854  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000105637 (* 1 = 0.000105637 loss)
I0803 18:27:00.619871  6069 sgd_solver.cpp:106] Iteration 3240, lr = 9.95942e-05
I0803 18:27:22.527848  6069 solver.cpp:228] Iteration 3260, loss = 0.206033
I0803 18:27:22.527935  6069 solver.cpp:244]     Train net output #0: local/loss = 0.102567 (* 1 = 0.102567 loss)
I0803 18:27:22.527961  6069 solver.cpp:244]     Train net output #1: loss = 0.103328 (* 1 = 0.103328 loss)
I0803 18:27:22.527974  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000138906 (* 1 = 0.000138906 loss)
I0803 18:27:22.527987  6069 sgd_solver.cpp:106] Iteration 3260, lr = 9.95917e-05
I0803 18:27:48.736919  6069 solver.cpp:228] Iteration 3280, loss = 0.208437
I0803 18:27:48.737166  6069 solver.cpp:244]     Train net output #0: local/loss = 0.111093 (* 1 = 0.111093 loss)
I0803 18:27:48.737205  6069 solver.cpp:244]     Train net output #1: loss = 0.0972418 (* 1 = 0.0972418 loss)
I0803 18:27:48.737226  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000101975 (* 1 = 0.000101975 loss)
I0803 18:27:48.737252  6069 sgd_solver.cpp:106] Iteration 3280, lr = 9.95892e-05
I0803 18:28:09.626730  6069 solver.cpp:228] Iteration 3300, loss = 0.19599
I0803 18:28:09.626813  6069 solver.cpp:244]     Train net output #0: local/loss = 0.100425 (* 1 = 0.100425 loss)
I0803 18:28:09.626827  6069 solver.cpp:244]     Train net output #1: loss = 0.0952596 (* 1 = 0.0952596 loss)
I0803 18:28:09.626837  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00030563 (* 1 = 0.00030563 loss)
I0803 18:28:09.626848  6069 sgd_solver.cpp:106] Iteration 3300, lr = 9.95866e-05
I0803 18:28:31.930585  6069 solver.cpp:228] Iteration 3320, loss = 0.245531
I0803 18:28:31.930809  6069 solver.cpp:244]     Train net output #0: local/loss = 0.118003 (* 1 = 0.118003 loss)
I0803 18:28:31.930840  6069 solver.cpp:244]     Train net output #1: loss = 0.127069 (* 1 = 0.127069 loss)
I0803 18:28:31.930855  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000458412 (* 1 = 0.000458412 loss)
I0803 18:28:31.930872  6069 sgd_solver.cpp:106] Iteration 3320, lr = 9.95841e-05
I0803 18:28:57.021644  6069 solver.cpp:228] Iteration 3340, loss = 0.211268
I0803 18:28:57.021721  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0973275 (* 1 = 0.0973275 loss)
I0803 18:28:57.021734  6069 solver.cpp:244]     Train net output #1: loss = 0.113705 (* 1 = 0.113705 loss)
I0803 18:28:57.021744  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000235362 (* 1 = 0.000235362 loss)
I0803 18:28:57.021756  6069 sgd_solver.cpp:106] Iteration 3340, lr = 9.95816e-05
I0803 18:29:22.752395  6069 solver.cpp:228] Iteration 3360, loss = 0.212054
I0803 18:29:22.752578  6069 solver.cpp:244]     Train net output #0: local/loss = 0.110611 (* 1 = 0.110611 loss)
I0803 18:29:22.752593  6069 solver.cpp:244]     Train net output #1: loss = 0.101374 (* 1 = 0.101374 loss)
I0803 18:29:22.752604  6069 solver.cpp:244]     Train net output #2: theta_loss = 6.97112e-05 (* 1 = 6.97112e-05 loss)
I0803 18:29:22.752616  6069 sgd_solver.cpp:106] Iteration 3360, lr = 9.95791e-05
I0803 18:29:48.543110  6069 solver.cpp:228] Iteration 3380, loss = 0.224639
I0803 18:29:48.543200  6069 solver.cpp:244]     Train net output #0: local/loss = 0.117585 (* 1 = 0.117585 loss)
I0803 18:29:48.543217  6069 solver.cpp:244]     Train net output #1: loss = 0.106888 (* 1 = 0.106888 loss)
I0803 18:29:48.543236  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000165573 (* 1 = 0.000165573 loss)
I0803 18:29:48.543252  6069 sgd_solver.cpp:106] Iteration 3380, lr = 9.95766e-05
I0803 18:30:09.869595  6069 solver.cpp:228] Iteration 3400, loss = 0.200722
I0803 18:30:09.869845  6069 solver.cpp:244]     Train net output #0: local/loss = 0.100203 (* 1 = 0.100203 loss)
I0803 18:30:09.869880  6069 solver.cpp:244]     Train net output #1: loss = 0.10042 (* 1 = 0.10042 loss)
I0803 18:30:09.869896  6069 solver.cpp:244]     Train net output #2: theta_loss = 9.84325e-05 (* 1 = 9.84325e-05 loss)
I0803 18:30:09.869913  6069 sgd_solver.cpp:106] Iteration 3400, lr = 9.95741e-05
I0803 18:30:28.189347  6069 solver.cpp:228] Iteration 3420, loss = 0.190099
I0803 18:30:28.189422  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0929576 (* 1 = 0.0929576 loss)
I0803 18:30:28.189435  6069 solver.cpp:244]     Train net output #1: loss = 0.0970958 (* 1 = 0.0970958 loss)
I0803 18:30:28.189446  6069 solver.cpp:244]     Train net output #2: theta_loss = 4.54529e-05 (* 1 = 4.54529e-05 loss)
I0803 18:30:28.189458  6069 sgd_solver.cpp:106] Iteration 3420, lr = 9.95716e-05
I0803 18:30:52.176559  6069 solver.cpp:228] Iteration 3440, loss = 0.223412
I0803 18:30:52.176784  6069 solver.cpp:244]     Train net output #0: local/loss = 0.114523 (* 1 = 0.114523 loss)
I0803 18:30:52.176820  6069 solver.cpp:244]     Train net output #1: loss = 0.108777 (* 1 = 0.108777 loss)
I0803 18:30:52.176834  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000111849 (* 1 = 0.000111849 loss)
I0803 18:30:52.176852  6069 sgd_solver.cpp:106] Iteration 3440, lr = 9.95691e-05
I0803 18:31:14.941757  6069 solver.cpp:228] Iteration 3460, loss = 0.223805
I0803 18:31:14.941807  6069 solver.cpp:244]     Train net output #0: local/loss = 0.120426 (* 1 = 0.120426 loss)
I0803 18:31:14.941819  6069 solver.cpp:244]     Train net output #1: loss = 0.103151 (* 1 = 0.103151 loss)
I0803 18:31:14.941828  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000227585 (* 1 = 0.000227585 loss)
I0803 18:31:14.941839  6069 sgd_solver.cpp:106] Iteration 3460, lr = 9.95666e-05
I0803 18:31:36.700242  6069 solver.cpp:228] Iteration 3480, loss = 0.211859
I0803 18:31:36.700448  6069 solver.cpp:244]     Train net output #0: local/loss = 0.109015 (* 1 = 0.109015 loss)
I0803 18:31:36.700477  6069 solver.cpp:244]     Train net output #1: loss = 0.102753 (* 1 = 0.102753 loss)
I0803 18:31:36.700492  6069 solver.cpp:244]     Train net output #2: theta_loss = 9.03369e-05 (* 1 = 9.03369e-05 loss)
I0803 18:31:36.700507  6069 sgd_solver.cpp:106] Iteration 3480, lr = 9.9564e-05
I0803 18:31:59.667850  6069 solver.cpp:228] Iteration 3500, loss = 0.205348
I0803 18:31:59.667922  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0996153 (* 1 = 0.0996153 loss)
I0803 18:31:59.667937  6069 solver.cpp:244]     Train net output #1: loss = 0.105662 (* 1 = 0.105662 loss)
I0803 18:31:59.667946  6069 solver.cpp:244]     Train net output #2: theta_loss = 7.05559e-05 (* 1 = 7.05559e-05 loss)
I0803 18:31:59.667969  6069 sgd_solver.cpp:106] Iteration 3500, lr = 9.95615e-05
I0803 18:32:18.183040  6069 solver.cpp:228] Iteration 3520, loss = 0.229347
I0803 18:32:18.183210  6069 solver.cpp:244]     Train net output #0: local/loss = 0.121998 (* 1 = 0.121998 loss)
I0803 18:32:18.183225  6069 solver.cpp:244]     Train net output #1: loss = 0.107245 (* 1 = 0.107245 loss)
I0803 18:32:18.183236  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000103103 (* 1 = 0.000103103 loss)
I0803 18:32:18.183250  6069 sgd_solver.cpp:106] Iteration 3520, lr = 9.9559e-05
I0803 18:32:18.957659  6069 blocking_queue.cpp:50] Data layer prefetch queue empty
I0803 18:32:37.032408  6069 solver.cpp:228] Iteration 3540, loss = 0.209709
I0803 18:32:37.032480  6069 solver.cpp:244]     Train net output #0: local/loss = 0.105871 (* 1 = 0.105871 loss)
I0803 18:32:37.032492  6069 solver.cpp:244]     Train net output #1: loss = 0.103768 (* 1 = 0.103768 loss)
I0803 18:32:37.032502  6069 solver.cpp:244]     Train net output #2: theta_loss = 7.09355e-05 (* 1 = 7.09355e-05 loss)
I0803 18:32:37.032515  6069 sgd_solver.cpp:106] Iteration 3540, lr = 9.95565e-05
I0803 18:32:57.984727  6069 solver.cpp:228] Iteration 3560, loss = 0.180282
I0803 18:32:57.985019  6069 solver.cpp:244]     Train net output #0: local/loss = 0.0902856 (* 1 = 0.0902856 loss)
I0803 18:32:57.985051  6069 solver.cpp:244]     Train net output #1: loss = 0.0898668 (* 1 = 0.0898668 loss)
I0803 18:32:57.985066  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00012947 (* 1 = 0.00012947 loss)
I0803 18:32:57.985080  6069 sgd_solver.cpp:106] Iteration 3560, lr = 9.9554e-05
I0803 18:33:12.143637  6069 solver.cpp:228] Iteration 3580, loss = 0.217094
I0803 18:33:12.143720  6069 solver.cpp:244]     Train net output #0: local/loss = 0.106847 (* 1 = 0.106847 loss)
I0803 18:33:12.143736  6069 solver.cpp:244]     Train net output #1: loss = 0.110085 (* 1 = 0.110085 loss)
I0803 18:33:12.143748  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000162719 (* 1 = 0.000162719 loss)
I0803 18:33:12.143762  6069 sgd_solver.cpp:106] Iteration 3580, lr = 9.95515e-05
I0803 18:33:39.988698  6069 solver.cpp:228] Iteration 3600, loss = 0.251275
I0803 18:33:39.988909  6069 solver.cpp:244]     Train net output #0: local/loss = 0.125409 (* 1 = 0.125409 loss)
I0803 18:33:39.988929  6069 solver.cpp:244]     Train net output #1: loss = 0.125755 (* 1 = 0.125755 loss)
I0803 18:33:39.988937  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.00011073 (* 1 = 0.00011073 loss)
I0803 18:33:39.988947  6069 sgd_solver.cpp:106] Iteration 3600, lr = 9.9549e-05
I0803 18:34:03.430716  6069 solver.cpp:228] Iteration 3620, loss = 0.219006
I0803 18:34:03.430788  6069 solver.cpp:244]     Train net output #0: local/loss = 0.108832 (* 1 = 0.108832 loss)
I0803 18:34:03.430802  6069 solver.cpp:244]     Train net output #1: loss = 0.110038 (* 1 = 0.110038 loss)
I0803 18:34:03.430814  6069 solver.cpp:244]     Train net output #2: theta_loss = 0.000135995 (* 1 = 0.000135995 loss)
I0803 18:34:03.430830  6069 sgd_solver.cpp:106] Iteration 3620, lr = 9.95465e-05
I0803 18:34:18.117686  6069 solver.cpp:454] Snapshotting to binary proto file model/facial_point_iter_3638.caffemodel
I0803 18:38:54.576953  6069 sgd_solver.cpp:273] Snapshotting solver state to binary proto file model/facial_point_iter_3638.solverstate
I0803 18:38:55.916189  6069 solver.cpp:301] Optimization stopped early.
I0803 18:38:55.916262  6069 caffe.cpp:222] Optimization Done.
